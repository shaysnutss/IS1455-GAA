[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "IS1455-GAA",
    "section": "",
    "text": "Welcome to IS415 Geospatial Analytics and Applications.\nThis is the course website of the IS415 mod that I am embarking on this semester. You will find my course materials here. Have fun:)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html",
    "title": "Hands-On Exercise 2 : Thematic Mapping and GeoVisualisation with R",
    "section": "",
    "text": "Hello! In this page, i will be describing how i performed Thematic Mapping and GeoVisualisation with R. I will mainly be trying to create a choropleth map using the tmap package!\nWe also use pivot_wider() of the tidyr package, and mutate() ,group_by() , select() and filter() of the dplyr package, which i will be explaining in detail.\nI hope you can follow along, and be able to create choropleth maps!"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#loading-packages",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#loading-packages",
    "title": "Hands-On Exercise 2 : Thematic Mapping and GeoVisualisation with R",
    "section": "Loading packages",
    "text": "Loading packages\nThis is a very important step as we have to load the packages before we can use them.\n\npacman::p_load(sf, tmap, tidyverse)\n\n\nWhat is the tmap package?\nThe tmap package provides you with many handy functions to create your own thematic maps like choropleth maps."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#importing-the-data",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#importing-the-data",
    "title": "Hands-On Exercise 2 : Thematic Mapping and GeoVisualisation with R",
    "section": "Importing the data",
    "text": "Importing the data\n\nDownloading the data\nThe links of the data i used are here (download shp format) and here\n\n\nImporting Geospatial Data into R\nAs learnt in the previous page, we use st_read() function of the sf package to read in the data.\n\nmpsz &lt;- st_read(dsn = \"data/geospatial\", \n                layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `C:\\shaysnutss\\IS1455-GAA\\Hands-on_Ex\\Hands-on_Ex02\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\nWhen we realtiple features (geographical entities), with 15 fields, or attributes (such as temperature, precipitation for example).\nLet us examine it further by just printing mpsz (the variable we have stored the data in, that is of sf data object).\n\nmpsz\n\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\nFirst 10 features:\n   OBJECTID SUBZONE_NO       SUBZONE_N SUBZONE_C CA_IND      PLN_AREA_N\n1         1          1    MARINA SOUTH    MSSZ01      Y    MARINA SOUTH\n2         2          1    PEARL'S HILL    OTSZ01      Y          OUTRAM\n3         3          3       BOAT QUAY    SRSZ03      Y SINGAPORE RIVER\n4         4          8  HENDERSON HILL    BMSZ08      N     BUKIT MERAH\n5         5          3         REDHILL    BMSZ03      N     BUKIT MERAH\n6         6          7  ALEXANDRA HILL    BMSZ07      N     BUKIT MERAH\n7         7          9   BUKIT HO SWEE    BMSZ09      N     BUKIT MERAH\n8         8          2     CLARKE QUAY    SRSZ02      Y SINGAPORE RIVER\n9         9         13 PASIR PANJANG 1    QTSZ13      N      QUEENSTOWN\n10       10          7       QUEENSWAY    QTSZ07      N      QUEENSTOWN\n   PLN_AREA_C       REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR\n1          MS CENTRAL REGION       CR 5ED7EB253F99252E 2014-12-05 31595.84\n2          OT CENTRAL REGION       CR 8C7149B9EB32EEFC 2014-12-05 28679.06\n3          SR CENTRAL REGION       CR C35FEFF02B13E0E5 2014-12-05 29654.96\n4          BM CENTRAL REGION       CR 3775D82C5DDBEFBD 2014-12-05 26782.83\n5          BM CENTRAL REGION       CR 85D9ABEF0A40678F 2014-12-05 26201.96\n6          BM CENTRAL REGION       CR 9D286521EF5E3B59 2014-12-05 25358.82\n7          BM CENTRAL REGION       CR 7839A8577144EFE2 2014-12-05 27680.06\n8          SR CENTRAL REGION       CR 48661DC0FBA09F7A 2014-12-05 29253.21\n9          QT CENTRAL REGION       CR 1F721290C421BFAB 2014-12-05 22077.34\n10         QT CENTRAL REGION       CR 3580D2AFFBEE914C 2014-12-05 24168.31\n     Y_ADDR SHAPE_Leng SHAPE_Area                       geometry\n1  29220.19   5267.381  1630379.3 MULTIPOLYGON (((31495.56 30...\n2  29782.05   3506.107   559816.2 MULTIPOLYGON (((29092.28 30...\n3  29974.66   1740.926   160807.5 MULTIPOLYGON (((29932.33 29...\n4  29933.77   3313.625   595428.9 MULTIPOLYGON (((27131.28 30...\n5  30005.70   2825.594   387429.4 MULTIPOLYGON (((26451.03 30...\n6  29991.38   4428.913  1030378.8 MULTIPOLYGON (((25899.7 297...\n7  30230.86   3275.312   551732.0 MULTIPOLYGON (((27746.95 30...\n8  30222.86   2208.619   290184.7 MULTIPOLYGON (((29351.26 29...\n9  29893.78   6571.323  1084792.3 MULTIPOLYGON (((20996.49 30...\n10 30104.18   3454.239   631644.3 MULTIPOLYGON (((24472.11 29...\n\n\nWe can also use glimpse() or head().\n\n\nImporting Attribute Data into R\nNow let us import the aspatial or attribute data into R.\n\nWhat is aspatial/attribute data?\nIt is data that’s not related to shape/location or coordinates but provides more context to the data. For example, if the data is about Airbnb, it tells you the number of rooms, the pricing, the number of people living in each Airbnb house.\nLet’s import it:\n\npopdata &lt;- read_csv(\"data/aspatial/respopagesextod2011to2020.csv\")\n\nNow, let’s view it:\n\nlist(popdata) \n\n[[1]]\n# A tibble: 984,656 × 7\n   PA         SZ                     AG     Sex     TOD                Pop  Time\n   &lt;chr&gt;      &lt;chr&gt;                  &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt;            &lt;dbl&gt; &lt;dbl&gt;\n 1 Ang Mo Kio Ang Mo Kio Town Centre 0_to_4 Males   HDB 1- and 2-Ro…     0  2011\n 2 Ang Mo Kio Ang Mo Kio Town Centre 0_to_4 Males   HDB 3-Room Flats    10  2011\n 3 Ang Mo Kio Ang Mo Kio Town Centre 0_to_4 Males   HDB 4-Room Flats    30  2011\n 4 Ang Mo Kio Ang Mo Kio Town Centre 0_to_4 Males   HDB 5-Room and …    50  2011\n 5 Ang Mo Kio Ang Mo Kio Town Centre 0_to_4 Males   HUDC Flats (exc…     0  2011\n 6 Ang Mo Kio Ang Mo Kio Town Centre 0_to_4 Males   Landed Properti…     0  2011\n 7 Ang Mo Kio Ang Mo Kio Town Centre 0_to_4 Males   Condominiums an…    40  2011\n 8 Ang Mo Kio Ang Mo Kio Town Centre 0_to_4 Males   Others               0  2011\n 9 Ang Mo Kio Ang Mo Kio Town Centre 0_to_4 Females HDB 1- and 2-Ro…     0  2011\n10 Ang Mo Kio Ang Mo Kio Town Centre 0_to_4 Females HDB 3-Room Flats    10  2011\n# ℹ 984,646 more rows\n\n\n\n\n\nData Preparation\nWe need to prepare data to be in the form of a data table (like a normal sql table) before we can prepare a choropleth map.\nTo prepare the data, we use a few methods. i will go through them here before we actually execute them.\nThese are the methods we will go through:\n\npivot_wider() - tidyr package\nmutate() - dplyr package\ngroup_by() - dplyr package\nselect() - dplyr package\nfilter() - dplyr package\n\n\npivot_wider()\nIt helps transform data from long format to wide format.\nLong format: There are more rows and less columns. Here data is normalised and there are many rows for one subject. This can make it hard to compare the different subjects, and is not very good for visualization.\nHere is how a long format table could look like:\n\nHere there are multiple rows if you want to see the amount of sales in a particular month , or for a particular product. It is not very conducive for comparisons.\nWide format: There are less rows and more columns.\n\nHere there are lesser rows (usually there are more rows but in this case it worked out to still being 3 columns). And it is clearer as to how much sales there is per product per month.\nHow it’s used:\nwide_data &lt;- long_data %&gt;% pivot_wider(names_from = Product, values_from = Sales)\nnames_from: is the argument where you specify the column that will be used to create columns in the wide format\nvalues_from: specifies the column containing the values that will be used to file the new wide format columns\n\n\nmutate()\nmutate() is used to execute operations on existing columns and transform them, create new columns, or transform/create data using conditional statements.\n\nHere we see that we create a new column called Grade and if the score is greater than 90, we populate it with A , if not B. We then process existing data by increasing age by 1.\n\n\nfilter()\nThis method is used to filter rows based on a condition. Here is how it is used:\n\nHere we filter the rows and include rows in the data frame ONLY if the score was equal to or greater than 90.\n\n\nselect()\nThis is like the select query in sql, and you use this to select columns you are interested in.\n\n\n\ngroup_by()\nHere you group the data into groups due to a column, like gender for instance. You can then do group wide operations.\n\n\n\nData Wrangling\nNow this is the code we are supposed to run to prepare the data. Before we prepare the data let’s take a step back.\nOur purpose here is to build a choropleth map that shows you the distribution of the dependency variable across the country/ per region. The dependency variable is calculated like so: DEPENDENCY = (YOUNG + AGED) /`ECONOMY ACTIVE. Thus, we need to know the number of people who are young, aged and economy active in each region. Hence, we have to make sure we make rows that tell us these values per region - that requires group by the two values PA and SZ. We can also group_by AG first so we can see the number of people per age group in each location.\nI have added the explanation for each line of code above that code.\n\npopdata2020 &lt;- popdata %&gt;%\n  filter(Time == 2020) %&gt;%\n  group_by(PA, SZ, AG) %&gt;%\n  summarise(`POP` = sum(`Pop`)) %&gt;%\n  ungroup()%&gt;%\n  pivot_wider(names_from=AG, \n              values_from=POP) %&gt;%\n  mutate(YOUNG = rowSums(.[3:6])\n         +rowSums(.[12])) %&gt;%\nmutate(`ECONOMY ACTIVE` = rowSums(.[7:11])+\nrowSums(.[13:15]))%&gt;%\nmutate(`AGED`=rowSums(.[16:21])) %&gt;%\nmutate(`TOTAL`=rowSums(.[3:21])) %&gt;%  \nmutate(`DEPENDENCY` = (`YOUNG` + `AGED`)\n/`ECONOMY ACTIVE`) %&gt;%\n  # we then just want to display columns that are these\n  select(`PA`, `SZ`, `YOUNG`, \n       `ECONOMY ACTIVE`, `AGED`, \n       `TOTAL`, `DEPENDENCY`)\n\n\nCode walkthrough\n\nfilter(Time == 2020) %&gt;%\n\nHere, we are only selecting the rows that have the value 2020 under the time category.\nHere’s a sample data set to help you understand.\n\n\ngroup_by(PA, SZ, AG) %&gt;%\n\nIn group by, we make different groups, each with a unique combination of PA, SZ, and AG. Any row in the dataset that matches that group’s combination of PA, SZ, and AG will be in that group. Hence, each group will have different number of rows. All of the columns of the row will still be there when just executing a group_by(). When grouping, we can do group wide operations, instead of data wide.\n\nsummarise(POP = sum(Pop)) %&gt;%\n\nWe now have the data in groups, with each group having different number of rows.\nWe then add up all the population figures of all the rows in each group and store them in a new column called pop. Now, since the population values in all rows in each group are added together, we have one row per group. Each group has its own combination of pa, sz and ag and its aggragated total summary for population. All the other columns like sex and tod are discarded. You chose to add the population values of all rows together, and did not mention what to do with the other rows, hence they are discarded!\nIn the picture below, you can see that the number of rows have decreased as there were some groups with more than 1 row, and when the population values were added, they were summarised into a row.\nWe now know the number of people per age group per region. This is vital because we have different categories such as young, aged and economy active that we need to compute. Each young, aged and economy active category would consitute of different age categories. Having various different age categories allows us to pick and choose the varying age categories we want to compute to find out the population for each of the young, aged and economy active categories for each region.\n\n\npivot_wider(names_from=AG, values_from=POP) %&gt;%\n\nTo make it easier to compute the population values for each young, aged and economy active category per region, we should make the age categories column names. Then, we can make the population the values under each age category. As illustrated below, we can easily see the number of people per age category per region.\n\n\nmutate(YOUNG = rowSums(.[3:6]) +rowSums(.[12])) %&gt;%\n\nAs we said before, there are different age categories , like 0-4 being one column 5-9 being another column. Hence, we add up populations under columns 3 to 6 in each row as that is the age range that we think is young. After calculating that for each row, we create a column called young and populate it.\n\n\nmutate(ECONOMY ACTIVE = rowSums(.[7:11])+ rowSums(.[13:15]))%&gt;%\n\nnext we want to find out what is the total number of pople in each subzone that are economically active. so we want to add up the values under age range categories that we think are eco active in each row and then make a eco active column and populate it underneath that\n\nmutate(AGED=rowSums(.[16:21])) %&gt;%\n\nDo the same thing for total number of people for each subzone\n\nmutate(TOTAL=rowSums(.[3:21])) %&gt;%\n\nDo the same thing for total number of people for each subzone\n\nmutate(DEPENDENCY = (YOUNG + AGED) /ECONOMY ACTIVE) %&gt;%\n\nwe then calculate the dependency\n\nselect(PA, SZ, YOUNG, ECONOMY ACTIVE, AGED, TOTAL, DEPENDENCY)\n\nwe then just select the columns we want\n\n\n\n\n\nJoining attribute and geographical data\nThe values in PA and SZ are made up of lower and upper case so lets make them all uppercase\n\npopdata2020 &lt;- popdata2020 %&gt;%\n  mutate_at(.vars = vars(PA, SZ), \n          .funs = list(toupper)) %&gt;%\n  filter(`ECONOMY ACTIVE` &gt; 0)\n\nNow let’s join them based on one common column that is SZ - it is present in both the geospatial and attribute data.\n\nmpsz_pop2020 &lt;- left_join(mpsz, popdata2020,\n                          by = c(\"SUBZONE_N\" = \"SZ\"))\n\nLet’s write it into a file. Remember to create the directory and file data/rds/mpszpop2020.rds before executing this.\n\nwrite_rds(mpsz_pop2020, \"data/rds/mpszpop2020.rds\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#choropleth-mapping-geospatial-data-using-tmap",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#choropleth-mapping-geospatial-data-using-tmap",
    "title": "Hands-On Exercise 2 : Thematic Mapping and GeoVisualisation with R",
    "section": "Choropleth Mapping Geospatial Data using tmap",
    "text": "Choropleth Mapping Geospatial Data using tmap\nYou can do this two ways\n\nPlotting a thematic map quickly by using qtm()\nPlotting highly customisable thematic map by using tmap elements\n\n\nUsing qtm()\nWhen we say fill = “DEPENDENCY”, we mean that the colors of the choropleth map would vary based on the values of dependency.\nThis method is quick but reduces the scope for customization.\n\ntmap_mode(\"plot\")\nqtm(mpsz_pop2020, \n    fill = \"DEPENDENCY\")\n\n\n\n\n\n\nUsing tmap()\nWe can start of with learning about using tm_shape() and tm_fill().\nWe are showing a geographical distribution of dependency.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\")\n\n\n\n\nIf we wanted to customise more, and use tm_borders,\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\") +\n  tm_borders(lwd = 0.1,  alpha = 1)\n\n\n\n\nIf we wanted ultimate customisation, these are the functions and arguments available.\nstyle= quantile shows that we use quantile to classify the data and come up with categories/classes. This means there are four categories with the number of data points each category being equal.\nThe palette=blues is the theme of the map.\nThe layout method often focuses on the aesthetics of the map and is quite self explanatory.\ntm_compass() adds a compass to the map.\ntm_scale_bar() adds the scale to show the relationship between the real measurements to the measurements on the screen.\ntm_grid() adds grid lines to show longitutde and latitude.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"Dependency ratio\") +\n  tm_layout(main.title = \"Distribution of Dependency Ratio by planning subzone\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar() +\n  tm_grid(alpha =0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\\n and Population data from Department of Statistics DOS\", \n             position = c(\"left\", \"bottom\"))\n\n\n\n\nHere we have used tmap and used tm_shape() + tm_fill(). tm_fill() automatically draws out polygons in your map and fills them with the color necessiated by its relevant value.\nLet’s say you wanted to draw other shapes in your map other than polygons. You could do that with tm_lines() or tm_raster()\nBut let’s say we wanted to say outright that we wanted to draw out polygons. We can use tm_polygons()\n\n\nUsing tm_polygons()\n\ntm_shape(mpsz_pop2020) +\n  tm_polygons()\n\n\n\n\nRemember: we have to do tm_shape first to load our data in.\nIf we wanted to also say what variable we wanted to show a geographical distribution of :\n\ntm_shape(mpsz_pop2020)+\n  tm_polygons(\"DEPENDENCY\")\n\n\n\n\n\n\nDrawing a choropleth map using tm_fill() and *tm_border()*\nWritten right underneath the tmap section for better flow.\n\n\nData classification methods of tmap\nThere are different ways to classify data into categories.\n\nPlotting choropleth maps with built-in classification methods\nHere we are using jenks, which means it looks for natural clusters in the data and then groups them together. And we are looking to divide the data into 5 natural clusters.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"jenks\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\nWe can also use equal, where the interval in data values is equal. But this will not be good when the data is skewed in one direction as there might not be any data points in many of the intervals.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"equal\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\nDIY (1)\nDIY: Using what you had learned, prepare choropleth maps by using different classification methods supported by tmap and compare their differences.\nLet’s try sd:\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"sd\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\nLet’s try quantile:\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"quantile\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\nI can see that quantile shows more of a geographic distribution. sd shows a lesser range- perhaps the sd was too huge. Hence, there were large number of data points per category and consequently, the number of categories was also less - showing less of a range.\n\n\nDIY (2)\nDIY: Preparing choropleth maps by using similar classification method but with different numbers of classes (i.e. 2, 6, 10, 20). Compare the output maps, what observation can you draw?\nLet’s try quantile.\nLet’s start with 6 classes.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 6,\n          style = \"quantile\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\nLet’s start with 6 classes.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 7,\n          style = \"quantile\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\nWe can see the map when the classes are 5,6 and 7. As it increases from 5 to 6, we get a greater nuance in the geographic distribution of the dependency column. However, when we go from 6 to 7, there is not much of an increase in nuance. The difference in category boundaries are very minimal. Hence, the ideal number of classes would be 6.\n\n\nPlotting choropleth maps with custom breaks\nIn the methods we used before, the breakpoints were set automatically by them. If we wanted to define the boundaries of each category ourselves, we can do that too. However, to do that, we need more information about the data to understand it. Getting the data’s mean/quantile values, minimum, median, can help with setting those boundaries.\nSo let’s get those values through the use of this method.\n\nsummary(mpsz_pop2020$DEPENDENCY)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n 0.1111  0.7147  0.7866  0.8585  0.8763 19.0000      92 \n\n\nNow let’s choose our breakpoints and plot the choropleth map.\nOur breakpoints are at 0, 0.6. 0.7, 0., 0.9, 1.00.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          breaks = c(0, 0.60, 0.70, 0.80, 0.90, 1.00)) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\nColor Scheme\nNow on to the fun part! We get to delve into the color scheme of things\n\nUsing ColourBrewer palette\nLet’s try the blue palette.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 6,\n          style = \"quantile\",\n          palette = \"Blues\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\nLet’s try the green palette.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          style = \"quantile\",\n          palette = \"-Greens\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\nMap Layouts\nWhen we talk about map layouts, we want to control how a map looks. We want to ensure that all aspects of a map are present. This includes the title, the scale bar, the compass, legend, margins and aspect ratios. We also can control the style of the map.\n\nMap Style\nThis refers to tmap_style(), not the style argument in your tm_fill where you declare the type of data classification you choose to employ.\nIt controls how your map would look like. These are the arguments you could use:\nThe available styles are: \"white\", \"gray\", \"natural\", \"cobalt\", \"col_blind\", \"albatross\", \"beaver\", \"bw\", \"classic\", \"watercolor\" \nIf we were to use classic:\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"-Greens\") +\n  tm_borders(alpha = 0.5) +\n  tmap_style(\"classic\")\n\n\n\n\nIf we used dark,\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"-Greens\") +\n  tm_borders(alpha = 0.5) +\n  tmap_style(\"cobalt\")\n\n\n\n\n\n\nMap Legend\nLet’s try adding the legend first.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"jenks\", \n          palette = \"Blues\", \n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(main.title = \"Distribution of Dependency Ratio by planning subzone \\n(Jenks classification)\",\n            main.title.position = \"center\",\n            main.title.size = 1,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            legend.outside = FALSE,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = FALSE) +\n  tm_borders(alpha = 0.5)\n\n\n\n\nLet’s go through what the legend part of the code means.\nlegend.hist= TRUE means that we are going to include a histogram like legend. This will allow the viewer to see the distribution of values in the form of a histogram as well.\nlegend.is.portrait = TRUE means that the legend/histogram will be in portrait orientation.\nlegend.hist.z=0.1 means the legend will be slightly behind the map.\n\n\nCartographic furniture\nThis refers to adding of the compass, scale bar and grid lines that we talked about earlier.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"No. of persons\") +\n  tm_layout(main.title = \"Distribution of Dependency Ratio \\nby planning subzone\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar(width = 0.15) +\n  tm_grid(lwd = 0.1, alpha = 0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\\n and Population data from Department of Statistics DOS\", \n             position = c(\"left\", \"bottom\"))\n\n\n\n\n\n\n\nDrawing Small Multiple Choropleth Maps\nSmall multiple maps, also referred to as facet maps, are composed of many maps arranged side-by-side or vertically. Small multiple maps enable the visualisation of how spatial relationships change with respect to another variable, such as time.\nIn tmap, small multiple maps can be plotted in three ways:\n\nby assigning multiple values to at least one of the asthetic arguments,\nby defining a group-by variable in tm_facets(), and\nby creating multiple stand-alone maps with tmap_arrange().\n\n\nBy assigning multiple values to at least one of the aesthetic arguments\nHere you want both to have the data classification of equal.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(c(\"YOUNG\", \"AGED\"),\n          style = \"equal\", \n          palette = \"Blues\") +\n  tm_layout(legend.position = c(\"right\", \"bottom\")) +\n  tm_borders(alpha = 0.5) +\n  tmap_style(\"white\")\n\n\n\n\nNow, you want each of them to have different data classifications, and different looks.\n\ntm_shape(mpsz_pop2020)+ \n  tm_polygons(c(\"DEPENDENCY\",\"AGED\"),\n          style = c(\"equal\", \"quantile\"), \n          palette = list(\"Blues\",\"Greens\")) +\n  tm_layout(legend.position = c(\"right\", \"bottom\"))\n\n\n\n\nIn these two sets of graphs (4), the two maps in each set are showcasing different variables across Singapore . In the first set, we show the distribution of young and then the distribution of old.\nNow what if we wanted the maps to be linked to each other, as in, show the distribution of the same variable?\n\n\nBy defining a group-by variable in tm_facets()\n\ntm_shape(mpsz_pop2020) +\n  tm_fill(\"DEPENDENCY\",\n          style = \"quantile\",\n          palette = \"Blues\",\n          thres.poly = 0) + \n  tm_facets(by=\"REGION_N\", \n            free.coords=TRUE, \n            drop.shapes=TRUE) +\n  tm_layout(legend.show = FALSE,\n            title.position = c(\"center\", \"center\"), \n            title.size = 20) +\n  tm_borders(alpha = 0.5)\n\n\n\n\nHere, we show the distribution of the same dependency variable across multiple regions.\n\n\nBy creating multiple stand-alone maps with tmap_arrange()\n\nyoungmap &lt;- tm_shape(mpsz_pop2020)+ \n  tm_polygons(\"YOUNG\", \n              style = \"quantile\", \n              palette = \"Blues\")\n\nagedmap &lt;- tm_shape(mpsz_pop2020)+ \n  tm_polygons(\"AGED\", \n              style = \"quantile\", \n              palette = \"Blues\")\n\ntmap_arrange(youngmap, agedmap, asp=1, ncol=2)\n\n\n\n\n\n\n\nMappping Spatial Object Meeting a Selection Criterion\nInstead of creating small multiple choropleth map, you can also use selection funtion to map spatial objects meeting the selection criterion.\n\ntm_shape(mpsz_pop2020[mpsz_pop2020$REGION_N==\"CENTRAL REGION\", ])+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\", \n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(legend.outside = TRUE,\n            legend.height = 0.45, \n            legend.width = 5.0,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = FALSE) +\n  tm_borders(alpha = 0.5)"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-On_Ex01.html",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-On_Ex01.html",
    "title": "Hands-On Exercise 1 : Geospatial Wrangling With R",
    "section": "",
    "text": "Hello! In this page, i will be describing how i performed data wrangling - which is basically cleaning and transforming raw data into a more structured and usable format or later analysis."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-On_Ex01.html#importing-polyline-feature-data-in-shapefile-form",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-On_Ex01.html#importing-polyline-feature-data-in-shapefile-form",
    "title": "Hands-On Exercise 1 : Geospatial Wrangling With R",
    "section": "Importing polyline feature data in shapefile form",
    "text": "Importing polyline feature data in shapefile form\n\nWhat is a polyline feature data frame?"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-On_Ex01.html#plotting-data",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-On_Ex01.html#plotting-data",
    "title": "Hands-On Exercise 1 : Geospatial Wrangling With R",
    "section": "Plotting Data",
    "text": "Plotting Data\nNow, let’s plot the data in our mpsz as a visualization. We use the plot() function for it.\n\nplot(mpsz)"
  },
  {
    "objectID": "In-class_Ex/In-class-Ex02/In-class_Ex02.html",
    "href": "In-class_Ex/In-class-Ex02/In-class_Ex02.html",
    "title": "In-class exercise 2 : R for geospatial data science",
    "section": "",
    "text": "Hey friends! Let’s learn about a few package:"
  },
  {
    "objectID": "In-class_Ex/In-class-Ex02/In-class_Ex02.html#loading-packages",
    "href": "In-class_Ex/In-class-Ex02/In-class_Ex02.html#loading-packages",
    "title": "In-class exercise 2 : R for geospatial data science",
    "section": "Loading packages",
    "text": "Loading packages\nThis is a very important step as we have to load the packages before we can use them.\n\npacman::p_load(sf, tmap, tidyverse,lubridate,arrow)"
  },
  {
    "objectID": "In-class_Ex/In-class-Ex02/In-class_Ex02.html#importing-the-data",
    "href": "In-class_Ex/In-class-Ex02/In-class_Ex02.html#importing-the-data",
    "title": "In-class exercise 2 : R for geospatial data science",
    "section": "Importing the data",
    "text": "Importing the data\n\nImporting Geospatial Data into R\nAs learnt in the previous page, we use st_read() function of the sf package to read in the data.\n\ndf &lt;- read_parquet(\"../../data/GrabPosisi/part-00000-8bbff892-97d2-4011-9961-703e38972569.c000.snappy.parquet\")\n\n\n\nConversion of data type\nHere we convert the int data type to date time data type. We are not using mutate, as we are over writing the data field in the file.\n\ndf$pingtimestamp &lt;-as_datetime(df$pingtimestamp)\n\n\n# get origin\norigin_df &lt;- df %&gt;% #use df\n  group_by(trj_id) %&gt;% #group according to trj_id\n  arrange(pingtimestamp) %&gt;% #sort according to timestamp asc (default)\n  filter(row_number()==1) %&gt;% #the first coordinate for every trip should be the origin\n  mutate(weekday = wday(pingtimestamp, label=TRUE, abbr=TRUE),\n         start_hr = factor(hour(pingtimestamp)),\n         day = factor(mday(pingtimestamp)))\n\neach trip will have multiple rows because every minute the new location one be sent to the server . so when we arrange it, the first row is the origin location.\n\n# get end\ndestination_df &lt;- df %&gt;% #use df\n  group_by(trj_id) %&gt;% #group according to trj_id\n  arrange(desc(pingtimestamp)) %&gt;% #sort according to timestamp asc (default)\n  filter(row_number()==1) %&gt;% #the first coordinate for every trip should be the origin\n  mutate(weekday = wday(pingtimestamp, label=TRUE, abbr=TRUE),\n         end_hr = factor(hour(pingtimestamp)),\n         day = factor(mday(pingtimestamp)))\n\n\n# he writes in rds if he still wants to write in r\nwrite_rds(origin_df, \"../../data/rds/origin_df.rds\")\nwrite_rds(destination_df,\n          \"../../data/rds/destination_df.rds\")\n\n#import data\n\norigin_df &lt;- read_rds(\"../../data/rds/origin_df.rds\")\ndestination_df &lt;- read_rds(\"../../data/rds/destination_df.rds\")"
  }
]