[
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex01/Take-Home_Ex01.html",
    "href": "Take-Home_Ex/Take-Home_Ex01/Take-Home_Ex01.html",
    "title": "Take-home Exercise 1 : Application of Spatial Point Patterns Analysis to discover the geographical distribution of Grab hailing services in Singapore",
    "section": "",
    "text": "Hello everyone! In tiny dense Singapore, movement of people is large and fast. Hence, in depth analysis of our movement can provide meaningful and actionable insights on our spatial-temporal behaviour.\nIn Singapore, we move by utilising a multitude of avenues such as buses, MRTs, LRTs and private hail services. In this exercise, I will be exploring the movement of our people using grab. We will be utilising spatial point analysis methods to discover the geographical and spatio-temporal distribution of Grab hailing service locations in Singaore."
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex01/Take-Home_Ex01.html#data-download",
    "href": "Take-Home_Ex/Take-Home_Ex01/Take-Home_Ex01.html#data-download",
    "title": "Take-home Exercise 1 : Application of Spatial Point Patterns Analysis to discover the geographical distribution of Grab hailing services in Singapore",
    "section": "2.1 Data Download",
    "text": "2.1 Data Download\nLet us first look at the data we are utilising to conduct this analysis.\n\nOverview Off Data Used\n\n\nData Name\nDescription\nType\nFormat\nSource\n\n\nGrab-Posisi\nData on where grab taxis are at a specific time, and at what speed.\nAspatial\n.parquet\nSource\n\n\nRoad data set\nData on the roads present in Singapore, Malaysia, Brunei.\nGeospatial\nESRI shapefile\nSource\n\n\nMaster Plan 2019 Subzone Boundary (No Sea)\nThis data provides the internal and external boundary in Singapore.\nGeospatial\n.kml\nSource"
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex01/Take-Home_Ex01.html#data-import",
    "href": "Take-Home_Ex/Take-Home_Ex01/Take-Home_Ex01.html#data-import",
    "title": "Take-home Exercise 1 : Application of Spatial Point Patterns Analysis to discover the geographical distribution of Grab hailing services in Singapore",
    "section": "2.2 Data Import",
    "text": "2.2 Data Import\n\nGrab-PosisiRoad Data SetMaster Plan 2019 Subzone Boundary\n\n\n\ngrab &lt;- read_parquet(\"../../data/GrabPosisi/part-00000-8bbff892-97d2-4011-9961-703e38972569.c000.snappy.parquet\")\n\n\n\n\nroads &lt;- st_read(dsn = \"../../data\",\n                   layer = \"gis_osm_roads_free_1\")\n\nReading layer `gis_osm_roads_free_1' from data source \n  `C:\\shaysnutss\\IS1455-GAA\\data' using driver `ESRI Shapefile'\nSimple feature collection with 1765176 features and 10 fields\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: 99.66041 ymin: 0.8021131 xmax: 119.2601 ymax: 7.514393\nGeodetic CRS:  WGS 84\n\n\n\n\n\nmpsz &lt;- st_read(dsn = \"../../data\",\n                   layer = \"MPSZ-2019\")\n\nReading layer `MPSZ-2019' from data source `C:\\shaysnutss\\IS1455-GAA\\data' using driver `ESRI Shapefile'\nSimple feature collection with 332 features and 6 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 103.6057 ymin: 1.158699 xmax: 104.0885 ymax: 1.470775\nGeodetic CRS:  WGS 84"
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex01/Take-Home_Ex01.html#data-wrangling",
    "href": "Take-Home_Ex/Take-Home_Ex01/Take-Home_Ex01.html#data-wrangling",
    "title": "Take-home Exercise 1 : Application of Spatial Point Patterns Analysis to discover the geographical distribution of Grab hailing services in Singapore",
    "section": "2.3 Data Wrangling",
    "text": "2.3 Data Wrangling\nWe now have our data imported into our R environment, means we can interact with it and use it!\nHowever, data often can’t be used as it is . We would have to change the data to fit our analysis needs. And that is what we will be doing in this section!\nWe will be doing it dataset by dataset:\n\n2.3.1 Master Plan 2019 Subzone Boundary\n\n2.3.1.1 Coordinate Reference System\nLet’s start with checking the coordinate reference system of the dataset.\n\nst_crs(mpsz)\n\nCoordinate Reference System:\n  User input: WGS 84 \n  wkt:\nGEOGCRS[\"WGS 84\",\n    DATUM[\"World Geodetic System 1984\",\n        ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n            LENGTHUNIT[\"metre\",1]]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"latitude\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"longitude\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    ID[\"EPSG\",4326]]\n\n\nWe know that the coordinate reference system is WGS84. We have to convert it to SVY21. This is because we want to minimise distortions and enable accurate area and distance measurements. The earth is curved, and the WGS84 reference system results in the distance between two points in the equator being different from the distance between the equally spaced points up in north america.\nSo let’s convert this to SVY21:\n\nmpsz &lt;- st_transform(mpsz, crs = 3414)\n\nLet’s check if it has been converted correctly\n\nst_crs(mpsz)\n\nCoordinate Reference System:\n  User input: EPSG:3414 \n  wkt:\nPROJCRS[\"SVY21 / Singapore TM\",\n    BASEGEOGCRS[\"SVY21\",\n        DATUM[\"SVY21\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4757]],\n    CONVERSION[\"Singapore Transverse Mercator\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"northing (N)\",north,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"easting (E)\",east,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Cadastre, engineering survey, topographic mapping.\"],\n        AREA[\"Singapore - onshore and offshore.\"],\n        BBOX[1.13,103.59,1.47,104.07]],\n    ID[\"EPSG\",3414]]\n\n\n\n\n2.3.1.2 Dropping Z coordinate\n\nmpsz &lt;- st_zm(mpsz)\n\n\n\n2.3.1.3 Geometry Validity\nI know we have dropped the Z coordinate, but we still want to check if the geometries we have are valid or if they have any errors. Some of the errors could be zero-area polygons or unclosed rings. Ensuring the data is valid and correcting it before analysis is crucial for an accurate and comprehensive analysis.\nSo this is how we check if the geometries are valid:\n\nlength(which(st_is_valid(mpsz) == FALSE))\n\n[1] 6\n\n\nOh no! We have 6 invalid geometries. Thank god we caught it!\nLet’s make it right using this function:\n\nmpsz &lt;- st_make_valid(mpsz)\nlength(which(st_is_valid(mpsz) == FALSE))\n\n[1] 0\n\n\n\n\n2.3.1.4 Missing values\nWe want to check if there are any empty rows. Datasets can be huge and consequently, we could have many empty rows that is just a waste of memory and processing power. Removing them would be useful.\nHere’s how we do it:\n\n# Use the filter function to check for empty rows\nempty_rows &lt;- mpsz %&gt;%\n  filter_all(all_vars(is.na(.)))\n\n# Check if there are any empty rows\nif (nrow(empty_rows) &gt; 0) {\n  cat(\"There are empty rows in the sf data tibble.\\n\")\n} else {\n  cat(\"There are no empty rows in the sf data tibble.\\n\")\n}\n\nThere are no empty rows in the sf data tibble.\n\n\nWe know that there are no empty rows from running this so we do not need to remove anything.\n\n\n2.3.1.5 mpsz check-in\nLet’s now check the properties of mpsz before moving on:\n\nmpsz\n\nSimple feature collection with 332 features and 6 fields\nGeometry type: GEOMETRY\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21 / Singapore TM\nFirst 10 features:\n                 SUBZONE_N SUBZONE_C       PLN_AREA_N PLN_AREA_C       REGION_N\n1              MARINA EAST    MESZ01      MARINA EAST         ME CENTRAL REGION\n2         INSTITUTION HILL    RVSZ05     RIVER VALLEY         RV CENTRAL REGION\n3           ROBERTSON QUAY    SRSZ01  SINGAPORE RIVER         SR CENTRAL REGION\n4  JURONG ISLAND AND BUKOM    WISZ01  WESTERN ISLANDS         WI    WEST REGION\n5             FORT CANNING    MUSZ02           MUSEUM         MU CENTRAL REGION\n6         MARINA EAST (MP)    MPSZ05    MARINE PARADE         MP CENTRAL REGION\n7                   SUDONG    WISZ03  WESTERN ISLANDS         WI    WEST REGION\n8                  SEMAKAU    WISZ02  WESTERN ISLANDS         WI    WEST REGION\n9           SOUTHERN GROUP    SISZ02 SOUTHERN ISLANDS         SI CENTRAL REGION\n10                 SENTOSA    SISZ01 SOUTHERN ISLANDS         SI CENTRAL REGION\n   REGION_C                       geometry\n1        CR POLYGON ((33222.98 29588.13...\n2        CR POLYGON ((28481.45 30886.22...\n3        CR POLYGON ((28087.34 30541, 2...\n4        WR MULTIPOLYGON (((13919 22120...\n5        CR POLYGON ((29542.53 31041.2,...\n6        CR POLYGON ((35279.55 30886.05...\n7        WR MULTIPOLYGON (((17759.05 15...\n8        WR MULTIPOLYGON (((19713.94 19...\n9        CR MULTIPOLYGON (((28298.9 217...\n10       CR MULTIPOLYGON (((27524.76 24...\n\n\n\nclass(mpsz)\n\n[1] \"sf\"         \"data.frame\"\n\n\nWe know that mpsz now is a sf tibble data frame with 332 features and 2 fields with SVY21 as its coordinate reference system.\nLet’s plot it to see it visually also!\n\nplot(mpsz)\n\n\n\n\n\n\n2.3.1.6 Removal of internal boundaries\nIn this analysis, we also want just the singapore boundary so that we can see what grab taxis are within Singapore. We might not need all the internal breakup and boundaries and constantly using that might clog up processing power.\nHowever, there might be use cases where we need the mpsz object in sf type. So let’s save it first by assigning it to another variable:\n\nmpsz_sf &lt;- mpsz\n\nNow let’s remove the interal boudaries:\n\nmpsz &lt;- mpsz %&gt;%\n  st_union()\n\nLet’s plot mpsz now to see the external boundary:\n\nplot(mpsz)\n\n\n\n\nYay! We have got it.\n\n\n\n2.3.2 Road Data Set\n\n2.3.2.1 Coordinate Reference System\nLet us check the coordinate reference system.\n\nst_geometry(roads)\n\nGeometry set for 1765176 features \nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: 99.66041 ymin: 0.8021131 xmax: 119.2601 ymax: 7.514393\nGeodetic CRS:  WGS 84\nFirst 5 geometries:\n\n\nWe can see it is not in EPSG:3414, therefore, we do a transformation to assign it the right code. We have explained why we do this under the mpsz section.\n\nroads &lt;- st_transform(roads, \n                              crs = 3414)\n\n\n\n2.3.2.2 Constriction of roads to Singapore\nThe data of the roads is related to roads in multiple countries on top of Singapore. Let us constrict it to just to have roads in Singapore as Singapore is our area of study.\nBesides, this will reduce the amount of data tremedously and allow the processing to be faster.\n\nroads_inSG &lt;- st_intersection(roads, mpsz)\n\n\n\n2.3.2.3 Validity Of Geometries\n\nlength(which(st_is_valid(roads_inSG) == FALSE))\n\n[1] 0\n\n\nLuckily, all geometries are validity!\n\n\n2.3.2.4 Missing Values\n\n# Use the filter function to check for empty rows\nempty_rows &lt;- roads_inSG %&gt;%\n  filter_all(all_vars(is.na(.)))\n\n# Check if there are any empty rows\nif (nrow(empty_rows) &gt; 0) {\n  cat(\"There are empty rows in the sf data tibble.\\n\")\n} else {\n  cat(\"There are no empty rows in the sf data tibble.\\n\")\n}\n\nThere are no empty rows in the sf data tibble.\n\n\nLuckily, there are no empty values again!\n\n\n\n2.3.3 Grab Posisi\nNow let’s wrangle the data of Grab posisi.\n\n2.3.3.1 Convert Data type\nHere we convert the int data type to date time data type. We are not using mutate, as we are over writing the data field in the file.\n\ngrab$pingtimestamp &lt;-as_datetime(grab$pingtimestamp)\n\n\n\n2.3.3.2 Origin Datapoints of Each Ride\nIn this data, the location and specifics of each car ride is sent to the server every minute. Here, we only want the data for the origin point for each ride. So let’s wrangle the data this way:\n\n#origin\ngrabOrigin &lt;- grab %&gt;% #use df\n  group_by(trj_id) %&gt;% #group according to trj_id\n  arrange(pingtimestamp) %&gt;% #sort according to timestamp asc (default)\n  filter(row_number()==1) %&gt;% #the first coordinate for every trip should be the origin\n  mutate(weekday = wday(pingtimestamp, label=TRUE, abbr=TRUE),\n         start_hr = factor(hour(pingtimestamp)),\n         day = factor(mday(pingtimestamp)))\n\n\n2.3.3.2.1 Code Walkthrough\nLet me explain step by step so you understand this better!\ngrabOrigin &lt;- grab\nHere we take the data from the grab data and wrangle it and assign it to grabOrigin.\ngroup_by(trj_id)\nWhen we use group_by, we can group the different data records in the data by one column, in this case, trj_id. Here, each group will have rows with the same trj_id, meaning all of the rows in one group will relate to one trip.\narrange(pingtimestamp)\nHere in each group, we arrange all the rows according to the time stamp in an ascending order. Hence, the first row will be the origin location the grab taxi departs from.\nfilter(row_number()==1)\nFilter method is used to filter out the rows that match a criteria.\nHere, we take the first row only - only the origin location the taxi departs from matters.\nmutate(weekday = wday(pingtimestamp, label=TRUE, abbr=TRUE)\nMutate function can be used to create a new column. In this case, we create a column called weekday. The wday function extracts the weekday values from pingtimestamp. We fill the weekday column with the value generated by the wday function.\nstart_hr = factor(hour(pingtimestamp))\nHere, we create a column called start_hr. We use a function called hour to extract the hour from the pingtimestamp and then place it into the start_hr column.\nday = factor(mday(pingtimestamp)))\nHere, we create a column called day. We use a function called mday to extract the day of the month from the pingtimestamp and then place it into the day column.\nI hope the code is clearer now, and you understand how we get the origin data points of each ride.\n\n\n\n2.3.3.3 Destination Datapoints of Each Ride\nLet’s do almost the same to get the destination of each trip.\n\n# get end\ndestination_df &lt;- grab %&gt;% #use df\n  group_by(trj_id) %&gt;% #group according to trj_id\n  arrange(desc(pingtimestamp)) %&gt;% #sort according to timestamp asc (default)\n  filter(row_number()==1) %&gt;% #the first coordinate for every trip should be the origin\n  mutate(weekday = wday(pingtimestamp, label=TRUE, abbr=TRUE),\n         end_hr = factor(hour(pingtimestamp)),\n         day = factor(mday(pingtimestamp)))\n\nThe code is mostly the same except the arrange(desc()) where we arranged the rows descendingly\n\n\n2.3.3.4 Coordinate reference frame\n\ngrabOrigin &lt;- st_as_sf(grabOrigin, coords = c(\"rawlng\", \"rawlat\"), crs = 4326) %&gt;%\n  st_transform(crs = 3414)\n\n\ndestination_df &lt;- st_as_sf(destination_df, coords = c(\"rawlng\", \"rawlat\"), crs = 4326) %&gt;%\n  st_transform(crs = 3414)\n\nAgain, we transform the coordinate reference system into SVY21.\nLet’s check if it’s been transformed correctly.\n\nst_geometry(grabOrigin) \n\nGeometry set for 28000 features \nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 3661.475 ymin: 25201.14 xmax: 49845.23 ymax: 49685.08\nProjected CRS: SVY21 / Singapore TM\nFirst 5 geometries:\n\n\n\nst_geometry(destination_df)\n\nGeometry set for 28000 features \nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 3638.685 ymin: 25350.05 xmax: 50024.92 ymax: 49469.41\nProjected CRS: SVY21 / Singapore TM\nFirst 5 geometries:\n\n\nAnd both have been transformed well!\n\n\n2.3.3.5 Validity Of Geometries\n\nlength(which(st_is_valid(grabOrigin) == FALSE))\n\n[1] 0\n\n\n\nlength(which(st_is_valid(destination_df) == FALSE))\n\n[1] 0\n\n\nLuckily all geometries are valid!\n\n\n2.3.3.6 Drop Z coordinate\n\ngrabOrigin &lt;- st_zm(grabOrigin)\n\n\ndestination_df &lt;- st_zm(destination_df)\n\n\n\n2.3.3.7 Create ppp objects\n\ngrabOriginPPP &lt;- as.ppp(grabOrigin)\n\n\ndestination_dfPP &lt;- as.ppp(destination_df)\n\n\nplot(grabOriginPPP)\n\n\n\n\nLet’s look at the summary statistics of the newly created ppp object\n\nsummary(grabOriginPPP)\n\nMarked planar point pattern:  28000 points\nAverage intensity 2.47621e-05 points per square unit\n\nCoordinates are given to 3 decimal places\ni.e. rounded to the nearest multiple of 0.001 units\n\nmarks are of type 'character'\nSummary:\n   Length     Class      Mode \n    28000 character character \n\nWindow: rectangle = [3661.47, 49845.23] x [25201.14, 49685.08] units\n                    (46180 x 24480 units)\nWindow area = 1130760000 square units\n\n\n\nplot(destination_dfPP)\n\n\n\n\nLet’s look at the summary statistics of the newly created ppp object\n\nsummary(destination_dfPP)\n\nMarked planar point pattern:  28000 points\nAverage intensity 2.502667e-05 points per square unit\n\nCoordinates are given to 3 decimal places\ni.e. rounded to the nearest multiple of 0.001 units\n\nmarks are of type 'character'\nSummary:\n   Length     Class      Mode \n    28000 character character \n\nWindow: rectangle = [3638.69, 50024.92] x [25350.05, 49469.41] units\n                    (46390 x 24120 units)\nWindow area = 1118810000 square units\n\n\n\n\n2.3.3.8 Handling duplicate points\nWe are looking at a huge area, and having multiple points in one area might make us miss important details.\nSo let us check if there are duplicate points.\n\nany(duplicated(grabOrigin))\n\n[1] FALSE\n\n\n\nany(duplicated(destination_df))\n\n[1] FALSE\n\n\n\n\n2.3.3.9 Conversion To Owin Objects\nWe do this to define the region we can do spatial point analysis in. Let’s take a refresher - the points are the locations of our grab taxis. So, in this case, we want to define Singapore as the region we execute spatial point analysis in.\n\nmpsz_owin &lt;- as.owin(mpsz)\n\n\ngrabOriginPPP_SG &lt;- grabOriginPPP[mpsz_owin]\n\n\ndestination_dfPPP_SG &lt;- destination_dfPP[mpsz_owin]"
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex01/Take-Home_Ex01.html#location-visualizations",
    "href": "Take-Home_Ex/Take-Home_Ex01/Take-Home_Ex01.html#location-visualizations",
    "title": "Take-home Exercise 1 : Application of Spatial Point Patterns Analysis to discover the geographical distribution of Grab hailing services in Singapore",
    "section": "3.1 Location Visualizations",
    "text": "3.1 Location Visualizations\nHere we visualise the origin and destination locations of grab to understand them better!\n\n3.1.1 Grab origin in Singapore\nWe can see where the grabs rides generally start off from in Singapore.\n\ntmap_mode(\"plot\") \n    tm_shape(mpsz_sf) +\n    tm_polygons() +\n    tm_shape(grabOrigin) +\n    tm_dots()\n\n\n\n\n\n\n3.1.2 Grab destination in Singapore\nHere, we can see where the grab rides generally end in Singapore.\n\ntmap_mode(\"plot\") \n    tm_shape(mpsz_sf) +\n    tm_polygons() +\n    tm_shape(destination_df) +\n    tm_dots()"
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex01/Take-Home_Ex01.html#spatial-analysis",
    "href": "Take-Home_Ex/Take-Home_Ex01/Take-Home_Ex01.html#spatial-analysis",
    "title": "Take-home Exercise 1 : Application of Spatial Point Patterns Analysis to discover the geographical distribution of Grab hailing services in Singapore",
    "section": "3.2 Spatial Analysis",
    "text": "3.2 Spatial Analysis\n\n3.2.1 Net outflow from northern Singapore\n\n3.2.1.1 Method 1 of analysis\nThough not conclusive, from staring at the before and after pictures, we can see that there are lesser dots in the extreme east in the destination map than the origin map.\nLet’s prove this through code!\nThis was not done much in class and is new, but good for exploration!\nFirstly, get’s create a bounding box for the northern part of Singapore to extract the northern part of Singapore as we want to study it.\nLet’s extract the north region in Singapore\n\nnorth &lt;- mpsz_sf %&gt;% \n  filter(REGION_N == \"NORTH REGION\")\n\nLet’s see what “north” of Singapore looks like:\n\nplot(north)\n\n\n\n\n\nplot(north)\n\n\n\n\nNow let’s see the number of grab cars that are leaving the north:\n\ncarsOriginInNorth &lt;- st_intersection(grabOrigin, north)\n\n\ncarsOriginInNorth &lt;- st_intersection(grabOrigin, north)\n\nLet’s get the number now\n\ncarsOriginInNorth\n\nSimple feature collection with 4547 features and 16 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 13681.67 ymin: 35382.98 xmax: 31339.97 ymax: 49685.08\nProjected CRS: SVY21 / Singapore TM\n# A tibble: 4,547 × 17\n   trj_id driving_mode osname pingtimestamp       speed bearing accuracy weekday\n * &lt;chr&gt;  &lt;chr&gt;        &lt;chr&gt;  &lt;dttm&gt;              &lt;dbl&gt;   &lt;int&gt;    &lt;dbl&gt; &lt;ord&gt;  \n 1 14555  car          ios    2019-04-10 09:15:04 10.6      107     10   Wed    \n 2 79752  car          ios    2019-04-10 10:17:35  7.52      99      6   Wed    \n 3 77697  car          ios    2019-04-11 04:44:09  2.12     143     12   Thu    \n 4 10587  car          ios    2019-04-11 10:23:59  6.42     101      8   Thu    \n 5 45173  car          andro… 2019-04-11 13:08:29 12.0       85      3.9 Thu    \n 6 76870  car          ios    2019-04-12 06:19:28  7.31      96      6   Fri    \n 7 79546  car          andro… 2019-04-12 07:22:33 17.0       83      3.9 Fri    \n 8 66711  car          ios    2019-04-12 08:10:38 18.3       12      8   Fri    \n 9 35891  car          ios    2019-04-13 03:35:23  2.71     354      5   Sat    \n10 76149  car          andro… 2019-04-13 10:05:54 17.6       86      3.9 Sat    \n# ℹ 4,537 more rows\n# ℹ 9 more variables: start_hr &lt;fct&gt;, day &lt;fct&gt;, SUBZONE_N &lt;chr&gt;,\n#   SUBZONE_C &lt;chr&gt;, PLN_AREA_N &lt;chr&gt;, PLN_AREA_C &lt;chr&gt;, REGION_N &lt;chr&gt;,\n#   REGION_C &lt;chr&gt;, geometry &lt;POINT [m]&gt;\n\n\nWe can see 4547 cars are leaving the North\n\ntmap_mode(\"plot\") \n    tm_shape(north) +\n    tm_polygons() +\n    tm_shape(carsOriginInNorth) +\n    tm_dots()\n\n\n\n\nNow, let’s see the number of cars arriving to the north:\n\ncarsDestInNorth &lt;- st_intersection(destination_df, north)\n\nLet’s get the number now:\n\ncarsDestInNorth\n\nSimple feature collection with 4242 features and 16 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 13369.5 ymin: 35350.07 xmax: 31403.56 ymax: 49469.41\nProjected CRS: SVY21 / Singapore TM\n# A tibble: 4,242 × 17\n   trj_id driving_mode osname pingtimestamp       speed bearing accuracy weekday\n * &lt;chr&gt;  &lt;chr&gt;        &lt;chr&gt;  &lt;dttm&gt;              &lt;dbl&gt;   &lt;int&gt;    &lt;dbl&gt; &lt;ord&gt;  \n 1 69056  car          andro… 2019-04-21 05:02:36 13.2      348      3.9 Sun    \n 2 49387  car          ios    2019-04-21 04:21:53  4.07     283      8   Sun    \n 3 47116  car          ios    2019-04-20 10:42:37 14.4      280     10   Sat    \n 4 68012  car          andro… 2019-04-20 08:27:09 14.2      266      5   Sat    \n 5 10894  car          andro… 2019-04-18 11:50:43  0          0      3   Thu    \n 6 74071  car          andro… 2019-04-18 10:58:28 15.7      348      3.9 Thu    \n 7 9325   car          ios    2019-04-15 02:47:36  7.37     349      5   Mon    \n 8 80203  car          andro… 2019-04-11 18:29:44 17.3        3      4   Thu    \n 9 76951  car          ios    2019-04-21 23:47:07 10.3       26      6   Sun    \n10 19847  car          andro… 2019-04-21 23:40:18  8.20      26     11   Sun    \n# ℹ 4,232 more rows\n# ℹ 9 more variables: end_hr &lt;fct&gt;, day &lt;fct&gt;, SUBZONE_N &lt;chr&gt;,\n#   SUBZONE_C &lt;chr&gt;, PLN_AREA_N &lt;chr&gt;, PLN_AREA_C &lt;chr&gt;, REGION_N &lt;chr&gt;,\n#   REGION_C &lt;chr&gt;, geometry &lt;POINT [m]&gt;\n\n\nWe can see that 4242 cars are leaving the North\nLet’s see visually how it looks like:\n\ntmap_mode(\"plot\") \n    tm_shape(north) +\n    tm_polygons() +\n    tm_shape(carsDestInNorth) +\n    tm_dots()\n\n\n\n\nThough it may not be clear visually, we can see that more cars are leaving the North (4547) than they are going to the North (4242). That makes sense as the North is largely a residential area, and people generally live there more than work there. However, we notice that the difference isn’t too great, and that could be attributed to the fact that the north has slowly been developing many industrial estates that people might work at.\nOne issue with the data above is that there is a huge portion of area that is Considered “North” is not actually a region considered to be the north.\nRefer to the picture below:\n\nThis area is the central catchment area, with parks and large bodies of water. When thinking of “North Of Singapore”, people generally do not consider the central catchment area as such. And nor is it a place people generally work and live at.\nSo let’s do a second round of analysis and remove it.\n\n\n3.2.1.2 Method 2 of analysis\nFor this part, I have used this data : Master Plan 2019 Subzone Boundary (No Sea) from Data.gov.sg. I have not shown the import of wrangling of this data on this page, due to the lack of memory and processing space. However, you can wrangle this data the same way I have wrangled the current mpsz data!\nFirstly, get’s create a bounding box for the northern part of Singapore to extract the northern part of Singapore as we want to study it.\nThese coordinates are not random. I have found them on google maps by selecting points and some trial and error to correctly extract the northern area.\n\nNow, let’s convert the coordinate reference system of the bounding box to match that of mpsz_sf which is SVY21. After that, we will crop Singapore to just the area specified by the bounding box.\n\n\nAnd we have!\nNow, let’s see the intersection of grab locations when leaving the northern Singapore.\n\n\nHere we can see that our hypothesis is indeed correct. The number of cars starting at north was 3681 while the number of cars arriving at the north was 2838. The gap between the people leaving the North to coming to the north is much greater when i reduced the study area to areas actually relevent to peoplei n the north.\nThis aligns with common knowledge that the northern parts of Singapore are more residential, and in the mornings, people generally tend to leave the northern areas where they live to go to other parts of Singapore where they work."
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex01/Take-Home_Ex01.html#computing-kde-with-automatic-bandwidth",
    "href": "Take-Home_Ex/Take-Home_Ex01/Take-Home_Ex01.html#computing-kde-with-automatic-bandwidth",
    "title": "Take-home Exercise 1 : Application of Spatial Point Patterns Analysis to discover the geographical distribution of Grab hailing services in Singapore",
    "section": "3.1 Computing KDE with automatic bandwidth",
    "text": "3.1 Computing KDE with automatic bandwidth\nWe use automatic bandwidths in this section, where the base of each hill is of different size. Automatic bandwidths can be good in areas where points are really clustered to one another. Hence, sublocations with more points can have smaller based hills to detect small nuanced changes in density. This detection might not be possible if all hills had the same sized hill bases, as the tiny differences in density can be missed out.\n\n3.1.1 Computing KDE with automatic bandwidth and gaussian kernel\nThere are different ways to calculate the automatic bandwidth. Let’s try some of them/\n\n3.1.1.1 Using bw.diggle()\nHere we calculate the density of grabs that are just leaving their location.\n\ngrabOriginPPP_SG_km &lt;- rescale(grabOriginPPP_SG, 1000, \"km\")\n\nkde_grabOriginSG_bw_gaus &lt;- density(grabOriginPPP_SG_km,\n                              sigma=bw.diggle,\n                              edge=TRUE,\n                            kernel=\"gaussian\") \n\n\npar(mfrow = c(1, 1), mar = c(1, 1, 1, 1))\n\nplot(kde_grabOriginSG_bw_gaus)\n\n\n\n\nHere we calculate the density of grabs that are just arriving to their location.\n\ngrabDestinationPPP_SG &lt;- rescale(destination_dfPPP_SG, 1000, \"km\")\nkde_grabDestSG_bw_gaus &lt;- density(grabDestinationPPP_SG,\n                              sigma=bw.diggle,\n                              edge=TRUE,\n                            kernel=\"gaussian\")\n\n\npar(mfrow = c(1, 1), mar = c(1, 1, 2, 2))\nplot(kde_grabDestSG_bw_gaus)\n\n\n\n\nThe southern part of Singapore is more dense with grab cars as their destination, than their origin. This means that more cars come to the southern part of Singapore than they leave, and that makes sense as the CBD is located there, and a lot of workplaces are situated there. Many people would come to the CBD to work daily, leading to the high densities.\n\n\n3.1.1.1 Using bw.CvL()\n\nbw.CvL(grabOriginPPP_SG)\n\n   sigma \n6801.213 \n\n\n\n\n3.1.1.2 Using bw.scott()\n\nbw.scott(grabOriginPPP_SG) \n\n  sigma.x   sigma.y \n1587.8068  935.0244 \n\n\n\n\n3.1.1.3 Using bw.ppl()\n\nbw.ppl(grabOriginPPP_SG)\n\n   sigma \n382.9118 \n\n\n\nkde_grabOriginSG_ppl_gaus &lt;-  density(grabOriginPPP_SG_km, sigma=bw.ppl, edge=TRUE, kernel=\"gaussian\")\n\n\npar(mfrow = c(1, 1), mar = c(1, 1, 1, 1))\nplot(kde_grabOriginSG_bw_gaus, main = \"bw.diggle\")\n\n\n\nplot(kde_grabOriginSG_ppl_gaus, main = \"bw.ppl\")\n\n\n\n\nWe can see that the densities are more clearly marked when using bw.ppl than when using bw.diggle. This is because the bandwidth, or the base of the hill is much smaller. Hence, the nuances and differences in densities can be more easily picked up.\nBaddeley et. (2016) suggested the use of the bw.ppl() algorithm because in ther experience it tends to produce the more appropriate values when the pattern consists predominantly of tight clusters. \n\n\n\n3.1.2 Computing KDE with automatic bandwidth and various kernels\nAS mentioned before, when we use various kernels, we are using different methods to smoothen out the hill to ensure that the differences between two hills aren’t too sharp.\nThe quartic kernel makes a flat disc-shaped hill, only considering points close to each other.\n\npar(mfrow = c(1, 1), mar = c(1, 1, 1, 1))\n\nplot(density(grabOriginPPP_SG_km, \n             sigma=bw.ppl, \n             edge=TRUE, \n             kernel=\"epanechnikov\"), \n     main=\"Epanechnikov\")\n\n\n\n\nThe quartic kernel is similar to epanechnikov the but makes wider hills.\n\npar(mfrow = c(1, 1), mar = c(1, 1, 1, 1))\n\nplot(density(grabOriginPPP_SG_km,\n             sigma=bw.ppl, \n             edge=TRUE, \n             kernel=\"quartic\"), \n     main=\"Quartic\")\n\n\n\n\nThe Gaussian kernel is smooth and assigns higher weights to nearby points.\n\npar(mfrow = c(1, 1), mar = c(1, 1, 1, 1))\n\n\n\nplot(density(grabOriginPPP_SG_km, \n             sigma=bw.ppl, \n             edge=TRUE, \n             kernel=\"gaussian\"), \n     main=\"Gaussian\")\n\n\n\n\nhe Disc kernel treats all points within a certain distance equally, making a simple, flat circle around each point.\n\npar(mfrow = c(1, 1), mar = c(1, 1, 1, 1))\nplot(density(grabOriginPPP_SG_km, \n             sigma=bw.ppl, \n             edge=TRUE, \n             kernel=\"disc\"), \n     main=\"Disc\")"
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex01/Take-Home_Ex01.html#comparing-spatial-point-patterns-using-kde",
    "href": "Take-Home_Ex/Take-Home_Ex01/Take-Home_Ex01.html#comparing-spatial-point-patterns-using-kde",
    "title": "Take-home Exercise 1 : Application of Spatial Point Patterns Analysis to discover the geographical distribution of Grab hailing services in Singapore",
    "section": "3.2 Comparing Spatial Point Patterns using KDE",
    "text": "3.2 Comparing Spatial Point Patterns using KDE\n\n3.2.1 Extracting study areas\nFrom our analysis before, we could see that the southern part of Singapore was highly populated due to the CBD and numerous other workplaces. However, we could only see a high level picture of it, now, let’s dive a little deeper and see which areas in the southern part of Singapore were more densely populated with grab cars then the others.\nBy analysing this, we can somewhat deduce which areas have more workplaces than others!\n\ndowntown_core &lt;- mpsz_sf %&gt;% \n  filter(PLN_AREA_N == \"DOWNTOWN CORE\")\nriver_valley &lt;- mpsz_sf %&gt;% \n  filter(PLN_AREA_N == \"RIVER VALLEY\")\norchard &lt;- mpsz_sf %&gt;% \n  filter(PLN_AREA_N == \"ORCHARD\")\n\n\nplot(st_geometry(downtown_core), main = \"downtown_core\")\n\n\n\n\n\npar(mfrow = c(1, 1), mar = c(1, 1, 1, 1))\nplot(st_geometry(river_valley), main = \"river_valley\")\n\n\n\n\n\npar(mfrow = c(1, 1), mar = c(1, 1, 1, 1))\nplot(st_geometry(orchard), main = \"orchard\")\n\n\n\n\n\n\n3.2.2 Combining grab points and the study areas\n\n3.2.2.1 Create Owin Object\n\ndowntown_core &lt;- as.owin(downtown_core)\n\n\norchard &lt;- as.owin(orchard)\n\n\nriver_valley &lt;- as.owin(river_valley)\n\n\n\n3.2.2.2 Combination\nHere , we are deriving the points that would be located in each of the locations.\n\ngrabDestinationPPP_downtown &lt;- destination_dfPP[downtown_core]\ngrabDestinationPPP_orchard &lt;- destination_dfPP[orchard]\ngrabDestinationPPP_river &lt;- destination_dfPP[river_valley]\n\nLet’s rescale it, as the default was in meters.\n\ngrabDestinationPPP_downtown = rescale(grabDestinationPPP_downtown, 1000, \"km\")\ngrabDestinationPPP_orchard = rescale(grabDestinationPPP_orchard, 1000, \"km\")\ngrabDestinationPPP_river = rescale(grabDestinationPPP_river, 1000, \"km\")\n\n\n\n3.2.2.3 Plot points\nHere, we just plot the points of grab cars arriving. Later, we will plot the KDE.\nHere we plot grab cars arriving in downtown.\n\npar(mfrow = c(1, 1), mar = c(1, 1, 1, 1))\nplot(grabDestinationPPP_downtown)\n\n\n\n\nHere we plot grab cars arriving in orchard.\n\npar(mfrow = c(1, 1), mar = c(1, 1, 1, 1))\nplot(grabDestinationPPP_orchard)\n\n\n\n\nHere we plot grab cars arriving in river valley.\n\npar(mfrow = c(1, 1), mar = c(1, 1, 1, 1))\nplot(grabDestinationPPP_river)\n\n\n\n\nWhen conducting analysis in a more granular way, we get to see two things\n\nThe differences in grab cars , and by extension workplaces, between different areas in southern Singapore\n\nWe can see from these images that downtown is the more popular location in all of the southern parts of singapore that we have analysed.\n\nWe see the distribution of grab cars within each area itself, telling us which locations are more popular, or have drop offs.\n\nFor example, in Orchard, we can see that the borders of Orchard are more popular.\n\n\n\n\n\n3.2.3 KDE in central areas\nNow let’s compute and plot the KDE in different areas in southern Singapore. This can give us a better idea of the distribution of grab car destinations.\nLet’s calculate KDE for downtown.\n\npar(mfrow = c(1, 1), mar = c(1, 1, 1, 1))\nplot(density(grabDestinationPPP_downtown, \n             sigma=bw.diggle, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Downtown\")\n\n\n\n\nLet’s calculate KDE for orchard.\n\npar(mfrow = c(1, 1), mar = c(1, 1, 1, 1))\nplot(density(grabDestinationPPP_orchard, \n             sigma=bw.diggle, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Orchard\")\n\n\n\n\nLet’s calculate KDE for river valley.\n\npar(mfrow = c(1, 1), mar = c(1, 1, 1, 1))\nplot(density(grabDestinationPPP_river, \n             sigma=bw.diggle, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"River Valley\")\n\n\n\n\n#from here\n\n\n3.2.4 Converting KDE output into raster format\nWhy do we do this?\nThis is mainly just for mapping purposes. Conversion into raster format allows us to visualise the KDE in a pixelated format, allowing us to better understand spatial point patterns.\n\nkde_grabOriginSG_ppl_gaus_conv &lt;- as.SpatialGridDataFrame.im(kde_grabOriginSG_ppl_gaus) \n\n\nspplot(kde_grabOriginSG_ppl_gaus_conv)\n\n\n\n\nNext, we will convert the gridded kernal density objects into RasterLayer object by using raster() of raster package.\n\nkde_grabOriginSG_ppl_gaus_conv &lt;- raster(kde_grabOriginSG_ppl_gaus_conv)\n\nLet’s look at the gridded kernel desnity object\n\nkde_grabOriginSG_ppl_gaus_conv\n\nclass      : RasterLayer \ndimensions : 128, 128, 16384  (nrow, ncol, ncell)\nresolution : 0.419757, 0.2695907  (x, y)\nextent     : 2.667538, 56.39644, 15.74872, 50.25633  (xmin, xmax, ymin, ymax)\ncrs        : NA \nsource     : memory\nnames      : v \nvalues     : -8.533748e-14, 845.4849  (min, max)\n\n\nNotice that the CRS is NA so lets’ set it to SVY21. Remember, we convert everything to the same coordinate reference system!\n\nprojection(kde_grabOriginSG_ppl_gaus_conv) &lt;- CRS(\"+init=EPSG:3414\")\n\n\n\n3.2.5 Displaying KDE on open streetmap of Singapore\n\ntm_basemap(\"OpenStreetMap\") +\ntm_shape(kde_grabOriginSG_ppl_gaus_conv) +\n  tm_raster(\"v\", palette = \"PuRd\", alpha=0.65) + \n  tm_layout(legend.position = c(\"right\", \"bottom\"), \n            main.title = \"Grab's Origin KDE \",\n            frame = FALSE)\n\n\n\n\nFrom this, we know that the areas we could be more interested in are the East. the sourthern part of Singapore and Northern part of Singapore. We see that the densities are higher there."
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex01/Take-Home_Ex01.html#extracting-study-areas-1",
    "href": "Take-Home_Ex/Take-Home_Ex01/Take-Home_Ex01.html#extracting-study-areas-1",
    "title": "Take-home Exercise 1 : Application of Spatial Point Patterns Analysis to discover the geographical distribution of Grab hailing services in Singapore",
    "section": "4.1 Extracting study areas",
    "text": "4.1 Extracting study areas\nAs mentioned before there are only three areas we are concerned with ar the moment. This is also better for processing and memory power and we do not have to load and process the entire roads dataset.\n\n4.1.1 Extracting the North\n\n4.1.1.1 Extracting the North from mpsz\nHere, we are extracting the north from mpsz. Thankfully we already have it from previous processing. Let’s print it.\n\nnorth\n\nSimple feature collection with 41 features and 6 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 12386.29 ymin: 35330.11 xmax: 32377.19 ymax: 50256.33\nProjected CRS: SVY21 / Singapore TM\nFirst 10 features:\n                 SUBZONE_N SUBZONE_C              PLN_AREA_N PLN_AREA_C\n1                 PANG SUA    SKSZ04            SUNGEI KADUT         SK\n2                   KHATIB    YSSZ08                  YISHUN         YS\n3              MANDAI WEST    MDSZ01                  MANDAI         MD\n4             YISHUN SOUTH    YSSZ04                  YISHUN         YS\n5            LOWER SELETAR    YSSZ05                  YISHUN         YS\n6            MANDAI ESTATE    MDSZ03                  MANDAI         MD\n7           YISHUN CENTRAL    YSSZ01                  YISHUN         YS\n8                GALI BATU    SKSZ03            SUNGEI KADUT         SK\n9               SPRINGLEAF    YSSZ06                  YISHUN         YS\n10 CENTRAL WATER CATCHMENT    CCSZ01 CENTRAL WATER CATCHMENT         CC\n       REGION_N REGION_C                       geometry\n1  NORTH REGION       NR POLYGON ((19156.47 44276.42...\n2  NORTH REGION       NR POLYGON ((27819.05 44960.39...\n3  NORTH REGION       NR POLYGON ((24684.68 44271.39...\n4  NORTH REGION       NR POLYGON ((29187.4 44991.3, ...\n5  NORTH REGION       NR POLYGON ((30918.91 45252.69...\n6  NORTH REGION       NR POLYGON ((27119.56 45071.12...\n7  NORTH REGION       NR POLYGON ((28293.43 44966.31...\n8  NORTH REGION       NR POLYGON ((21410.16 41655.28...\n9  NORTH REGION       NR POLYGON ((27253.37 41646.98...\n10 NORTH REGION       NR POLYGON ((25073.29 43675.36...\n\n\n\n\n4.1.1.2 Extracting the roads in the North\nNow, let’s get the roads in the north.\n\nroads_inNorth &lt;- st_intersection(roads, north)\n\n\n\n4.1.1.3 Extracting the grab origin points in the North\n\ngrabO_inNorth &lt;- st_intersection(grabOrigin, north)\n\n\n\n\n4.1.2 Extracting the East\n\n4.1.2.1 Extracting the East from mpsz\nHere, we are extracting the east from mpsz.\n\neast &lt;- mpsz_sf %&gt;% \n  filter(REGION_N == \"EAST REGION\")\n\n\n\n4.1.2.2 Extracting the roads in the East\nNow, let’s get the roads in the east.\n\nroads_inEast &lt;- st_intersection(roads, east)\n\n\n\n4.1.2.3 Extracting the grab origin points in the East\n\ngrabO_inEast &lt;- st_intersection(grabOrigin, east)\n\n\n\n\n4.1.3 Extracting the Downtown core\n\n4.1.3.1 Extracting the downtown core from mpsz\nHere, we are extracting the downtown core from mpsz.\n\ndowntown_core &lt;- mpsz_sf %&gt;% \n  filter(PLN_AREA_N == \"DOWNTOWN CORE\")\n\n\n\n4.1.3.2 Extracting the roads in the Downtown core\nNow, let’s get the roads in the Downtown core\n\nroads_inDowntown &lt;- st_intersection(roads, downtown_core)\n\n\n\n4.1.3.3 Extracting the grab origin points in the Downtown core\n\ngrabO_in_downtown_core &lt;- st_intersection(grabOrigin, downtown_core)"
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex01/Take-Home_Ex01.html#deriving-the-network-constrained-kde-analysis",
    "href": "Take-Home_Ex/Take-Home_Ex01/Take-Home_Ex01.html#deriving-the-network-constrained-kde-analysis",
    "title": "Take-home Exercise 1 : Application of Spatial Point Patterns Analysis to discover the geographical distribution of Grab hailing services in Singapore",
    "section": "4.2 Deriving the network constrained KDE Analysis",
    "text": "4.2 Deriving the network constrained KDE Analysis\n\n4.2.1 Deriving KDE in the East\n\n4.2.1.1 Preparing the lixels objects\nThis code snippet means that the roads in the east must be cut into segments, with each of them being at 700m in length, and the minimum length a segment can be is 350m.\n\nroads_inEast &lt;- roads_inEast %&gt;%\n  st_sf() %&gt;%\n  st_cast(\"LINESTRING\")\nlixelsEast &lt;- lixelize_lines(roads_inEast, \n                         700, \n                         mindist = 350)\n\n\n\n4.2.1.2 Generating line centre points\nThis code is intended to generate the center points of the lixels using the lines_center function. These central points are used for netKDE.\n\nsamplesEast &lt;- lines_center(lixelsEast)\n\n\n\n4.2.1.3 Performing netKDE\n\ndensities &lt;- nkde(roads_inEast, \n                  events = grabO_inEast,\n                  w = rep(1,nrow(grabO_inEast)),\n                  samples = samplesEast,\n                  kernel_name = \"quartic\",\n                  bw = 300, \n                  div= \"bw\", \n                  method = \"simple\", \n                  digits = 1, \n                  tol = 1,\n                  grid_shape = c(1,1), \n                  max_depth = 8,\n                  agg = 5, #we aggregate events within a 5m radius (faster calculation)\n                  sparse = TRUE,\n                  verbose = FALSE)\n\n\n\n\n4.2.2 Deriving KDE in the North\n\n4.2.2.1 Preparing the lixels objects\n\nroads_inNorth &lt;- roads_inNorth %&gt;%\n  st_sf() %&gt;%\n  st_cast(\"LINESTRING\")\n\nlixelsNorth &lt;- lixelize_lines(roads_inNorth, \n                         700, \n                         mindist = 350)\n\n\n\n4.2.2.2 Generating line centre points\n\nsamplesNorth &lt;- lines_center(lixelsNorth)\n\n\n\n4.2.2.3 Performing netKDE\n\ndensitiesNorth &lt;- nkde(roads_inNorth, \n                  events = grabO_inNorth,\n                  w = rep(1,nrow(grabO_inNorth)),\n                  samples = samplesNorth,\n                  kernel_name = \"quartic\",\n                  bw = 300, \n                  div= \"bw\", \n                  method = \"simple\", \n                  digits = 1, \n                  tol = 1,\n                  grid_shape = c(1,1), \n                  max_depth = 8,\n                  agg = 5, #we aggregate events within a 5m radius (faster calculation)\n                  sparse = TRUE,\n                  verbose = FALSE)\n\n\n\n\n4.2.3 Deriving KDE in the Downtown\n\n4.2.2.1 Preparing the lixels objects\n\nroads_inDowntown &lt;- roads_inDowntown %&gt;%\n  st_sf() %&gt;%\n  st_cast(\"LINESTRING\")\nlixelsDowntown&lt;- lixelize_lines(roads_inDowntown, \n                         700, \n                         mindist = 350)\n\n\n\n4.2.2.3 Generating line centre points\n\nsamplesDowntown &lt;- lines_center(lixelsDowntown)\n\n\n\n4.2.2.4 Performing netKDE\n\ndensitiesDown &lt;- nkde(roads_inDowntown, \n                  events = grabO_in_downtown_core,\n                  w = rep(1,nrow(grabO_in_downtown_core)),\n                  samples = samplesDowntown,\n                  kernel_name = \"quartic\",\n                  bw = 300, \n                  div= \"bw\", \n                  method = \"simple\", \n                  digits = 1, \n                  tol = 1,\n                  grid_shape = c(1,1), \n                  max_depth = 8,\n                  agg = 5, #we aggregate events within a 5m radius (faster calculation)\n                  sparse = TRUE,\n                  verbose = FALSE)"
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex01/Take-Home_Ex01.html#visualising-nkde",
    "href": "Take-Home_Ex/Take-Home_Ex01/Take-Home_Ex01.html#visualising-nkde",
    "title": "Take-home Exercise 1 : Application of Spatial Point Patterns Analysis to discover the geographical distribution of Grab hailing services in Singapore",
    "section": "4.3 Visualising nKDE",
    "text": "4.3 Visualising nKDE\n\n4.3.1 Visualising nKDE for Southern/Downtown of Singapore\nBefore we can visualise the NetKDE values, code chunk below will be used to insert the computed density values (i.e. densities) into samples and lixels objects as density field.\n\nsamplesDowntown$density &lt;- densitiesDown\nsamplesDowntown$density &lt;- densitiesDown\n\nSince svy21 projection system is in meter, the computed density values are very small i.e. 0.0000005. The code chunk below is used to resale the density values from number of events per meter to number of events per kilometer.\n\n# rescaling to help the mapping\nsamplesDowntown$density &lt;- samplesDowntown$density*1000\nsamplesDowntown$density &lt;- samplesDowntown$density*1000\n\n\n#tmap_mode('plot')   \n#tm_shape(lixelsDowntown)+\n  #tm_lines(col=\"density\")+\n # tm_shape(grabO_in_downtown_core)+\n  #tm_dots()\n\n\n\n4.3.2 Visualising nKDE for North of Singapore\nBefore we can visualise the NetKDE values, code chunk below will be used to insert the computed density values (i.e. densities) into samples and lixels objects as density field.\n\nsamplesNorth$density &lt;- densitiesNorth\nsamplesNorth$density &lt;- densitiesNorth\n\nSince svy21 projection system is in meter, the computed density values are very small i.e. 0.0000005. The code chunk below is used to resale the density values from number of events per meter to number of events per kilometer.\n\n# rescaling to help the mapping\nsamplesNorth$density &lt;- samplesNorth$density*1000\nsamplesNorth$density &lt;- samplesNorth$density*1000\n\n\n#tmap_mode('plot')\n#tm_shape(lixelsNorth)+\n # tm_lines(col=\"density\")+\n#tm_shape(grabO_inNorth)+\n # tm_dots()\n\n\n\n4.3.3 Visualising nKDE for East of Singapore\nBefore we can visualise the NetKDE values, code chunk below will be used to insert the computed density values (i.e. densities) into samples and lixels objects as density field.\n\nsamplesEast$density &lt;- densities\nlixelsEast$density &lt;- densities\n\nSince svy21 projection system is in meter, the computed density values are very small i.e. 0.0000005. The code chunk below is used to resale the density values from number of events per meter to number of events per kilometer.\n\n# rescaling to help the mapping\nsamplesEast$density &lt;- samplesEast$density*1000\nlixelsEast$density &lt;- lixelsEast$density*1000\n\n\ntmap_mode('plot')\ntm_shape(lixelsEast)+\n  tm_lines(col=\"density\")+\ntm_shape(grabO_inEast)+\n  tm_dots()"
  },
  {
    "objectID": "In-class_Ex/In-class-Ex05/In-class-Ex05.html",
    "href": "In-class_Ex/In-class-Ex05/In-class-Ex05.html",
    "title": "Take-home Exercise 1 : Application of Spatial Point Patterns Analysis to discover the geographical distribution of Grab hailing services in Singapore",
    "section": "",
    "text": "We would need this take home exercise 2!"
  },
  {
    "objectID": "In-class_Ex/In-class-Ex05/In-class-Ex05.html#installing-and-loading-r-packages",
    "href": "In-class_Ex/In-class-Ex05/In-class-Ex05.html#installing-and-loading-r-packages",
    "title": "Take-home Exercise 1 : Application of Spatial Point Patterns Analysis to discover the geographical distribution of Grab hailing services in Singapore",
    "section": "Installing and loading R packages",
    "text": "Installing and loading R packages\nLet’s import !\n\npacman::p_load(sfdep, sf, tmap, tidyverse)"
  },
  {
    "objectID": "In-class_Ex/In-class-Ex05/In-class-Ex05.html#data-import",
    "href": "In-class_Ex/In-class-Ex05/In-class-Ex05.html#data-import",
    "title": "Take-home Exercise 1 : Application of Spatial Point Patterns Analysis to discover the geographical distribution of Grab hailing services in Singapore",
    "section": "Data Import",
    "text": "Data Import\n\nGeospatial\n\nhunan &lt;- st_read(dsn = \"data/data/geospatial\",\n layer  = \"Hunan\")\n\nReading layer `Hunan' from data source \n  `C:\\shaysnutss\\IS1455-GAA\\In-class_Ex\\In-class-Ex05\\data\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 88 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\n\n\n\n\nAspatial\n\nhunan2012 &lt;- read_csv(\"data/data/aspatial/Hunan_2012.csv\")"
  },
  {
    "objectID": "In-class_Ex/In-class-Ex05/In-class-Ex05.html#data-wrangling",
    "href": "In-class_Ex/In-class-Ex05/In-class-Ex05.html#data-wrangling",
    "title": "Take-home Exercise 1 : Application of Spatial Point Patterns Analysis to discover the geographical distribution of Grab hailing services in Singapore",
    "section": "Data Wrangling",
    "text": "Data Wrangling\nMake sure that the names of the columns you are joining ais the same. Not just that, make sure the column has the same values. To do so, you sort it first, and compare the values of both of them. Since they are the same , you can do a relational join.\n\n\n\n\n\n\nNote\n\n\n\nWhat’s the difference between left join and right join? Find out\n\n\n\nhuman_GDPPC &lt;- left_join(hunan, hunan2012) %&gt;%\n  select(1:4, 7, 15)\n# only need gdp per capita and geometry, thats why we take those columns\n\neval:false is a directive that tells the Quarto rendering engine not to evaluate or execute the code in this chunk. This can be useful when you want to show the code for instructional or illustrative purposes without actually running it\n\nwrite_rds(hunan_GDPPC,\n          \"data/rds/hunan_GDPPC.rds\")\n\n\ntmap_mode(\"plot\")\ntm_shape(human_GDPPC) +\n  tm_fill(\"GDPPC\",\n          style = \"quantile\",\n          palette = \"Blues\",\n          title = \"GDPPC\") + \n  tm_layout(main.title = \"distribution of gdp per capita\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45,\n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type = \"8star\", size = 2) +\n  tm_scale_bar() +\n  tm_grid(alpha = 0.2)\n\n\n\n\n\nwm_q &lt;- human_GDPPC %&gt;%\n  mutate(nb = st_contiguity(geometry),\n         wt= st_weights(nb, style = \"W\"),\n         .before = 1)\n\n\nComputing Global Moran’ I\n\nmoranI &lt;- global_moran(wm_q$GDPPC,\n                       wm_q$nb,\n                       wm_q$wt)\nglimpse(moranI)\n\nList of 2\n $ I: num 0.301\n $ K: num 7.64\n\n\n\n\nPerforming global moran’I permutation test\nThis is the only one required for take home exercise, the one directly before is not needed.\n\nset.seed(1234)\n\n\nglobal_moran_perm(wm_q$GDPPC,\n                  wm_q$nb,\n                  wm_q$wt,\n                  nsim=99)\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  x \nweights: listw  \nnumber of simulations + 1: 100 \n\nstatistic = 0.30075, observed rank = 100, p-value &lt; 2.2e-16\nalternative hypothesis: two.sided\n\n\nthe report show that p value is smaller than alpha value of o,o,5. ALWAYS SAY U INFER NOT PROVING. we have enouh stat evidence to reject nullhypo that spatial distriution ."
  },
  {
    "objectID": "In-class_Ex/In-class-Ex02/In-class_Ex02.html",
    "href": "In-class_Ex/In-class-Ex02/In-class_Ex02.html",
    "title": "In-class exercise 2 : R for geospatial data science",
    "section": "",
    "text": "Hey friends! Let’s learn about a few package:"
  },
  {
    "objectID": "In-class_Ex/In-class-Ex02/In-class_Ex02.html#loading-packages",
    "href": "In-class_Ex/In-class-Ex02/In-class_Ex02.html#loading-packages",
    "title": "In-class exercise 2 : R for geospatial data science",
    "section": "Loading packages",
    "text": "Loading packages\nThis is a very important step as we have to load the packages before we can use them.\n\npacman::p_load(sf, tmap, tidyverse,lubridate,arrow)"
  },
  {
    "objectID": "In-class_Ex/In-class-Ex02/In-class_Ex02.html#importing-the-data",
    "href": "In-class_Ex/In-class-Ex02/In-class_Ex02.html#importing-the-data",
    "title": "In-class exercise 2 : R for geospatial data science",
    "section": "Importing the data",
    "text": "Importing the data\n\nImporting Geospatial Data into R\nAs learnt in the previous page, we use st_read() function of the sf package to read in the data.\n\ndf &lt;- read_parquet(\"../../data/GrabPosisi/part-00000-8bbff892-97d2-4011-9961-703e38972569.c000.snappy.parquet\")\n\n\n\nConversion of data type\nHere we convert the int data type to date time data type. We are not using mutate, as we are over writing the data field in the file.\n\ndf$pingtimestamp &lt;-as_datetime(df$pingtimestamp)\n\n\n# get origin\norigin_df &lt;- df %&gt;% #use df\n  group_by(trj_id) %&gt;% #group according to trj_id\n  arrange(pingtimestamp) %&gt;% #sort according to timestamp asc (default)\n  filter(row_number()==1) %&gt;% #the first coordinate for every trip should be the origin\n  mutate(weekday = wday(pingtimestamp, label=TRUE, abbr=TRUE),\n         start_hr = factor(hour(pingtimestamp)),\n         day = factor(mday(pingtimestamp)))\n\n\ndf_summary &lt;- df %&gt;%\n  group_by(trj_id) %&gt;%\n  summarize(count = n(), .groups = 'drop')\n\n# Display the first few rows of the summary to check\nhead(df_summary)\n\n# A tibble: 6 × 2\n  trj_id count\n  &lt;chr&gt;  &lt;int&gt;\n1 10       104\n2 100       72\n3 1000      89\n4 10001    121\n5 10004     84\n6 10005     89\n\n\n\ndf_summary &lt;- origin_df %&gt;%\n  group_by(trj_id) %&gt;%\n  summarize(count = n(), .groups = 'drop')\n\n# Display the first few rows of the summary to check\nhead(df_summary)\n\n# A tibble: 6 × 2\n  trj_id count\n  &lt;chr&gt;  &lt;int&gt;\n1 10         1\n2 100        1\n3 1000       1\n4 10001      1\n5 10004      1\n6 10005      1\n\n\neach trip will have multiple rows because every minute the new location one be sent to the server . so when we arrange it, the first row is the origin location.\n\n# get end\ndestination_df &lt;- df %&gt;% #use df\n  group_by(trj_id) %&gt;% #group according to trj_id\n  arrange(desc(pingtimestamp)) %&gt;% #sort according to timestamp asc (default)\n  filter(row_number()==1) %&gt;% #the first coordinate for every trip should be the origin\n  mutate(weekday = wday(pingtimestamp, label=TRUE, abbr=TRUE),\n         end_hr = factor(hour(pingtimestamp)),\n         day = factor(mday(pingtimestamp)))\n\n\n# he writes in rds if he still wants to write in r\nwrite_rds(origin_df, \"../../data/rds/origin_df.rds\")\nwrite_rds(destination_df,\n          \"../../data/rds/destination_df.rds\")\n\n#import data\n\norigin_df &lt;- read_rds(\"../../data/rds/origin_df.rds\")\ndestination_df &lt;- read_rds(\"../../data/rds/destination_df.rds\")\n\n\nrm(grabOriginSg)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-On_Ex05-Local.html",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-On_Ex05-Local.html",
    "title": "Hands-on Exercise 9 : Global Measures of Spatial Autocorrelation",
    "section": "",
    "text": "In this hands-on exercise, you will learn how to compute Global and Local Measure of Spatial Autocorrelation (GLSA) by using spdep package. By the end to this hands-on exercise, you will be able to:\n\nimport geospatial data using appropriate function(s) of sf package,\nimport csv file using appropriate function of readr package,\nperform relational join using appropriate join function of dplyr package,\ncompute Global Spatial Autocorrelation (GSA) statistics by using appropriate functions of spdep package,\n\nplot Moran scatterplot,\ncompute and plot spatial correlogram using appropriate function of spdep package.\n\ncompute Local Indicator of Spatial Association (LISA) statistics for detecting clusters and outliers by using appropriate functions spdep package;\ncompute Getis-Ord’s Gi-statistics for detecting hot spot or/and cold spot area by using appropriate functions of spdep package; and\nto visualise the analysis output by using tmap package."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-On_Ex05-Local.html#overview",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-On_Ex05-Local.html#overview",
    "title": "Hands-on Exercise 9 : Global Measures of Spatial Autocorrelation",
    "section": "",
    "text": "In this hands-on exercise, you will learn how to compute Global and Local Measure of Spatial Autocorrelation (GLSA) by using spdep package. By the end to this hands-on exercise, you will be able to:\n\nimport geospatial data using appropriate function(s) of sf package,\nimport csv file using appropriate function of readr package,\nperform relational join using appropriate join function of dplyr package,\ncompute Global Spatial Autocorrelation (GSA) statistics by using appropriate functions of spdep package,\n\nplot Moran scatterplot,\ncompute and plot spatial correlogram using appropriate function of spdep package.\n\ncompute Local Indicator of Spatial Association (LISA) statistics for detecting clusters and outliers by using appropriate functions spdep package;\ncompute Getis-Ord’s Gi-statistics for detecting hot spot or/and cold spot area by using appropriate functions of spdep package; and\nto visualise the analysis output by using tmap package."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-On_Ex05-Local.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-On_Ex05-Local.html#getting-started",
    "title": "Hands-on Exercise 9 : Global Measures of Spatial Autocorrelation",
    "section": "10.2 Getting Started",
    "text": "10.2 Getting Started\n\n10.2.1 The analytical question\nIn spatial policy, one of the main development objective of the local govenment and planners is to ensure equal distribution of development in the province. Our task in this study, hence, is to apply appropriate spatial statistical methods to discover if development are even distributed geographically. If the answer is No. Then, our next question will be “is there sign of spatial clustering?”. And, if the answer for this question is yes, then our next question will be “where are these clusters?”\nIn this case study, we are interested to examine the spatial pattern of a selected development indicator (i.e. GDP per capita) of Hunan Provice, People Republic of China.(https://en.wikipedia.org/wiki/Hunan)\n\n\n10.2.2 The Study Area and Data\nTwo data sets will be used in this hands-on exercise, they are:\n\nHunan province administrative boundary layer at county level. This is a geospatial data set in ESRI shapefile format.\nHunan_2012.csv: This csv file contains selected Hunan’s local development indicators in 2012.\n\n\n\n10.2.3 Setting the Analytical Toolls\nBefore we get started, we need to ensure that spdep, sf, tmap and tidyverse packages of R are currently installed in your R.\n\nsf is use for importing and handling geospatial data in R,\ntidyverse is mainly use for wrangling attribute data in R,\nspdep will be used to compute spatial weights, global and local spatial autocorrelation statistics, and\ntmap will be used to prepare cartographic quality chropleth map.\n\nThe code chunk below is used to perform the following tasks:\n\ncreating a package list containing the necessary R packages,\nchecking if the R packages in the package list have been installed in R,\n\nif they have yet to be installed, RStudio will installed the missing packages,\n\nlaunching the packages into R environment.\n\n\npacman::p_load(sf, spdep, tmap, tidyverse)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-On_Ex05-Local.html#getting-the-data-into-r-environment",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-On_Ex05-Local.html#getting-the-data-into-r-environment",
    "title": "Hands-on Exercise 9 : Global Measures of Spatial Autocorrelation",
    "section": "10.3 Getting the Data Into R Environment",
    "text": "10.3 Getting the Data Into R Environment\nIn this section, you will learn how to bring a geospatial data and its associated attribute table into R environment. The geospatial data is in ESRI shapefile format and the attribute table is in csv fomat.\n\n10.3.1 Import shapefile into r environment\nThe code chunk below uses st_read() of sf package to import Hunan shapefile into R. The imported shapefile will be simple features Object of sf.\n\nhunan &lt;- st_read(dsn = \"../Hands-On_Ex04/data/geospatial\", \n                 layer = \"Hunan\")\n\nReading layer `Hunan' from data source \n  `C:\\shaysnutss\\IS1455-GAA\\Hands-on_Ex\\Hands-on_Ex04\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 88 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\n\n\n\n\n10.3.2 Import csv file into r environment\nNext, we will import Hunan_2012.csv into R by using read_csv() of readr package. The output is R data frame class.\n\nhunan2012 &lt;- read_csv(\"../Hands-On_Ex04/data/aspatial/Hunan_2012.csv\")\n\n\n\n10.3.3 Performing relational join\nThe code chunk below will be used to update the attribute table of hunan’s SpatialPolygonsDataFrame with the attribute fields of hunan2012 dataframe. This is performed by using left_join() of dplyr package.\n\nhunan &lt;- left_join(hunan,hunan2012) %&gt;%\n  select(1:4, 7, 15)\n\n\n\n10.3.4 Visualising Regional Development Indicator\nNow, we are going to prepare a basemap and a choropleth map showing the distribution of GDPPC 2012 by using qtm() of tmap package.\n\nequal &lt;- tm_shape(hunan) +\n  tm_fill(\"GDPPC\",\n          n = 5,\n          style = \"equal\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Equal interval classification\")\n\nquantile &lt;- tm_shape(hunan) +\n  tm_fill(\"GDPPC\",\n          n = 5,\n          style = \"quantile\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Equal quantile classification\")\n\ntmap_arrange(equal, \n             quantile, \n             asp=1, \n             ncol=2)\n\n\n\n\n\n\n10.4.1 Computing Contiguity Spatial Weights\nBefore we can compute the global spatial autocorrelation statistics, we need to construct a spatial weights of the study area. The spatial weights is used to define the neighbourhood relationships between the geographical units (i.e. county) in the study area.\nIn the code chunk below, poly2nb() of spdep package is used to compute contiguity weight matrices for the study area. This function builds a neighbours list based on regions with contiguous boundaries. If you look at the documentation you will see that you can pass a “queen” argument that takes TRUE or FALSE as options. If you do not specify this argument the default is set to TRUE, that is, if you don’t specify queen = FALSE this function will return a list of first order neighbours using the Queen criteria.\nMore specifically, the code chunk below is used to compute Queen contiguity weight matrix.\n\nwm_q &lt;- poly2nb(hunan, \n                queen=TRUE)\nsummary(wm_q)\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \nLink number distribution:\n\n 1  2  3  4  5  6  7  8  9 11 \n 2  2 12 16 24 14 11  4  2  1 \n2 least connected regions:\n30 65 with 1 link\n1 most connected region:\n85 with 11 links\n\n\nThe summary report above shows that there are 88 area units in Hunan. The most connected area unit has 11 neighbours. There are two area units with only one neighbours.\n\n\n10.4.2 Row-standardised weights matrix\nNext, we need to assign weights to each neighboring polygon. In our case, each neighboring polygon will be assigned equal weight (style=“W”). This is accomplished by assigning the fraction 1/(#ofneighbors) to each neighboring county then summing the weighted income values. While this is the most intuitive way to summaries the neighbors’ values it has one drawback in that polygons along the edges of the study area will base their lagged values on fewer polygons thus potentially over- or under-estimating the true nature of the spatial autocorrelation in the data. For this example, we’ll stick with the style=“W” option for simplicity’s sake but note that other more robust options are available, notably style=“B”.\n\nrswm_q &lt;- nb2listw(wm_q, \n                   style=\"W\", \n                   zero.policy = TRUE)\nrswm_q\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \n\nWeights style: W \nWeights constants summary:\n   n   nn S0       S1       S2\nW 88 7744 88 37.86334 365.9147\n\n\n\n\n10.4.3 Global Spatial Autocorrelation: Moran’s I\nIn this section, you will learn how to perform Moran’s I statistics testing by using moran.test() of spdep.\n\n\n10.4.4 Maron’s I test\nThe code chunk below performs Moran’s I statistical testing using moran.test() of spdep.\n\nmoran.test(hunan$GDPPC, \n           listw=rswm_q, \n           zero.policy = TRUE, \n           na.action=na.omit)\n\n\n    Moran I test under randomisation\n\ndata:  hunan$GDPPC  \nweights: rswm_q    \n\nMoran I statistic standard deviate = 4.7351, p-value = 1.095e-06\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n      0.300749970      -0.011494253       0.004348351 \n\n\n\n10.4.4.1 Computing Monte Carlo Moran’s I\nThe code chunk below performs permutation test for Moran’s I statistic by using moran.mc() of spdep. A total of 1000 simulation will be performed.\n\nset.seed(1234)\nbperm= moran.mc(hunan$GDPPC, \n                listw=rswm_q, \n                nsim=999, \n                zero.policy = TRUE, \n                na.action=na.omit)\nbperm\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  hunan$GDPPC \nweights: rswm_q  \nnumber of simulations + 1: 1000 \n\nstatistic = 0.30075, observed rank = 1000, p-value = 0.001\nalternative hypothesis: greater\n\n\n\n\n10.4.4.2 Visualising Monte Carlo Moran’s I\nIt is always a good practice for us the examine the simulated Moran’s I test statistics in greater detail. This can be achieved by plotting the distribution of the statistical values as a histogram by using the code chunk below.\nIn the code chunk below hist() and abline() of R Graphics are used.\n\nmean(bperm$res[1:999])\n\n[1] -0.01504572\n\n\n\nvar(bperm$res[1:999])\n\n[1] 0.004371574\n\n\n\nsummary(bperm$res[1:999])\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n-0.18339 -0.06168 -0.02125 -0.01505  0.02611  0.27593 \n\n\n\nhist(bperm$res, \n     freq=TRUE, \n     breaks=20, \n     xlab=\"Simulated Moran's I\")\nabline(v=0, \n       col=\"red\") \n\n\n\n\n\n\n\n10.4.5 Global Spatial Autocorrelation: Geary’s\nIn this section, you will learn how to perform Geary’s c statistics testing by using appropriate functions of spdep package.\n\n10.4.5.1 Geary’s C test\nThe code chunk below performs Geary’s C test for spatial autocorrelation by using geary.test() of spdep.\n\ngeary.test(hunan$GDPPC, listw=rswm_q)\n\n\n    Geary C test under randomisation\n\ndata:  hunan$GDPPC \nweights: rswm_q \n\nGeary C statistic standard deviate = 3.6108, p-value = 0.0001526\nalternative hypothesis: Expectation greater than statistic\nsample estimates:\nGeary C statistic       Expectation          Variance \n        0.6907223         1.0000000         0.0073364 \n\n\n\n\n10.4.5.2 Computing Monte Carlo Geary’s C\nThe code chunk below performs permutation test for Geary’s C statistic by using geary.mc() of spdep.\n\nset.seed(1234)\nbperm=geary.mc(hunan$GDPPC, \n               listw=rswm_q, \n               nsim=999)\nbperm\n\n\n    Monte-Carlo simulation of Geary C\n\ndata:  hunan$GDPPC \nweights: rswm_q \nnumber of simulations + 1: 1000 \n\nstatistic = 0.69072, observed rank = 1, p-value = 0.001\nalternative hypothesis: greater\n\n\nWhen you perform a Monte Carlo simulation, you’re essentially creating many random samples (in your case, 1000) based on the null hypothesis, which assumes there is no spatial autocorrelation. Each of these samples will have its own Geary’s C statistic. These simulated statistics create a distribution against which you can compare the Geary’s C statistic from your actual data.\nThe “observed rank” is where the actual Geary’s C statistic falls within this distribution. If the “observed rank” is 1, as in your output, it means that the actual statistic is the smallest out of all the simulated statistics — it’s ranked first, or in other words, no simulated statistic is smaller than the actual one.\n\n\nInterpreting the Observed Rank\nThe observed rank helps to understand the p-value in a Monte Carlo simulation:\n\nA low rank (like 1) means that the actual statistic is at the very low end of the simulated distribution, indicating that the actual observed spatial pattern is more pronounced than almost all patterns generated under the null hypothesis.\nConversely, a high rank would indicate that the actual statistic is at the high end of the simulated distribution, closer to patterns one might expect by chance.\n\nThe rank, when considered with the p-value, tells us about the statistical significance of the actual Geary’s C statistic:\n\nWith a rank of 1 and a p-value of 0.001, it’s very unlikely that the observed pattern happened by chance. This low p-value suggests that the spatial pattern detected by the actual Geary’s C statistic is statistically significant.\n\nIn sum, the rank in a Monte Carlo simulation for Geary’s C provides context for the p-value and helps confirm the statistical significance of the spatial pattern observed in your data.\n\n\n10.4.5.3 Visualising the Monte Carlo Geary’s C\nNext, we will plot a histogram to reveal the distribution of the simulated values by using the code chunk below.\n\nmean(bperm$res[1:999])\n\n[1] 1.004402\n\n\n\nvar(bperm$res[1:999])\n\n[1] 0.007436493\n\n\n\nsummary(bperm$res[1:999])\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.7142  0.9502  1.0052  1.0044  1.0595  1.2722 \n\n\n\nhist(bperm$res, freq=TRUE, breaks=20, xlab=\"Simulated Geary c\")\nabline(v=1, col=\"red\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-On_Ex05-Local.html#spatial-correlogram",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-On_Ex05-Local.html#spatial-correlogram",
    "title": "Hands-on Exercise 9 : Global Measures of Spatial Autocorrelation",
    "section": "10.5 Spatial Correlogram",
    "text": "10.5 Spatial Correlogram\nSpatial correlograms are great to examine patterns of spatial autocorrelation in your data or model residuals. They show how correlated are pairs of spatial observations when you increase the distance (lag) between them - they are plots of some index of autocorrelation (Moran’s I or Geary’s c) against distance.Although correlograms are not as fundamental as variograms (a keystone concept of geostatistics), they are very useful as an exploratory and descriptive tool. For this purpose they actually provide richer information than variograms.\n\n10.5.1 Compute Moran’s I correlogram\nIn the code chunk below, sp.correlogram() of spdep package is used to compute a 6-lag spatial correlogram of GDPPC. The global spatial autocorrelation used in Moran’s I. The plot() of base Graph is then used to plot the output.\nA correlogram is a chart that shows how well the value of a variable, measured at one location, predicts the value at another location across various distances. In spatial analysis, this means looking at how the similarity between places changes as the distance between them increases.\n\nX-axis (Lags): The x-axis shows the “lags”, which represent different levels of spatial relationship or distance classes. A lag of 1 indicates immediate neighbors, lag 2 indicates the neighbors of those neighbors, and so on, up to lag 6.\nY-axis (Moran’s I): The y-axis represents the Moran’s I statistic for each lag. Moran’s I values range from -1 to 1. Positive values indicate positive spatial autocorrelation (similar values cluster together), values around zero indicate a random spatial pattern, and negative values indicate negative spatial autocorrelation (dissimilar values cluster together).\npoints: Each point on the correlogram represents the Moran’s I statistic calculated for that particular lag. The value of Moran’s I at lag 1 being the highest and positive suggests that neighboring areas (the first level of neighbors) have the most significant positive spatial autocorrelation, indicating that areas with similar GDP per capita values are geographically close.\nVertical Lines (Error Bars): The vertical lines extending from each point represent the uncertainty or variability in the Moran’s I estimates, often corresponding to confidence intervals. The length of the line indicates the range of Moran’s I values that are statistically plausible for that lag. If the line crosses the horizontal line at Moran’s I = 0, the spatial autocorrelation for that lag is not statistically significant at the chosen confidence level.\n\n\nMI_corr &lt;- sp.correlogram(wm_q, \n                          hunan$GDPPC, \n                          order=6, \n                          method=\"I\", \n                          style=\"W\")\nplot(MI_corr)\n\n\n\n\nBy plotting the output might not allow us to provide complete interpretation. This is because not all autocorrelation values are statistically significant. Hence, it is important for us to examine the full analysis report by printing out the analysis results as in the code chunk below.\n\nprint(MI_corr)\n\nSpatial correlogram for hunan$GDPPC \nmethod: Moran's I\n         estimate expectation   variance standard deviate Pr(I) two sided    \n1 (88)  0.3007500  -0.0114943  0.0043484           4.7351       2.189e-06 ***\n2 (88)  0.2060084  -0.0114943  0.0020962           4.7505       2.029e-06 ***\n3 (88)  0.0668273  -0.0114943  0.0014602           2.0496        0.040400 *  \n4 (88)  0.0299470  -0.0114943  0.0011717           1.2107        0.226015    \n5 (88) -0.1530471  -0.0114943  0.0012440          -4.0134       5.984e-05 ***\n6 (88) -0.1187070  -0.0114943  0.0016791          -2.6164        0.008886 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n10.5.2 Compute Geary’s C correlogram and plot\nIn the code chunk below, sp.correlogram() of spdep package is used to compute a 6-lag spatial correlogram of GDPPC. The global spatial autocorrelation used in Geary’s C. The plot() of base Graph is then used to plot the output.\n\nGC_corr &lt;- sp.correlogram(wm_q, \n                          hunan$GDPPC, \n                          order=6, \n                          method=\"C\", \n                          style=\"W\")\nplot(GC_corr)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html",
    "title": "Hands-on Exercise 4 : Spatial Weights and Applications",
    "section": "",
    "text": "In this hands-on exercise, you will learn how to compute spatial weights using R. By the end to this hands-on exercise, you will be able t\n\nimport geospatial data using appropriate function(s) of sf package,\nimport csv file using appropriate function of readr package,\nperform relational join using appropriate join function of dplyr package,\ncompute spatial weights using appropriate functions of spdep package, and\ncalculate spatially lagged variables using appropriate functions of spdep package."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#overview",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#overview",
    "title": "Hands-on Exercise 4 : Spatial Weights and Applications",
    "section": "",
    "text": "In this hands-on exercise, you will learn how to compute spatial weights using R. By the end to this hands-on exercise, you will be able t\n\nimport geospatial data using appropriate function(s) of sf package,\nimport csv file using appropriate function of readr package,\nperform relational join using appropriate join function of dplyr package,\ncompute spatial weights using appropriate functions of spdep package, and\ncalculate spatially lagged variables using appropriate functions of spdep package."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#the-study-area-and-data",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#the-study-area-and-data",
    "title": "Hands-on Exercise 4 : Spatial Weights and Applications",
    "section": "8.2 The Study Area and Data",
    "text": "8.2 The Study Area and Data\nTwo data sets will be used in this hands-on exercise, they are:\n\nHunan county boundary layer. This is a geospatial data set in ESRI shapefile format.\nHunan_2012.csv: This csv file contains selected Hunan’s local development indicators in 2012.\n\n\n8.2.1 Getting Started\nBefore we get started, we need to ensure that spdep, sf, tmap and tidyverse packages of R are currently installed in your R.\n\npacman::p_load(sf, spdep, tmap, tidyverse, knitr)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#getting-the-data-into-r-environmen",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#getting-the-data-into-r-environmen",
    "title": "Hands-on Exercise 4 : Spatial Weights and Applications",
    "section": "8.3 Getting the Data Into R Environmen",
    "text": "8.3 Getting the Data Into R Environmen\nIn this section, you will learn how to bring a geospatial data and its associated attribute table into R environment. The geospatial data is in ESRI shapefile format and the attribute table is in csv fomat.\n\n8.3.1 Import shapefile into r environment\nThe code chunk below uses st_read() of sf package to import Hunan shapefile into R. The imported shapefile will be simple features Object of sf.\n\nhunan &lt;- st_read(dsn = \"data/geospatial\", \n                 layer = \"Hunan\")\n\nReading layer `Hunan' from data source \n  `C:\\shaysnutss\\IS1455-GAA\\Hands-on_Ex\\Hands-on_Ex04\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 88 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\n\n\n\n\n8.3.2 Import csv file into r environment\nNext, we will import Hunan_2012.csv into R by using read_csv() of readr package. The output is R dataframe class\n\nhunan2012 &lt;- read_csv(\"data/aspatial/Hunan_2012.csv\")\n\n\n\n8.3.3 Performing relational join\nThe code chunk below will be used to update the attribute table of hunan’s SpatialPolygonsDataFrame with the attribute fields of hunan2012 dataframe. This is performed by using left_join() of dplyr package.\n\nhunanNew &lt;- left_join(hunan,hunan2012)%&gt;%\n  select(1:4, 7, 15)\n\nThe left_join operation combines rows from hunan and hunan2012 based on a common column, which is likely the ID in this case. The result includes all rows from hunan and the matching rows from hunan2012. If there’s no match, the right side will have NA.\nThis indicates the selection of the first to fourth columns, the seventh column, and the fifteenth column."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#visualising-regional-development-indicator",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#visualising-regional-development-indicator",
    "title": "Hands-on Exercise 4 : Spatial Weights and Applications",
    "section": "8.4 Visualising Regional Development Indicator",
    "text": "8.4 Visualising Regional Development Indicator\nNow, we are going to prepare a basemap and a choropleth map showing the distribution of GDPPC 2012 by using qtm() of tmap package.\n\nbasemap &lt;- tm_shape(hunanNew) +\n  tm_polygons() +\n  tm_text(\"NAME_3\", size=0.5)\n\ngdppc &lt;- qtm(hunanNew, \"GDPPC\")\ntmap_arrange(basemap, gdppc, asp=1, ncol=2)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#computing-contiguity-spatial-weights",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#computing-contiguity-spatial-weights",
    "title": "Hands-on Exercise 4 : Spatial Weights and Applications",
    "section": "8.5 Computing Contiguity Spatial Weights",
    "text": "8.5 Computing Contiguity Spatial Weights\nWhat are spatial weights?\nThey are used to understand the relationship or connection between two locations.\nWhat are contiguity spatial weights?\nTwo geographical areas are considered neighbours if they share a boundary\nWhat is the sdep package used for?\nOne of the primary functions of the “spdep” package is to analyze spatial autocorrelation. Spatial autocorrelation refers to the degree to which the values of a variable tend to be similar (or dissimilar) in neighboring locations.\nIn this section, you will learn how to use poly2nb() of spdep package to compute contiguity weight matrices for the study area. This function builds a neighbours list based on regions with contiguous boundaries. If you look at the documentation you will see that you can pass a “queen” argument that takes TRUE or FALSE as options. If you do not specify this argument the default is set to TRUE, that is, if you don’t specify queen = FALSE this function will return a list of first order neighbours using the Queen criteria.\n\n8.5.1 Computing (QUEEN) contiguity based neighbours\n\nwm_q &lt;- poly2nb(hunanNew, queen=TRUE)\nsummary(wm_q)\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \nLink number distribution:\n\n 1  2  3  4  5  6  7  8  9 11 \n 2  2 12 16 24 14 11  4  2  1 \n2 least connected regions:\n30 65 with 1 link\n1 most connected region:\n85 with 11 links\n\n\nWhat does this function do?\nIt takes information about shapes or regions (polygons) and figures out which ones are next to each other. It does this by identifying regions that share a common boundary or vertex, essentially defining which regions are neighbors. The function then organizes this information into a list, making it easier to analyze the spatial relationships between regions in the dataset.\nNumber of regions: 88 \nIndicates the total number of regions (or polygons) in the dataset.\nNumber of nonzero links: 448 \nNonzero links refer to a geographical region’s neighbour , and in this context neighbour refers to a region that shares a boundary and tip with another region.\nIn the provided output, it states that there are 448 nonzero links, meaning there are 448 pairs of regions that are directly connected to each other in the spatial dataset.\nPercentage nonzero weights: 5.785124 \nThis represents the proportion of nonzero spatial weights relative to the total possible spatial relationships in the dataset.\nThe percentage nonzero weights indicate how many of these potential connections are actual nonzero links (i.e., regions that are actually neighbors).\nThe “total possible spatial relationships” refers to all potential pairwise connections between regions.\nAverage number of links: 5.090909 \nhe average number of links represents, on average, how many neighbors each location (or region) has in the spatial dataset\nLink number distribution:  \n1  2  3  4  5  6  7  8  9 11   \n2  2 12 16 24 14 11  4  2  1 \nThe numbers on the top(1, 2, 3, 4, 5, 6, 7, 8, 9, 11) represent the count of links. For example, “1” means there are regions with only 1 link, “2” means there are regions with 2 links, and so on.\n2 least connected regions: 30 65 with 1 linkk\n\nThis part identifies the regions with the fewest connections (or links) to neighboring regions.\nIn the example provided, regions 30 and 65 are the least connected, each having only 1 link to neighboring regions.\n\n1 most connected region: 85 with 11 links\n\nThis part identifies the region with the highest number of connections (or links) to neighboring regions.\nIn the example provided, region 85 is the most connected, having 11 links to neighboring regions.\n\nFor each polygon in our polygon object, wm_q lists all neighboring polygons. For example, to see the neighbors for the first polygon in the object, type:\n\nwm_q[[1]]\n\n[1]  2  3  4 57 85\n\n\nPolygon 1 has 5 neighbors. The numbers represent the polygon IDs as stored in hunan SpatialPolygonsDataFrame class.\nWe can retrive the county name of Polygon ID=1 by using the code chunk below:\n\nhunanNew$County[1]\n\n[1] \"Anxiang\"\n\n\nThe output reveals that Polygon ID=1 is Anxiang county.\nTo reveal the county names of the five neighboring polygons, the code chunk will be used:\n\nhunanNew$NAME_3[c(2,3,4,57,85)]\n\n[1] \"Hanshou\" \"Jinshi\"  \"Li\"      \"Nan\"     \"Taoyuan\"\n\n\nWe can retrieve the GDPPC of these five countries by using the code chunk below.\n\nnb1 &lt;- wm_q[[1]]\nnb1 &lt;- hunanNew$GDPPC[nb1]\nnb1\n\n[1] 20981 34592 24473 21311 22879\n\n\nThe printed output above shows that the GDPPC of the five nearest neighbours based on Queen’s method are 20981, 34592, 24473, 21311 and 22879 respectively.\n\n\n8.5.2 Creating (ROOK) contiguity based neighbours\nThe code chunk below is used to compute Rook contiguity weight matrix.\n\nwm_r &lt;- poly2nb(hunanNew, queen=FALSE)\nsummary(wm_r)\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 440 \nPercentage nonzero weights: 5.681818 \nAverage number of links: 5 \nLink number distribution:\n\n 1  2  3  4  5  6  7  8  9 10 \n 2  2 12 20 21 14 11  3  2  1 \n2 least connected regions:\n30 65 with 1 link\n1 most connected region:\n85 with 10 links\n\n\nThe summary report above shows that there are 88 area units in Hunan. The most connect area unit has 10 neighbours. There are two area units with only one heighbours.\n\n\n8.5.3 Visualising contiguity weights\nA connectivity graph is a visual representation that shows how points or polygons in a dataset are connected to each other. Each point is connected to its neighboring points through lines or edges.\nSince polygons are being used in the spatial dataset, we use polygon centroids as points to represent each polygon in the connectivity graph.\nin order to create the connectivity graph, the first step is to obtain points associated with each polygon.\nTo achieve this, we use the “sf” package, which is commonly used for handling spatial data in R.\nSpecifically, we use the st_centroid function to calculate the centroids of polygons. However, since the input data is likely in the form of an “sf” object (us.bound), some additional steps are required.\nHence, we use a mapping function.\nA mapping function is like a tool that applies another function to each item in a list or column.\nThe map_dbl function from the “purrr” package is used to do this. It takes each polygon as input, applies the st_centroid function to it, and returns the result.\nAfter finding the centroid for each polygon, we want to extract the longitude value from each centroid.\nThe st_centroid function returns a point, which is like a pair of coordinates (latitude and longitude).\n\nlongitude &lt;- map_dbl(hunanNew$geometry, ~st_centroid(.x)[[1]])\n\nmap_dbl: This function is part of the “purrr” package in R. It’s used to apply another function (in this case, st_centroid) to each element of a vector or list.\nHere, map_dbl is being used to apply st_centroid to each element in the vector hunanNew$geometry.\nThe st_centroid function takes a geometry object as input and returns a point geometry representing the centroid of that shape.\nThe ~ symbol indicates the start of a lambda function (also known as a “formula” or “anonymous” function). It’s a concise way to define a function inline.\nIn this lambda function, .x represents each element of the vector hunanNew$geometry. It’s a placeholder for the current element being processed\nst_centroid(.x) calculates the centroid of the current geometry element.\n[[1]] extracts the first value of the centroid point, which corresponds to the longitude coordinate.\nWe do the same for latitude with one key difference. We access the second value per each centroid with [[2]].\n\nlatitude &lt;- map_dbl(hunan$geometry, ~st_centroid(.x)[[2]])\n\nNow that we have latitude and longitude, we use cbind to put longitude and latitude into the same object.\n\ncoords &lt;- cbind(longitude, latitude)\n\nWe check the first few observations to see if things are formatted correctly.\n\nhead(coords)\n\n     longitude latitude\n[1,]  112.1531 29.44362\n[2,]  112.0372 28.86489\n[3,]  111.8917 29.47107\n[4,]  111.7031 29.74499\n[5,]  111.6138 29.49258\n[6,]  111.0341 29.79863\n\n\n\n8.5.3.1 Plotting Queen contiguity based neighbours map\n\nplot(hunanNew$geometry, border=\"lightgrey\")\nplot(wm_q, coords, pch = 19, cex = 0.6, add = TRUE, col= \"red\")\n\n\n\n\n\n\n8.5.3.2 Plotting Rook contiguity based neighbours map\n\nplot(hunanNew$geometry, border=\"lightgrey\")\nplot(wm_r, coords, pch = 19, cex = 0.6, add = TRUE, col = \"red\")\n\n\n\n\n\n\n8.5.3.3 Plotting both Queen and Rook contiguity based neighbours maps\n\npar(mfrow=c(1,2))\nplot(hunan$geometry, border=\"lightgrey\", main=\"Queen Contiguity\")\nplot(wm_q, coords, pch = 19, cex = 0.6, add = TRUE, col= \"red\")\nplot(hunan$geometry, border=\"lightgrey\", main=\"Rook Contiguity\")\nplot(wm_r, coords, pch = 19, cex = 0.6, add = TRUE, col = \"red\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#computing-distance-based-neighbours",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#computing-distance-based-neighbours",
    "title": "Hands-on Exercise 4 : Spatial Weights and Applications",
    "section": "8.6 Computing distance based neighbours",
    "text": "8.6 Computing distance based neighbours\nIn distance based neigbours, regions are considered neighbours when they are a certain distance apart.\nEuclidean distance is a measure of straight-line distance between two points in a Euclidean space (such as a two-dimensional plane).\nThe dnearneigh() function is part of the “spdep” package in R. It is used to identify neighboring regions (or points) based on Euclidean distance. The function calculates the distances between points and identifies neighboring points within a specified distance band.\nThe distance band is defined by lower (d1) and upper (d2) bounds. The bounds argument allows control over these bounds, specifying the range within which points are considered neighbors based on their distances.\nIf unprojected coordinates are used (i.e., coordinates specified as latitude and longitude) and longlat=TRUE is specified, the function calculates great circle distances in kilometers.\n\n8.6.1 Determine the cut-off distance\nFirstly, we need to determine the upper limit for distance band by using the steps below:\n\nfinding k nearest neighbours\n\nThe first step involves finding the k nearest neighbors for each point in the dataset.\nThis is done using the knearneigh() function from the “spdep” package in R.\nThe function returns a matrix with the indices of points that are the k nearest neighbors of each other.\n\neach row corresponds to a data point, and each column contains the indices (or positions) of the k nearest neighbors of that data point in the dataset.\n\n\nconverting output to neighbours list\n\nWhile the matrix returned by knearneigh() provides information about the nearest neighbors, it may not be in the most convenient format for further analysis.\nnew structure allows for easier manipulaton and interpretation of spatial relationships\n\ncalculating length of neighbour relationships\n\nAfter identifying the nearest neighbors for each data point, we want to know how far apart these neighbors are from each other.\nThis distance between neighboring points is referred to as the “length of neighbor relationship edges.”\nTo calculate these distances, we use the nbdists() function from the “spdep” package in R\n\nremoval of list structure\n\nunlist() function converts the output into a simple vector without any nested lists\n\n\n\n#coords &lt;- coordinates(hunan)\nk1 &lt;- knn2nb(knearneigh(coords))\nk1dists &lt;- unlist(nbdists(k1, coords, longlat = TRUE))\nsummary(k1dists)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  24.79   32.57   38.01   39.07   44.52   61.79 \n\n\n\n\n8.6.2 Computing fixed distance weight matrix\nNow, we will compute the distance weight matrix by using dnearneigh() as shown in the code chunk below.\nThe dnearneigh() function calculates the neighbors of each region (or point) in the dataset based on the specified distance range. If the distance between two regions falls within this distance band, they are considered neighbors according to the spatial weight matrix created by the function.\n\nwm_d62 &lt;- dnearneigh(coords, 0, 62, longlat = TRUE)\nwm_d62\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 324 \nPercentage nonzero weights: 4.183884 \nAverage number of links: 3.681818 \n\n\n\ncoords: This is the input data containing the coordinates of the points for which the spatial weight matrix will be calculated.\n0: This value specifies the lower bound of the distance band. In this case, it’s set to 0, meaning that all points within a certain distance (up to the upper bound) will be considered neighbors.\n62: This value specifies the upper bound of the distance band. It determines the maximum distance within which points will be considered neighbors.\nlonglat = TRUE: This argument indicates that the coordinates are given in latitude and longitude, and great circle distances should be used for calculations (measured in kilometers).\n\n\n8.6.2.1 Plotting fixed distance weight matrix\n\nplot(hunanNew$geometry, border=\"lightgrey\")\nplot(wm_d62, coords, add=TRUE)\nplot(k1, coords, add=TRUE, col=\"red\", length=0.08)\n\n\n\n\nThe red lines show the links of 1st nearest neighbours and the black lines show the links of neighbours within the cut-off distance of 62km.\nAlternatively, we can plot both of them next to each other by using the code chunk below.\n\npar(mfrow=c(1,2))\nplot(hunan$geometry, border=\"lightgrey\", main=\"1st nearest neighbours\")\nplot(k1, coords, add=TRUE, col=\"red\", length=0.08)\nplot(hunan$geometry, border=\"lightgrey\", main=\"Distance link\")\nplot(wm_d62, coords, add=TRUE, pch = 19, cex = 0.6)\n\n\n\n\n\n\n\n8.6.3 Computing adaptive distance weight matrix\nAn adaptive distance weight matrix helps address the issue where densely populated areas (such as urban areas) tend to have more neighbors, while sparsely populated areas (such as rural areas) tend to have fewer neighbors.\nBy adjusting the number of neighbors for each point based on its local density, an adaptive distance weight matrix ensures a more balanced representation of spatial relationships across different regions. This means that each point will have a consistent number of neighbors, regardless of the overall density of the dataset.\n\nknn6 &lt;- knn2nb(knearneigh(coords, k=6))\nknn6\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 528 \nPercentage nonzero weights: 6.818182 \nAverage number of links: 6 \nNon-symmetric neighbours list\n\n\n\n8.6.3.1 Plotting distance based neighbours\n\nplot(hunanNew$geometry, border=\"lightgrey\")\nplot(knn6, coords, pch = 19, cex = 0.6, add = TRUE, col = \"red\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#weights-based-on-idw",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#weights-based-on-idw",
    "title": "Hands-on Exercise 4 : Spatial Weights and Applications",
    "section": "8.7 Weights based on IDW",
    "text": "8.7 Weights based on IDW\nInverse distance calculation is a technique where the relationship between two locations is determined by the inverse of the distance between them.The basic idea is that closer locations have a stronger relationship, while farther locations have a weaker relationship.\n\ndist &lt;- nbdists(wm_q, coords, longlat = TRUE)\nids &lt;- lapply(dist, function(x) 1/(x))\nids\n\n[[1]]\n[1] 0.01535405 0.03916350 0.01820896 0.02807922 0.01145113\n\n[[2]]\n[1] 0.01535405 0.01764308 0.01925924 0.02323898 0.01719350\n\n[[3]]\n[1] 0.03916350 0.02822040 0.03695795 0.01395765\n\n[[4]]\n[1] 0.01820896 0.02822040 0.03414741 0.01539065\n\n[[5]]\n[1] 0.03695795 0.03414741 0.01524598 0.01618354\n\n[[6]]\n[1] 0.015390649 0.015245977 0.021748129 0.011883901 0.009810297\n\n[[7]]\n[1] 0.01708612 0.01473997 0.01150924 0.01872915\n\n[[8]]\n[1] 0.02022144 0.03453056 0.02529256 0.01036340 0.02284457 0.01500600 0.01515314\n\n[[9]]\n[1] 0.02022144 0.01574888 0.02109502 0.01508028 0.02902705 0.01502980\n\n[[10]]\n[1] 0.02281552 0.01387777 0.01538326 0.01346650 0.02100510 0.02631658 0.01874863\n[8] 0.01500046\n\n[[11]]\n[1] 0.01882869 0.02243492 0.02247473\n\n[[12]]\n[1] 0.02779227 0.02419652 0.02333385 0.02986130 0.02335429\n\n[[13]]\n[1] 0.02779227 0.02650020 0.02670323 0.01714243\n\n[[14]]\n[1] 0.01882869 0.01233868 0.02098555\n\n[[15]]\n[1] 0.02650020 0.01233868 0.01096284 0.01562226\n\n[[16]]\n[1] 0.02281552 0.02466962 0.02765018 0.01476814 0.01671430\n\n[[17]]\n[1] 0.01387777 0.02243492 0.02098555 0.01096284 0.02466962 0.01593341 0.01437996\n\n[[18]]\n[1] 0.02039779 0.02032767 0.01481665 0.01473691 0.01459380\n\n[[19]]\n[1] 0.01538326 0.01926323 0.02668415 0.02140253 0.01613589 0.01412874\n\n[[20]]\n[1] 0.01346650 0.02039779 0.01926323 0.01723025 0.02153130 0.01469240 0.02327034\n\n[[21]]\n[1] 0.02668415 0.01723025 0.01766299 0.02644986 0.02163800\n\n[[22]]\n[1] 0.02100510 0.02765018 0.02032767 0.02153130 0.01489296\n\n[[23]]\n[1] 0.01481665 0.01469240 0.01401432 0.02246233 0.01880425 0.01530458 0.01849605\n\n[[24]]\n[1] 0.02354598 0.01837201 0.02607264 0.01220154 0.02514180\n\n[[25]]\n[1] 0.02354598 0.02188032 0.01577283 0.01949232 0.02947957\n\n[[26]]\n[1] 0.02155798 0.01745522 0.02212108 0.02220532\n\n[[27]]\n[1] 0.02155798 0.02490625 0.01562326\n\n[[28]]\n[1] 0.01837201 0.02188032 0.02229549 0.03076171 0.02039506\n\n[[29]]\n[1] 0.02490625 0.01686587 0.01395022\n\n[[30]]\n[1] 0.02090587\n\n[[31]]\n[1] 0.02607264 0.01577283 0.01219005 0.01724850 0.01229012 0.01609781 0.01139438\n[8] 0.01150130\n\n[[32]]\n[1] 0.01220154 0.01219005 0.01712515 0.01340413 0.01280928 0.01198216 0.01053374\n[8] 0.01065655\n\n[[33]]\n[1] 0.01949232 0.01745522 0.02229549 0.02090587 0.01979045\n\n[[34]]\n[1] 0.03113041 0.03589551 0.02882915\n\n[[35]]\n[1] 0.01766299 0.02185795 0.02616766 0.02111721 0.02108253 0.01509020\n\n[[36]]\n[1] 0.01724850 0.03113041 0.01571707 0.01860991 0.02073549 0.01680129\n\n[[37]]\n[1] 0.01686587 0.02234793 0.01510990 0.01550676\n\n[[38]]\n[1] 0.01401432 0.02407426 0.02276151 0.01719415\n\n[[39]]\n[1] 0.01229012 0.02172543 0.01711924 0.02629732 0.01896385\n\n[[40]]\n[1] 0.01609781 0.01571707 0.02172543 0.01506473 0.01987922 0.01894207\n\n[[41]]\n[1] 0.02246233 0.02185795 0.02205991 0.01912542 0.01601083 0.01742892\n\n[[42]]\n[1] 0.02212108 0.01562326 0.01395022 0.02234793 0.01711924 0.01836831 0.01683518\n\n[[43]]\n[1] 0.01510990 0.02629732 0.01506473 0.01836831 0.03112027 0.01530782\n\n[[44]]\n[1] 0.01550676 0.02407426 0.03112027 0.01486508\n\n[[45]]\n[1] 0.03589551 0.01860991 0.01987922 0.02205991 0.02107101 0.01982700\n\n[[46]]\n[1] 0.03453056 0.04033752 0.02689769\n\n[[47]]\n[1] 0.02529256 0.02616766 0.04033752 0.01949145 0.02181458\n\n[[48]]\n[1] 0.02313819 0.03370576 0.02289485 0.01630057 0.01818085\n\n[[49]]\n[1] 0.03076171 0.02138091 0.02394529 0.01990000\n\n[[50]]\n[1] 0.01712515 0.02313819 0.02551427 0.02051530 0.02187179\n\n[[51]]\n[1] 0.03370576 0.02138091 0.02873854\n\n[[52]]\n[1] 0.02289485 0.02394529 0.02551427 0.02873854 0.03516672\n\n[[53]]\n[1] 0.01630057 0.01979945 0.01253977\n\n[[54]]\n[1] 0.02514180 0.02039506 0.01340413 0.01990000 0.02051530 0.03516672\n\n[[55]]\n[1] 0.01280928 0.01818085 0.02187179 0.01979945 0.01882298\n\n[[56]]\n[1] 0.01036340 0.01139438 0.01198216 0.02073549 0.01214479 0.01362855 0.01341697\n\n[[57]]\n[1] 0.028079221 0.017643082 0.031423501 0.029114131 0.013520292 0.009903702\n\n[[58]]\n[1] 0.01925924 0.03142350 0.02722997 0.01434859 0.01567192\n\n[[59]]\n[1] 0.01696711 0.01265572 0.01667105 0.01785036\n\n[[60]]\n[1] 0.02419652 0.02670323 0.01696711 0.02343040\n\n[[61]]\n[1] 0.02333385 0.01265572 0.02343040 0.02514093 0.02790764 0.01219751 0.02362452\n\n[[62]]\n[1] 0.02514093 0.02002219 0.02110260\n\n[[63]]\n[1] 0.02986130 0.02790764 0.01407043 0.01805987\n\n[[64]]\n[1] 0.02911413 0.01689892\n\n[[65]]\n[1] 0.02471705\n\n[[66]]\n[1] 0.01574888 0.01726461 0.03068853 0.01954805 0.01810569\n\n[[67]]\n[1] 0.01708612 0.01726461 0.01349843 0.01361172\n\n[[68]]\n[1] 0.02109502 0.02722997 0.03068853 0.01406357 0.01546511\n\n[[69]]\n[1] 0.02174813 0.01645838 0.01419926\n\n[[70]]\n[1] 0.02631658 0.01963168 0.02278487\n\n[[71]]\n[1] 0.01473997 0.01838483 0.03197403\n\n[[72]]\n[1] 0.01874863 0.02247473 0.01476814 0.01593341 0.01963168\n\n[[73]]\n[1] 0.01500046 0.02140253 0.02278487 0.01838483 0.01652709\n\n[[74]]\n[1] 0.01150924 0.01613589 0.03197403 0.01652709 0.01342099 0.02864567\n\n[[75]]\n[1] 0.011883901 0.010533736 0.012539774 0.018822977 0.016458383 0.008217581\n\n[[76]]\n[1] 0.01352029 0.01434859 0.01689892 0.02471705 0.01954805 0.01349843 0.01406357\n\n[[77]]\n[1] 0.014736909 0.018804247 0.022761507 0.012197506 0.020022195 0.014070428\n[7] 0.008440896\n\n[[78]]\n[1] 0.02323898 0.02284457 0.01508028 0.01214479 0.01567192 0.01546511 0.01140779\n\n[[79]]\n[1] 0.01530458 0.01719415 0.01894207 0.01912542 0.01530782 0.01486508 0.02107101\n\n[[80]]\n[1] 0.01500600 0.02882915 0.02111721 0.01680129 0.01601083 0.01982700 0.01949145\n[8] 0.01362855\n\n[[81]]\n[1] 0.02947957 0.02220532 0.01150130 0.01979045 0.01896385 0.01683518\n\n[[82]]\n[1] 0.02327034 0.02644986 0.01849605 0.02108253 0.01742892\n\n[[83]]\n[1] 0.023354289 0.017142433 0.015622258 0.016714303 0.014379961 0.014593799\n[7] 0.014892965 0.018059871 0.008440896\n\n[[84]]\n[1] 0.01872915 0.02902705 0.01810569 0.01361172 0.01342099 0.01297994\n\n[[85]]\n [1] 0.011451133 0.017193502 0.013957649 0.016183544 0.009810297 0.010656545\n [7] 0.013416965 0.009903702 0.014199260 0.008217581 0.011407794\n\n[[86]]\n[1] 0.01515314 0.01502980 0.01412874 0.02163800 0.01509020 0.02689769 0.02181458\n[8] 0.02864567 0.01297994\n\n[[87]]\n[1] 0.01667105 0.02362452 0.02110260 0.02058034\n\n[[88]]\n[1] 0.01785036 0.02058034\n\n\nThe nbdists() function calculates the distances between points in a neighbors list, which in this case is represented by wm_q.\nThe longlat = TRUE argument indicates that the coordinates are in latitude and longitude format, and great circle distances should be used for the calculations (measured in kilometers)\nOnce the distances are computed, they are transformed using the inverse distance method."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#row-standardised-weights-matrix",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#row-standardised-weights-matrix",
    "title": "Hands-on Exercise 4 : Spatial Weights and Applications",
    "section": "Row-standardised Weights Matrix",
    "text": "Row-standardised Weights Matrix\nThe goal is to assign weights to neighboring polygons based on their spatial relationships.\nIn this case, each neighboring polygon is assigned equal weight, indicated by the “W” style option.\nEach neighboring polygon is assigned a weight equal to the fraction 1number of neighborsnumber of neighbors1​.\nThis means that the weight assigned to each neighboring polygon is determined by dividing 1 by the total number of neighboring polygons.\nAssigning equal weights ensures that each neighboring polygon contributes equally to the analysis.\nassigning equal weights may lead to potential biases, especially for polygons along the edges of the study area\n\nrswm_q &lt;- nb2listw(wm_q, style=\"W\", zero.policy = TRUE)\nrswm_q\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \n\nWeights style: W \nWeights constants summary:\n   n   nn S0       S1       S2\nW 88 7744 88 37.86334 365.9147\n\n\nTo see the weight of the first polygon’s eight neighbors type:\n\nrswm_q$weights[10]\n\n[[1]]\n[1] 0.125 0.125 0.125 0.125 0.125 0.125 0.125 0.125\n\n\nEach neighbor is assigned a 0.125 of the total weight. This means that when R computes the average neighboring income values, each neighbor’s income will be multiplied by 0.125 before being tallied.\nUsing the same method, we can also derive a row standardised distance weight matrix by using the code chunk below.\n\nrswm_ids &lt;- nb2listw(wm_q, glist=ids, style=\"B\", zero.policy=TRUE)\nrswm_ids\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \n\nWeights style: B \nWeights constants summary:\n   n   nn       S0        S1     S2\nB 88 7744 8.786867 0.3776535 3.8137"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#application-of-spatial-weight-matrix",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#application-of-spatial-weight-matrix",
    "title": "Hands-on Exercise 4 : Spatial Weights and Applications",
    "section": "8.9 Application of Spatial Weight Matrix",
    "text": "8.9 Application of Spatial Weight Matrix\nIn this section, you will learn how to create four different spatial lagged variables, they are:\n\nspatial lag with row-standardized weights,\nspatial lag as a sum of neighbouring values,\nspatial window average, and\nspatial window sum.\n\nWhat’s a spatial lagged variable?\nImagine you’re studying crime rates in different neighborhoods of a city. You have data on the number of reported crimes in each neighborhood. You’re interested in understanding if the crime rate in one neighborhood is influenced by the crime rates in neighboring neighborhoods.\nThe spatial value is a single value that represents the influence of the neighbours crime rate on the crime rate of the focal area. Spatial lag is a way to measure how the value of something in one area is influenced by the values of the same thing in neighboring areas.\nThe spatial variable, refers to the original variable of interest (in this case, crime rate) for each individual area or observation in the dataset.\nWhat is Spatial lag with row-standardized weights?\nLet’s say we have a small town divided into three neighborhoods: A, B, and C.\n\nNeighborhood A has a crime rate of 10.\nNeighborhood B has a crime rate of 5.\nNeighborhood C has a crime rate of 8.\n\nWe want to calculate the spatial lag for each neighborhood’s crime rate using row-standardized weights.\nStep 1: Determine Neighbors:\n\nLet’s assume that neighborhoods A and B are neighbors, and neighborhoods B and C are neighbors.\n\nStep 2: Calculate Spatial Lag:\n\nFor each neighborhood, we’ll calculate the spatial lag by taking a weighted average of its neighbors’ crime rates.\nFor neighborhood A:\n\nIts neighbors are B and C.\nWeights: Since there are two neighbors, we divide 1 by 2 to get 0.5 for each weight (assuming equal weighting for simplicity).\nSpatial lag for A = (0.5 * 5) + (0.5 * 8) = 6.5.\n\nFor neighborhood B:\n\nIts neighbor is A.\nWeights: There is only one neighbor, so the weight is 1.\nSpatial lag for B = 1 * 10 = 10.\n\nFor neighborhood C:\n\nIts neighbor is B.\nWeights: There is only one neighbor, so the weight is 1.\nSpatial lag for C = 1 * 5 = 5.\n\n\nIn this example, neighborhood A are less influenced by neighbors’ crime rates compared to neighborhood B.\nWhat is spatial lag as a sum of neighboring values?\nLet’s use the same example of a small town divided into three neighborhoods: A, B, and C.\n\nNeighborhood A has a population of 100.\nNeighborhood B has a population of 150.\nNeighborhood C has a population of 200.\n\nWe want to calculate the spatial lag for each neighborhood’s population as a sum of neighboring values.\nStep 1: Determine Neighbors:\n\nLet’s assume that neighborhoods A and B are neighbors, and neighborhoods B and C are neighbors.\n\nStep 2: Calculate Spatial Lag as a Sum:\n\nFor each neighborhood, we’ll calculate the spatial lag by summing the population values of its neighboring neighborhoods.\nFor neighborhood A:\n\nIts neighbors are B and C.\nSpatial lag for A = Population of B (150) + Population of C (200) = 350.\n\nFor neighborhood B:\n\nIts neighbor is A.\nSpatial lag for B = Population of A (100) = 100.\n\nFor neighborhood C:\n\nIts neighbor is B.\nSpatial lag for C = Population of B (150) = 150.\n\n\nStep 3: Interpretation:\n\nThe spatial lag values represent the total population of neighboring neighborhoods for each focal neighborhood.\nIn our example, neighborhood A’s spatial lag of 350 means that its population is influenced by the total population of its neighbors, neighborhoods B and C.\n\nWhat is a spatial window ?\nA spatial window is a defined area or region around a focal observation or point of interest.\nIn spatial analysis, a spatial window is used to group neighboring observations or areas together for analysis.\nA focal observation could indeed be something like the crime rate in a specific area. So, when we talk about a spatial window in the context of crime rates, we’re defining an area around a specific location where we’re interested in understanding the surrounding crime rates.\nWhat is a spatial window average?\nSpatial window average calculates the average value of a variable within a defined spatial window around each observation.\nIt provides insight into the average characteristics of itself and its neighboring areas, considering the variable of interest within a specified spatial context.\nso the difference between the row standardized spatial lag and spatial window average is that in spatial window average we take the focal region into account too but in row standardised we just look at the neighbours\nWhat is spatial window sum?\nso difference between spatial window sum and spatial lag as a sum of neighbouring values is that spatial window sum takes focal region into account\n\n8.9.1 Spatial lag with row-standardized weights\nFinally, we’ll compute the average neighbor GDPPC value for each polygon. These values are often referred to as spatially lagged values.\n\nGDPPC.lag &lt;- lag.listw(rswm_q, hunanNew$GDPPC)\nGDPPC.lag\n\n [1] 24847.20 22724.80 24143.25 27737.50 27270.25 21248.80 43747.00 33582.71\n [9] 45651.17 32027.62 32671.00 20810.00 25711.50 30672.33 33457.75 31689.20\n[17] 20269.00 23901.60 25126.17 21903.43 22718.60 25918.80 20307.00 20023.80\n[25] 16576.80 18667.00 14394.67 19848.80 15516.33 20518.00 17572.00 15200.12\n[33] 18413.80 14419.33 24094.50 22019.83 12923.50 14756.00 13869.80 12296.67\n[41] 15775.17 14382.86 11566.33 13199.50 23412.00 39541.00 36186.60 16559.60\n[49] 20772.50 19471.20 19827.33 15466.80 12925.67 18577.17 14943.00 24913.00\n[57] 25093.00 24428.80 17003.00 21143.75 20435.00 17131.33 24569.75 23835.50\n[65] 26360.00 47383.40 55157.75 37058.00 21546.67 23348.67 42323.67 28938.60\n[73] 25880.80 47345.67 18711.33 29087.29 20748.29 35933.71 15439.71 29787.50\n[81] 18145.00 21617.00 29203.89 41363.67 22259.09 44939.56 16902.00 16930.00\n\n\n\nlag.list &lt;- list(hunanNew$NAME_3, lag.listw(rswm_q, hunanNew$GDPPC))\nlag.res &lt;- as.data.frame(lag.list)\ncolnames(lag.res) &lt;- c(\"NAME_3\", \"lag GDPPC\")\nhunanNew &lt;- left_join(hunanNew,lag.res)\n\n\nhead(hunanNew)\n\nSimple feature collection with 6 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 110.4922 ymin: 28.61762 xmax: 112.3013 ymax: 30.12812\nGeodetic CRS:  WGS 84\n   NAME_2  ID_3  NAME_3   ENGTYPE_3  County GDPPC lag GDPPC\n1 Changde 21098 Anxiang      County Anxiang 23667  24847.20\n2 Changde 21100 Hanshou      County Hanshou 20981  22724.80\n3 Changde 21101  Jinshi County City  Jinshi 34592  24143.25\n4 Changde 21102      Li      County      Li 24473  27737.50\n5 Changde 21103   Linli      County   Linli 25554  27270.25\n6 Changde 21104  Shimen      County  Shimen 27137  21248.80\n                        geometry\n1 POLYGON ((112.0625 29.75523...\n2 POLYGON ((112.2288 29.11684...\n3 POLYGON ((111.8927 29.6013,...\n4 POLYGON ((111.3731 29.94649...\n5 POLYGON ((111.6324 29.76288...\n6 POLYGON ((110.8825 30.11675...\n\n\nNext, we will plot both the GDPPC and spatial lag GDPPC for comparison using the code chunk below.\n\ngdppc &lt;- qtm(hunanNew, \"GDPPC\")\nlag_gdppc &lt;- qtm(hunanNew, \"lag GDPPC\")\ntmap_arrange(gdppc, lag_gdppc, asp=1, ncol=2)\n\n\n\n\n\n\n8.9.2 Spatial lag as a sum of neighboring values\nWe can calculate spatial lag as a sum of neighboring values by assigning binary weights. This requires us to go back to our neighbors list, then apply a function that will assign binary weights, then we use glist = in the nb2listw function to explicitly assign these weights.\nWe start by applying a function that will assign a value of 1 per each neighbor. This is done with lapply, which we have been using to manipulate the neighbors structure throughout the past notebooks. Basically it applies a function across each value in the neighbors structure.\n\nb_weights &lt;- lapply(wm_q, function(x) 0*x + 1)\nb_weights2 &lt;- nb2listw(wm_q, \n                       glist = b_weights, \n                       style = \"B\")\nb_weights2\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \n\nWeights style: B \nWeights constants summary:\n   n   nn  S0  S1    S2\nB 88 7744 448 896 10224\n\n\nBinary weights assign a value of 1 to each neighbor, indicating a binary relationship (either adjacent or not). This step involves creating a binary weight matrix based on the neighbors list.\nThe lapply() function is used to apply a function across each element of the neighbors list (wm_q). Inside the lapply() function, a simple function is defined: function(x) 0*x + 1. This function assigns a value of 1 to each neighbor in the list.\nThe second step involves converting the binary weights list (b_weights) into a binary spatial weights matrix (b_weights2) using the nb2listw() function.\nWhile binary weights have already been assigned in the first step, converting them to a spatial weights matrix format is necessary for certain spatial analysis techniques that require a formal spatial weights object.\nWith the proper weights assigned, we can use lag.listw to compute a lag variable from our weight and GDPPC.\n\nlag_sum &lt;- list(hunanNew$NAME_3, lag.listw(b_weights2, hunanNew$GDPPC))\nlag.res &lt;- as.data.frame(lag_sum)\ncolnames(lag.res) &lt;- c(\"NAME_3\", \"lag_sum GDPPC\")\n\n\nlag_sum\n\n[[1]]\n [1] \"Anxiang\"       \"Hanshou\"       \"Jinshi\"        \"Li\"           \n [5] \"Linli\"         \"Shimen\"        \"Liuyang\"       \"Ningxiang\"    \n [9] \"Wangcheng\"     \"Anren\"         \"Guidong\"       \"Jiahe\"        \n[13] \"Linwu\"         \"Rucheng\"       \"Yizhang\"       \"Yongxing\"     \n[17] \"Zixing\"        \"Changning\"     \"Hengdong\"      \"Hengnan\"      \n[21] \"Hengshan\"      \"Leiyang\"       \"Qidong\"        \"Chenxi\"       \n[25] \"Zhongfang\"     \"Huitong\"       \"Jingzhou\"      \"Mayang\"       \n[29] \"Tongdao\"       \"Xinhuang\"      \"Xupu\"          \"Yuanling\"     \n[33] \"Zhijiang\"      \"Lengshuijiang\" \"Shuangfeng\"    \"Xinhua\"       \n[37] \"Chengbu\"       \"Dongan\"        \"Dongkou\"       \"Longhui\"      \n[41] \"Shaodong\"      \"Suining\"       \"Wugang\"        \"Xinning\"      \n[45] \"Xinshao\"       \"Shaoshan\"      \"Xiangxiang\"    \"Baojing\"      \n[49] \"Fenghuang\"     \"Guzhang\"       \"Huayuan\"       \"Jishou\"       \n[53] \"Longshan\"      \"Luxi\"          \"Yongshun\"      \"Anhua\"        \n[57] \"Nan\"           \"Yuanjiang\"     \"Jianghua\"      \"Lanshan\"      \n[61] \"Ningyuan\"      \"Shuangpai\"     \"Xintian\"       \"Huarong\"      \n[65] \"Linxiang\"      \"Miluo\"         \"Pingjiang\"     \"Xiangyin\"     \n[69] \"Cili\"          \"Chaling\"       \"Liling\"        \"Yanling\"      \n[73] \"You\"           \"Zhuzhou\"       \"Sangzhi\"       \"Yueyang\"      \n[77] \"Qiyang\"        \"Taojiang\"      \"Shaoyang\"      \"Lianyuan\"     \n[81] \"Hongjiang\"     \"Hengyang\"      \"Guiyang\"       \"Changsha\"     \n[85] \"Taoyuan\"       \"Xiangtan\"      \"Dao\"           \"Jiangyong\"    \n\n[[2]]\n [1] 124236 113624  96573 110950 109081 106244 174988 235079 273907 256221\n[11]  98013 104050 102846  92017 133831 158446 141883 119508 150757 153324\n[21] 113593 129594 142149 100119  82884  74668  43184  99244  46549  20518\n[31] 140576 121601  92069  43258 144567 132119  51694  59024  69349  73780\n[41]  94651 100680  69398  52798 140472 118623 180933  82798  83090  97356\n[51]  59482  77334  38777 111463  74715 174391 150558 122144  68012  84575\n[61] 143045  51394  98279  47671  26360 236917 220631 185290  64640  70046\n[71] 126971 144693 129404 284074 112268 203611 145238 251536 108078 238300\n[81] 108870 108085 262835 248182 244850 404456  67608  33860\n\n\nNext, we will append the lag_sum GDPPC field into hunan sf data frame by using the code chunk below.\n\nhunanNew &lt;- left_join(hunanNew, lag.res)\n\n\nhunanNew\n\nSimple feature collection with 88 features and 8 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\nFirst 10 features:\n     NAME_2  ID_3    NAME_3   ENGTYPE_3    County GDPPC lag GDPPC lag_sum GDPPC\n1   Changde 21098   Anxiang      County   Anxiang 23667  24847.20        124236\n2   Changde 21100   Hanshou      County   Hanshou 20981  22724.80        113624\n3   Changde 21101    Jinshi County City    Jinshi 34592  24143.25         96573\n4   Changde 21102        Li      County        Li 24473  27737.50        110950\n5   Changde 21103     Linli      County     Linli 25554  27270.25        109081\n6   Changde 21104    Shimen      County    Shimen 27137  21248.80        106244\n7  Changsha 21109   Liuyang County City   Liuyang 63118  43747.00        174988\n8  Changsha 21110 Ningxiang      County Ningxiang 62202  33582.71        235079\n9  Changsha 21111 Wangcheng      County Wangcheng 70666  45651.17        273907\n10 Chenzhou 21112     Anren      County     Anren 12761  32027.62        256221\n                         geometry\n1  POLYGON ((112.0625 29.75523...\n2  POLYGON ((112.2288 29.11684...\n3  POLYGON ((111.8927 29.6013,...\n4  POLYGON ((111.3731 29.94649...\n5  POLYGON ((111.6324 29.76288...\n6  POLYGON ((110.8825 30.11675...\n7  POLYGON ((113.9905 28.5682,...\n8  POLYGON ((112.7181 28.38299...\n9  POLYGON ((112.7914 28.52688...\n10 POLYGON ((113.1757 26.82734...\n\n\n\ngdppc &lt;- qtm(hunanNew, \"GDPPC\")\nlag_sum_gdppc &lt;- qtm(hunanNew, \"lag_sum GDPPC\")\ntmap_arrange(gdppc, lag_sum_gdppc, asp=1, ncol=2)\n\n\n\n\n\n\n8.9.3 Spatial window average\nThe spatial window average uses row-standardized weights and includes the diagonal element. To do this in R, we need to go back to the neighbors structure and add the diagonal element before assigning weights.\nTo add the diagonal element to the neighbour list, we just need to use include.self() from spdep.\n\nwm_qs &lt;- include.self(wm_q)\n\n\nwm_qs\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 536 \nPercentage nonzero weights: 6.921488 \nAverage number of links: 6.090909 \n\n\nNotice that the Number of nonzero links, Percentage nonzero weights and Average number of links are 536, 6.921488 and 6.090909 respectively as compared to wm_q of 448, 5.785124 and 5.090909\nLet us take a good look at the neighbour list of area [1] by using the code chunk below.\n\nwm_qs[[1]]\n\n[1]  1  2  3  4 57 85\n\n\nNotice that now [1] has six neighbours instead of five.\nNow we obtain weights with nb2listw()\n\nwm_qs &lt;- nb2listw(wm_qs)\nwm_qs\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 536 \nPercentage nonzero weights: 6.921488 \nAverage number of links: 6.090909 \n\nWeights style: W \nWeights constants summary:\n   n   nn S0       S1       S2\nW 88 7744 88 30.90265 357.5308\n\n\nAgain, we use nb2listw() and glist() to explicitly assign weight values.\nLastly, we just need to create the lag variable from our weight structure and GDPPC variable.\n\nlag_w_avg_gpdpc &lt;- lag.listw(wm_qs, \n                             hunanNew$GDPPC)\nlag_w_avg_gpdpc\n\n [1] 24650.50 22434.17 26233.00 27084.60 26927.00 22230.17 47621.20 37160.12\n [9] 49224.71 29886.89 26627.50 22690.17 25366.40 25825.75 30329.00 32682.83\n[17] 25948.62 23987.67 25463.14 21904.38 23127.50 25949.83 20018.75 19524.17\n[25] 18955.00 17800.40 15883.00 18831.33 14832.50 17965.00 17159.89 16199.44\n[33] 18764.50 26878.75 23188.86 20788.14 12365.20 15985.00 13764.83 11907.43\n[41] 17128.14 14593.62 11644.29 12706.00 21712.29 43548.25 35049.00 16226.83\n[49] 19294.40 18156.00 19954.75 18145.17 12132.75 18419.29 14050.83 23619.75\n[57] 24552.71 24733.67 16762.60 20932.60 19467.75 18334.00 22541.00 26028.00\n[65] 29128.50 46569.00 47576.60 36545.50 20838.50 22531.00 42115.50 27619.00\n[73] 27611.33 44523.29 18127.43 28746.38 20734.50 33880.62 14716.38 28516.22\n[81] 18086.14 21244.50 29568.80 48119.71 22310.75 43151.60 17133.40 17009.33\n\n\nNext, we will convert the lag variable listw object into a data.frame by using as.data.frame().\n\nlag.list.wm_qs &lt;- list(hunanNew$NAME_3, lag.listw(wm_qs, hunanNew$GDPPC))\nlag_wm_qs.res &lt;- as.data.frame(lag.list.wm_qs)\ncolnames(lag_wm_qs.res) &lt;- c(\"NAME_3\", \"lag_window_avg GDPPC\")\n\nNote: The third command line on the code chunk above renames the field names of lag_wm_q1.res object into NAME_3 and lag_window_avg GDPPC respectively.\nNext, the code chunk below will be used to append lag_window_avg GDPPC values onto hunan sf data.frame by using left_join() of dplyr package.\n\nhunan &lt;- left_join(hunanNew, lag_wm_qs.res)\n\nTo compare the values of lag GDPPC and Spatial window average, kable() of Knitr package is used to prepare a table using the code chunk below.\n\nhunan %&gt;%\n  select(\"County\", \n         \"lag GDPPC\", \n         \"lag_window_avg GDPPC\") %&gt;%\n  kable()\n\n\n\n\n\n\n\n\n\n\nCounty\nlag GDPPC\nlag_window_avg GDPPC\ngeometry\n\n\n\n\nAnxiang\n24847.20\n24650.50\nPOLYGON ((112.0625 29.75523…\n\n\nHanshou\n22724.80\n22434.17\nPOLYGON ((112.2288 29.11684…\n\n\nJinshi\n24143.25\n26233.00\nPOLYGON ((111.8927 29.6013,…\n\n\nLi\n27737.50\n27084.60\nPOLYGON ((111.3731 29.94649…\n\n\nLinli\n27270.25\n26927.00\nPOLYGON ((111.6324 29.76288…\n\n\nShimen\n21248.80\n22230.17\nPOLYGON ((110.8825 30.11675…\n\n\nLiuyang\n43747.00\n47621.20\nPOLYGON ((113.9905 28.5682,…\n\n\nNingxiang\n33582.71\n37160.12\nPOLYGON ((112.7181 28.38299…\n\n\nWangcheng\n45651.17\n49224.71\nPOLYGON ((112.7914 28.52688…\n\n\nAnren\n32027.62\n29886.89\nPOLYGON ((113.1757 26.82734…\n\n\nGuidong\n32671.00\n26627.50\nPOLYGON ((114.1799 26.20117…\n\n\nJiahe\n20810.00\n22690.17\nPOLYGON ((112.4425 25.74358…\n\n\nLinwu\n25711.50\n25366.40\nPOLYGON ((112.5914 25.55143…\n\n\nRucheng\n30672.33\n25825.75\nPOLYGON ((113.6759 25.87578…\n\n\nYizhang\n33457.75\n30329.00\nPOLYGON ((113.2621 25.68394…\n\n\nYongxing\n31689.20\n32682.83\nPOLYGON ((113.3169 26.41843…\n\n\nZixing\n20269.00\n25948.62\nPOLYGON ((113.7311 26.16259…\n\n\nChangning\n23901.60\n23987.67\nPOLYGON ((112.6144 26.60198…\n\n\nHengdong\n25126.17\n25463.14\nPOLYGON ((113.1056 27.21007…\n\n\nHengnan\n21903.43\n21904.38\nPOLYGON ((112.7599 26.98149…\n\n\nHengshan\n22718.60\n23127.50\nPOLYGON ((112.607 27.4689, …\n\n\nLeiyang\n25918.80\n25949.83\nPOLYGON ((112.9996 26.69276…\n\n\nQidong\n20307.00\n20018.75\nPOLYGON ((111.7818 27.0383,…\n\n\nChenxi\n20023.80\n19524.17\nPOLYGON ((110.2624 28.21778…\n\n\nZhongfang\n16576.80\n18955.00\nPOLYGON ((109.9431 27.72858…\n\n\nHuitong\n18667.00\n17800.40\nPOLYGON ((109.9419 27.10512…\n\n\nJingzhou\n14394.67\n15883.00\nPOLYGON ((109.8186 26.75842…\n\n\nMayang\n19848.80\n18831.33\nPOLYGON ((109.795 27.98008,…\n\n\nTongdao\n15516.33\n14832.50\nPOLYGON ((109.9294 26.46561…\n\n\nXinhuang\n20518.00\n17965.00\nPOLYGON ((109.227 27.43733,…\n\n\nXupu\n17572.00\n17159.89\nPOLYGON ((110.7189 28.30485…\n\n\nYuanling\n15200.12\n16199.44\nPOLYGON ((110.9652 28.99895…\n\n\nZhijiang\n18413.80\n18764.50\nPOLYGON ((109.8818 27.60661…\n\n\nLengshuijiang\n14419.33\n26878.75\nPOLYGON ((111.5307 27.81472…\n\n\nShuangfeng\n24094.50\n23188.86\nPOLYGON ((112.263 27.70421,…\n\n\nXinhua\n22019.83\n20788.14\nPOLYGON ((111.3345 28.19642…\n\n\nChengbu\n12923.50\n12365.20\nPOLYGON ((110.4455 26.69317…\n\n\nDongan\n14756.00\n15985.00\nPOLYGON ((111.4531 26.86812…\n\n\nDongkou\n13869.80\n13764.83\nPOLYGON ((110.6622 27.37305…\n\n\nLonghui\n12296.67\n11907.43\nPOLYGON ((110.985 27.65983,…\n\n\nShaodong\n15775.17\n17128.14\nPOLYGON ((111.9054 27.40254…\n\n\nSuining\n14382.86\n14593.62\nPOLYGON ((110.389 27.10006,…\n\n\nWugang\n11566.33\n11644.29\nPOLYGON ((110.9878 27.03345…\n\n\nXinning\n13199.50\n12706.00\nPOLYGON ((111.0736 26.84627…\n\n\nXinshao\n23412.00\n21712.29\nPOLYGON ((111.6013 27.58275…\n\n\nShaoshan\n39541.00\n43548.25\nPOLYGON ((112.5391 27.97742…\n\n\nXiangxiang\n36186.60\n35049.00\nPOLYGON ((112.4549 28.05783…\n\n\nBaojing\n16559.60\n16226.83\nPOLYGON ((109.7015 28.82844…\n\n\nFenghuang\n20772.50\n19294.40\nPOLYGON ((109.5239 28.19206…\n\n\nGuzhang\n19471.20\n18156.00\nPOLYGON ((109.8968 28.74034…\n\n\nHuayuan\n19827.33\n19954.75\nPOLYGON ((109.5647 28.61712…\n\n\nJishou\n15466.80\n18145.17\nPOLYGON ((109.8375 28.4696,…\n\n\nLongshan\n12925.67\n12132.75\nPOLYGON ((109.6337 29.62521…\n\n\nLuxi\n18577.17\n18419.29\nPOLYGON ((110.1067 28.41835…\n\n\nYongshun\n14943.00\n14050.83\nPOLYGON ((110.0003 29.29499…\n\n\nAnhua\n24913.00\n23619.75\nPOLYGON ((111.6034 28.63716…\n\n\nNan\n25093.00\n24552.71\nPOLYGON ((112.3232 29.46074…\n\n\nYuanjiang\n24428.80\n24733.67\nPOLYGON ((112.4391 29.1791,…\n\n\nJianghua\n17003.00\n16762.60\nPOLYGON ((111.6461 25.29661…\n\n\nLanshan\n21143.75\n20932.60\nPOLYGON ((112.2286 25.61123…\n\n\nNingyuan\n20435.00\n19467.75\nPOLYGON ((112.0715 26.09892…\n\n\nShuangpai\n17131.33\n18334.00\nPOLYGON ((111.8864 26.11957…\n\n\nXintian\n24569.75\n22541.00\nPOLYGON ((112.2578 26.0796,…\n\n\nHuarong\n23835.50\n26028.00\nPOLYGON ((112.9242 29.69134…\n\n\nLinxiang\n26360.00\n29128.50\nPOLYGON ((113.5502 29.67418…\n\n\nMiluo\n47383.40\n46569.00\nPOLYGON ((112.9902 29.02139…\n\n\nPingjiang\n55157.75\n47576.60\nPOLYGON ((113.8436 29.06152…\n\n\nXiangyin\n37058.00\n36545.50\nPOLYGON ((112.9173 28.98264…\n\n\nCili\n21546.67\n20838.50\nPOLYGON ((110.8822 29.69017…\n\n\nChaling\n23348.67\n22531.00\nPOLYGON ((113.7666 27.10573…\n\n\nLiling\n42323.67\n42115.50\nPOLYGON ((113.5673 27.94346…\n\n\nYanling\n28938.60\n27619.00\nPOLYGON ((113.9292 26.6154,…\n\n\nYou\n25880.80\n27611.33\nPOLYGON ((113.5879 27.41324…\n\n\nZhuzhou\n47345.67\n44523.29\nPOLYGON ((113.2493 28.02411…\n\n\nSangzhi\n18711.33\n18127.43\nPOLYGON ((110.556 29.40543,…\n\n\nYueyang\n29087.29\n28746.38\nPOLYGON ((113.343 29.61064,…\n\n\nQiyang\n20748.29\n20734.50\nPOLYGON ((111.5563 26.81318…\n\n\nTaojiang\n35933.71\n33880.62\nPOLYGON ((112.0508 28.67265…\n\n\nShaoyang\n15439.71\n14716.38\nPOLYGON ((111.5013 27.30207…\n\n\nLianyuan\n29787.50\n28516.22\nPOLYGON ((111.6789 28.02946…\n\n\nHongjiang\n18145.00\n18086.14\nPOLYGON ((110.1441 27.47513…\n\n\nHengyang\n21617.00\n21244.50\nPOLYGON ((112.7144 26.98613…\n\n\nGuiyang\n29203.89\n29568.80\nPOLYGON ((113.0811 26.04963…\n\n\nChangsha\n41363.67\n48119.71\nPOLYGON ((112.9421 28.03722…\n\n\nTaoyuan\n22259.09\n22310.75\nPOLYGON ((112.0612 29.32855…\n\n\nXiangtan\n44939.56\n43151.60\nPOLYGON ((113.0426 27.8942,…\n\n\nDao\n16902.00\n17133.40\nPOLYGON ((111.498 25.81679,…\n\n\nJiangyong\n16930.00\n17009.33\nPOLYGON ((111.3659 25.39472…\n\n\n\n\n\nLastly, qtm() of tmap package is used to plot the lag_gdppc and w_ave_gdppc maps next to each other for quick comparison.\n\nw_avg_gdppc &lt;- qtm(hunanNew, \"lag_window_avg GDPPC\")\n\n\n\n8.9.4 Spatial window sum\nThe spatial window sum is the counter part of the window average, but without using row-standardized weights.\nTo add the diagonal element to the neighbour list, we just need to use include.self() from spdep.\n\nwm_qs &lt;- include.self(wm_q)\nwm_qs\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 536 \nPercentage nonzero weights: 6.921488 \nAverage number of links: 6.090909 \n\n\nNext, we will assign binary weights to the neighbour structure that includes the diagonal element.\n\nb_weights &lt;- lapply(wm_qs, function(x) 0*x + 1)\nb_weights[1]\n\n[[1]]\n[1] 1 1 1 1 1 1\n\n\nNotice that now [1] has six neighbours instead of five.\nAgain, we use nb2listw() and glist() to explicitly assign weight values.\n\nb_weights2 &lt;- nb2listw(wm_qs, \n                       glist = b_weights, \n                       style = \"B\")\nb_weights2\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 536 \nPercentage nonzero weights: 6.921488 \nAverage number of links: 6.090909 \n\nWeights style: B \nWeights constants summary:\n   n   nn  S0   S1    S2\nB 88 7744 536 1072 14160\n\n\nWith our new weight structure, we can compute the lag variable with lag.listw().\n\nw_sum_gdppc &lt;- list(hunanNew$NAME_3, lag.listw(b_weights2, hunanNew$GDPPC))\nw_sum_gdppc\n\n[[1]]\n [1] \"Anxiang\"       \"Hanshou\"       \"Jinshi\"        \"Li\"           \n [5] \"Linli\"         \"Shimen\"        \"Liuyang\"       \"Ningxiang\"    \n [9] \"Wangcheng\"     \"Anren\"         \"Guidong\"       \"Jiahe\"        \n[13] \"Linwu\"         \"Rucheng\"       \"Yizhang\"       \"Yongxing\"     \n[17] \"Zixing\"        \"Changning\"     \"Hengdong\"      \"Hengnan\"      \n[21] \"Hengshan\"      \"Leiyang\"       \"Qidong\"        \"Chenxi\"       \n[25] \"Zhongfang\"     \"Huitong\"       \"Jingzhou\"      \"Mayang\"       \n[29] \"Tongdao\"       \"Xinhuang\"      \"Xupu\"          \"Yuanling\"     \n[33] \"Zhijiang\"      \"Lengshuijiang\" \"Shuangfeng\"    \"Xinhua\"       \n[37] \"Chengbu\"       \"Dongan\"        \"Dongkou\"       \"Longhui\"      \n[41] \"Shaodong\"      \"Suining\"       \"Wugang\"        \"Xinning\"      \n[45] \"Xinshao\"       \"Shaoshan\"      \"Xiangxiang\"    \"Baojing\"      \n[49] \"Fenghuang\"     \"Guzhang\"       \"Huayuan\"       \"Jishou\"       \n[53] \"Longshan\"      \"Luxi\"          \"Yongshun\"      \"Anhua\"        \n[57] \"Nan\"           \"Yuanjiang\"     \"Jianghua\"      \"Lanshan\"      \n[61] \"Ningyuan\"      \"Shuangpai\"     \"Xintian\"       \"Huarong\"      \n[65] \"Linxiang\"      \"Miluo\"         \"Pingjiang\"     \"Xiangyin\"     \n[69] \"Cili\"          \"Chaling\"       \"Liling\"        \"Yanling\"      \n[73] \"You\"           \"Zhuzhou\"       \"Sangzhi\"       \"Yueyang\"      \n[77] \"Qiyang\"        \"Taojiang\"      \"Shaoyang\"      \"Lianyuan\"     \n[81] \"Hongjiang\"     \"Hengyang\"      \"Guiyang\"       \"Changsha\"     \n[85] \"Taoyuan\"       \"Xiangtan\"      \"Dao\"           \"Jiangyong\"    \n\n[[2]]\n [1] 147903 134605 131165 135423 134635 133381 238106 297281 344573 268982\n[11] 106510 136141 126832 103303 151645 196097 207589 143926 178242 175235\n[21] 138765 155699 160150 117145 113730  89002  63532 112988  59330  35930\n[31] 154439 145795 112587 107515 162322 145517  61826  79925  82589  83352\n[41] 119897 116749  81510  63530 151986 174193 210294  97361  96472 108936\n[51]  79819 108871  48531 128935  84305 188958 171869 148402  83813 104663\n[61] 155742  73336 112705  78084  58257 279414 237883 219273  83354  90124\n[71] 168462 165714 165668 311663 126892 229971 165876 271045 117731 256646\n[81] 126603 127467 295688 336838 267729 431516  85667  51028\n\n\nNext, we will convert the lag variable listw object into a data.frame by using as.data.frame().\n\nw_sum_gdppc.res &lt;- as.data.frame(w_sum_gdppc)\ncolnames(w_sum_gdppc.res) &lt;- c(\"NAME_3\", \"w_sum GDPPC\")\n\nNote: The second command line on the code chunk above renames the field names of w_sum_gdppc.res object into NAME_3 and w_sum GDPPC respectively.\nNext, the code chunk below will be used to append w_sum GDPPC values onto hunan sf data.frame by using left_join() of dplyr package.\n\nhunanNew &lt;- left_join(hunanNew, w_sum_gdppc.res)\n\nTo compare the values of lag GDPPC and Spatial window average, kable() of Knitr package is used to prepare a table using the code chunk below.\n\nhunanNew %&gt;%\n  select(\"County\", \"lag_sum GDPPC\", \"w_sum GDPPC\") %&gt;%\n  kable()\n\n\n\n\n\n\n\n\n\n\nCounty\nlag_sum GDPPC\nw_sum GDPPC\ngeometry\n\n\n\n\nAnxiang\n124236\n147903\nPOLYGON ((112.0625 29.75523…\n\n\nHanshou\n113624\n134605\nPOLYGON ((112.2288 29.11684…\n\n\nJinshi\n96573\n131165\nPOLYGON ((111.8927 29.6013,…\n\n\nLi\n110950\n135423\nPOLYGON ((111.3731 29.94649…\n\n\nLinli\n109081\n134635\nPOLYGON ((111.6324 29.76288…\n\n\nShimen\n106244\n133381\nPOLYGON ((110.8825 30.11675…\n\n\nLiuyang\n174988\n238106\nPOLYGON ((113.9905 28.5682,…\n\n\nNingxiang\n235079\n297281\nPOLYGON ((112.7181 28.38299…\n\n\nWangcheng\n273907\n344573\nPOLYGON ((112.7914 28.52688…\n\n\nAnren\n256221\n268982\nPOLYGON ((113.1757 26.82734…\n\n\nGuidong\n98013\n106510\nPOLYGON ((114.1799 26.20117…\n\n\nJiahe\n104050\n136141\nPOLYGON ((112.4425 25.74358…\n\n\nLinwu\n102846\n126832\nPOLYGON ((112.5914 25.55143…\n\n\nRucheng\n92017\n103303\nPOLYGON ((113.6759 25.87578…\n\n\nYizhang\n133831\n151645\nPOLYGON ((113.2621 25.68394…\n\n\nYongxing\n158446\n196097\nPOLYGON ((113.3169 26.41843…\n\n\nZixing\n141883\n207589\nPOLYGON ((113.7311 26.16259…\n\n\nChangning\n119508\n143926\nPOLYGON ((112.6144 26.60198…\n\n\nHengdong\n150757\n178242\nPOLYGON ((113.1056 27.21007…\n\n\nHengnan\n153324\n175235\nPOLYGON ((112.7599 26.98149…\n\n\nHengshan\n113593\n138765\nPOLYGON ((112.607 27.4689, …\n\n\nLeiyang\n129594\n155699\nPOLYGON ((112.9996 26.69276…\n\n\nQidong\n142149\n160150\nPOLYGON ((111.7818 27.0383,…\n\n\nChenxi\n100119\n117145\nPOLYGON ((110.2624 28.21778…\n\n\nZhongfang\n82884\n113730\nPOLYGON ((109.9431 27.72858…\n\n\nHuitong\n74668\n89002\nPOLYGON ((109.9419 27.10512…\n\n\nJingzhou\n43184\n63532\nPOLYGON ((109.8186 26.75842…\n\n\nMayang\n99244\n112988\nPOLYGON ((109.795 27.98008,…\n\n\nTongdao\n46549\n59330\nPOLYGON ((109.9294 26.46561…\n\n\nXinhuang\n20518\n35930\nPOLYGON ((109.227 27.43733,…\n\n\nXupu\n140576\n154439\nPOLYGON ((110.7189 28.30485…\n\n\nYuanling\n121601\n145795\nPOLYGON ((110.9652 28.99895…\n\n\nZhijiang\n92069\n112587\nPOLYGON ((109.8818 27.60661…\n\n\nLengshuijiang\n43258\n107515\nPOLYGON ((111.5307 27.81472…\n\n\nShuangfeng\n144567\n162322\nPOLYGON ((112.263 27.70421,…\n\n\nXinhua\n132119\n145517\nPOLYGON ((111.3345 28.19642…\n\n\nChengbu\n51694\n61826\nPOLYGON ((110.4455 26.69317…\n\n\nDongan\n59024\n79925\nPOLYGON ((111.4531 26.86812…\n\n\nDongkou\n69349\n82589\nPOLYGON ((110.6622 27.37305…\n\n\nLonghui\n73780\n83352\nPOLYGON ((110.985 27.65983,…\n\n\nShaodong\n94651\n119897\nPOLYGON ((111.9054 27.40254…\n\n\nSuining\n100680\n116749\nPOLYGON ((110.389 27.10006,…\n\n\nWugang\n69398\n81510\nPOLYGON ((110.9878 27.03345…\n\n\nXinning\n52798\n63530\nPOLYGON ((111.0736 26.84627…\n\n\nXinshao\n140472\n151986\nPOLYGON ((111.6013 27.58275…\n\n\nShaoshan\n118623\n174193\nPOLYGON ((112.5391 27.97742…\n\n\nXiangxiang\n180933\n210294\nPOLYGON ((112.4549 28.05783…\n\n\nBaojing\n82798\n97361\nPOLYGON ((109.7015 28.82844…\n\n\nFenghuang\n83090\n96472\nPOLYGON ((109.5239 28.19206…\n\n\nGuzhang\n97356\n108936\nPOLYGON ((109.8968 28.74034…\n\n\nHuayuan\n59482\n79819\nPOLYGON ((109.5647 28.61712…\n\n\nJishou\n77334\n108871\nPOLYGON ((109.8375 28.4696,…\n\n\nLongshan\n38777\n48531\nPOLYGON ((109.6337 29.62521…\n\n\nLuxi\n111463\n128935\nPOLYGON ((110.1067 28.41835…\n\n\nYongshun\n74715\n84305\nPOLYGON ((110.0003 29.29499…\n\n\nAnhua\n174391\n188958\nPOLYGON ((111.6034 28.63716…\n\n\nNan\n150558\n171869\nPOLYGON ((112.3232 29.46074…\n\n\nYuanjiang\n122144\n148402\nPOLYGON ((112.4391 29.1791,…\n\n\nJianghua\n68012\n83813\nPOLYGON ((111.6461 25.29661…\n\n\nLanshan\n84575\n104663\nPOLYGON ((112.2286 25.61123…\n\n\nNingyuan\n143045\n155742\nPOLYGON ((112.0715 26.09892…\n\n\nShuangpai\n51394\n73336\nPOLYGON ((111.8864 26.11957…\n\n\nXintian\n98279\n112705\nPOLYGON ((112.2578 26.0796,…\n\n\nHuarong\n47671\n78084\nPOLYGON ((112.9242 29.69134…\n\n\nLinxiang\n26360\n58257\nPOLYGON ((113.5502 29.67418…\n\n\nMiluo\n236917\n279414\nPOLYGON ((112.9902 29.02139…\n\n\nPingjiang\n220631\n237883\nPOLYGON ((113.8436 29.06152…\n\n\nXiangyin\n185290\n219273\nPOLYGON ((112.9173 28.98264…\n\n\nCili\n64640\n83354\nPOLYGON ((110.8822 29.69017…\n\n\nChaling\n70046\n90124\nPOLYGON ((113.7666 27.10573…\n\n\nLiling\n126971\n168462\nPOLYGON ((113.5673 27.94346…\n\n\nYanling\n144693\n165714\nPOLYGON ((113.9292 26.6154,…\n\n\nYou\n129404\n165668\nPOLYGON ((113.5879 27.41324…\n\n\nZhuzhou\n284074\n311663\nPOLYGON ((113.2493 28.02411…\n\n\nSangzhi\n112268\n126892\nPOLYGON ((110.556 29.40543,…\n\n\nYueyang\n203611\n229971\nPOLYGON ((113.343 29.61064,…\n\n\nQiyang\n145238\n165876\nPOLYGON ((111.5563 26.81318…\n\n\nTaojiang\n251536\n271045\nPOLYGON ((112.0508 28.67265…\n\n\nShaoyang\n108078\n117731\nPOLYGON ((111.5013 27.30207…\n\n\nLianyuan\n238300\n256646\nPOLYGON ((111.6789 28.02946…\n\n\nHongjiang\n108870\n126603\nPOLYGON ((110.1441 27.47513…\n\n\nHengyang\n108085\n127467\nPOLYGON ((112.7144 26.98613…\n\n\nGuiyang\n262835\n295688\nPOLYGON ((113.0811 26.04963…\n\n\nChangsha\n248182\n336838\nPOLYGON ((112.9421 28.03722…\n\n\nTaoyuan\n244850\n267729\nPOLYGON ((112.0612 29.32855…\n\n\nXiangtan\n404456\n431516\nPOLYGON ((113.0426 27.8942,…\n\n\nDao\n67608\n85667\nPOLYGON ((111.498 25.81679,…\n\n\nJiangyong\n33860\n51028\nPOLYGON ((111.3659 25.39472…"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-On_Ex01.html",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-On_Ex01.html",
    "title": "Hands-On Exercise 1 : Geospatial Wrangling With R",
    "section": "",
    "text": "Hello! In this page, i will be describing how i performed data wrangling - which is basically cleaning and transforming raw data into a more structured and usable format or later analysis."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-On_Ex01.html#installing-and-loading-r-packages",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-On_Ex01.html#installing-and-loading-r-packages",
    "title": "Hands-On Exercise 1 : Geospatial Wrangling With R",
    "section": "Installing and loading R packages",
    "text": "Installing and loading R packages\nIn this section, I will install and load ‘tidyverse’ and ‘sf’ packages.\n\nWhat are the sf and tidyverse packages?\nLet me give you a break down.\n\nTidyverse Package\nImagine you have a pile of dirty data that you need to handle. The Tidyverse package helps you handle your data. It actually has a bunch of packages within it to help handle data. It can do the following:\n\nreadr - reading and writing data into or out of a spreadsheet\ntidyr - organizing and tidying up your data\nggplot2 - visualizing your data\ndpylr - mainpulating your data, like doing some basic math to it.\n\n\n\nSF Package\nSF package provides us with tools to work with data related to maps and geospatial data. It can\n\nRead and write geospatial data from and into files.\nManipulate data - like cut out a specific area on a map\nVisualize data\n\nNow let’s load the packages into our environment.\n\n# Loads the p_load function on pacman which checks if packages are available, and then loads them into the R environment.\npacman::p_load(tidyverse, sf)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-On_Ex01.html#importing-geospatial-data",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-On_Ex01.html#importing-geospatial-data",
    "title": "Hands-On Exercise 1 : Geospatial Wrangling With R",
    "section": "Importing geospatial data",
    "text": "Importing geospatial data\n\nImporting the geospatial data in shapefile format\nHere we import the geospatial data which is in shapefile format as a a polygon feature data frame.\n\nWhat is a shapefile and what is a polygon feature data frame?\n\nShapefile\nShapefile format stores geographic location and attribute information of geographic features.\n\n\nPolygon feature data frame\nThink of polygons as shapes that represent areas on a map.\nData frames store details about the polygons (think area) on tables.\nSo polygon feature data frames store data about areas on the map.\n\n#st_read is a function from the SF package which helps with the handling of data. We read the data into the variable mpsz.\n\nmpsz &lt;- st_read(dsn = \"data/geospatial\",\n layer  = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `C:\\shaysnutss\\IS1455-GAA\\Hands-on_Ex\\Hands-on_Ex01\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\n\n\n\n\nImporting polyline feature data in shapefile form\n\nWhat is a polyline feature data frame?\nA polyline is made up of connected points and can represent things such as roads, rivers, or hiking trails.\nFeature Data Frames store information about these lines, like what is the name of the road, how long is it, what’s its surface?\nA polyline feature data frame stores information on lines that represent routes or rivers on a map.\nNow let’s import the polyline feature data\n\ncyclingpath = st_read(dsn = \"data/geospatial\", \n                         layer = \"CyclingPathGazette\")\n\nReading layer `CyclingPathGazette' from data source \n  `C:\\shaysnutss\\IS1455-GAA\\Hands-on_Ex\\Hands-on_Ex01\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 2558 features and 2 fields\nGeometry type: MULTILINESTRING\nDimension:     XY\nBounding box:  xmin: 11854.32 ymin: 28347.98 xmax: 42626.09 ymax: 48948.15\nProjected CRS: SVY21\n\n\n\n\n\nImporting GIS data in kml format\n\nWhat is a GIS data and kml format ?\n\nkml format\nIt is keyhole markup language used to describe places and features on a digital map. You can define shapes like drawing a line to show a hiking trail or a polygon to show a lake. You can also attach information to places.\n\n\nGIS data\nA map has information like shapes, paths, data and locations. GIS data is when all this information is organized in a computer friendly way. People use these data to analyse the environment, plan cities and much more.\nNow let’s import GIS data in kml format.\n\npreschool = st_read(\"data/geospatial/PreSchoolsLocation.kml\")\n\nReading layer `PRESCHOOLS_LOCATION' from data source \n  `C:\\shaysnutss\\IS1455-GAA\\Hands-on_Ex\\Hands-on_Ex01\\data\\geospatial\\PreSchoolsLocation.kml' \n  using driver `KML'\nSimple feature collection with 2290 features and 2 fields\nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 103.6878 ymin: 1.247759 xmax: 103.9897 ymax: 1.462134\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-On_Ex01.html#checking-out-the-contents-of-a-simple-feature-data-frame",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-On_Ex01.html#checking-out-the-contents-of-a-simple-feature-data-frame",
    "title": "Hands-On Exercise 1 : Geospatial Wrangling With R",
    "section": "Checking out the contents of a simple feature data frame",
    "text": "Checking out the contents of a simple feature data frame\n\nst_geometry\nShapes represent areas on a map. The data.frame in the package sf contains a column, and that column contains a list of those shapes in the specific class called ‘sfc’.\nLet’s execute this command\n\nst_geometry(mpsz)\n\nGeometry set for 323 features \nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\nFirst 5 geometries:\n\n\nThis commands tells you a few things:\n\nThe types of shapes you’re dealing with. Multipolygon means its an area with multiple connected lines.\nThe bounding box/ range of coordinates that coer the same\nAlso the first 5 shapes in the column of the data frame\n\n\n\nglimpse\nLet’s execute this command\n\nglimpse(mpsz)\n\nRows: 323\nColumns: 16\n$ OBJECTID   &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, …\n$ SUBZONE_NO &lt;int&gt; 1, 1, 3, 8, 3, 7, 9, 2, 13, 7, 12, 6, 1, 5, 1, 1, 3, 2, 2, …\n$ SUBZONE_N  &lt;chr&gt; \"MARINA SOUTH\", \"PEARL'S HILL\", \"BOAT QUAY\", \"HENDERSON HIL…\n$ SUBZONE_C  &lt;chr&gt; \"MSSZ01\", \"OTSZ01\", \"SRSZ03\", \"BMSZ08\", \"BMSZ03\", \"BMSZ07\",…\n$ CA_IND     &lt;chr&gt; \"Y\", \"Y\", \"Y\", \"N\", \"N\", \"N\", \"N\", \"Y\", \"N\", \"N\", \"N\", \"N\",…\n$ PLN_AREA_N &lt;chr&gt; \"MARINA SOUTH\", \"OUTRAM\", \"SINGAPORE RIVER\", \"BUKIT MERAH\",…\n$ PLN_AREA_C &lt;chr&gt; \"MS\", \"OT\", \"SR\", \"BM\", \"BM\", \"BM\", \"BM\", \"SR\", \"QT\", \"QT\",…\n$ REGION_N   &lt;chr&gt; \"CENTRAL REGION\", \"CENTRAL REGION\", \"CENTRAL REGION\", \"CENT…\n$ REGION_C   &lt;chr&gt; \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\",…\n$ INC_CRC    &lt;chr&gt; \"5ED7EB253F99252E\", \"8C7149B9EB32EEFC\", \"C35FEFF02B13E0E5\",…\n$ FMEL_UPD_D &lt;date&gt; 2014-12-05, 2014-12-05, 2014-12-05, 2014-12-05, 2014-12-05…\n$ X_ADDR     &lt;dbl&gt; 31595.84, 28679.06, 29654.96, 26782.83, 26201.96, 25358.82,…\n$ Y_ADDR     &lt;dbl&gt; 29220.19, 29782.05, 29974.66, 29933.77, 30005.70, 29991.38,…\n$ SHAPE_Leng &lt;dbl&gt; 5267.381, 3506.107, 1740.926, 3313.625, 2825.594, 4428.913,…\n$ SHAPE_Area &lt;dbl&gt; 1630379.27, 559816.25, 160807.50, 595428.89, 387429.44, 103…\n$ geometry   &lt;MULTIPOLYGON [m]&gt; MULTIPOLYGON (((31495.56 30..., MULTIPOLYGON (…\n\n\nThis command gives you a glimpse into the dataset.\nit tells you the following things:\n\nthe number of rows\nthe number of columns\nwhat columns are there\ndata type of each column\nfirst few values of each column\n\n\n\nhead()\nLet’s execute this command\n\nhead(mpsz, n=5) \n\nSimple feature collection with 5 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 25867.68 ymin: 28369.47 xmax: 32362.39 ymax: 30435.54\nProjected CRS: SVY21\n  OBJECTID SUBZONE_NO      SUBZONE_N SUBZONE_C CA_IND      PLN_AREA_N\n1        1          1   MARINA SOUTH    MSSZ01      Y    MARINA SOUTH\n2        2          1   PEARL'S HILL    OTSZ01      Y          OUTRAM\n3        3          3      BOAT QUAY    SRSZ03      Y SINGAPORE RIVER\n4        4          8 HENDERSON HILL    BMSZ08      N     BUKIT MERAH\n5        5          3        REDHILL    BMSZ03      N     BUKIT MERAH\n  PLN_AREA_C       REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR\n1         MS CENTRAL REGION       CR 5ED7EB253F99252E 2014-12-05 31595.84\n2         OT CENTRAL REGION       CR 8C7149B9EB32EEFC 2014-12-05 28679.06\n3         SR CENTRAL REGION       CR C35FEFF02B13E0E5 2014-12-05 29654.96\n4         BM CENTRAL REGION       CR 3775D82C5DDBEFBD 2014-12-05 26782.83\n5         BM CENTRAL REGION       CR 85D9ABEF0A40678F 2014-12-05 26201.96\n    Y_ADDR SHAPE_Leng SHAPE_Area                       geometry\n1 29220.19   5267.381  1630379.3 MULTIPOLYGON (((31495.56 30...\n2 29782.05   3506.107   559816.2 MULTIPOLYGON (((29092.28 30...\n3 29974.66   1740.926   160807.5 MULTIPOLYGON (((29932.33 29...\n4 29933.77   3313.625   595428.9 MULTIPOLYGON (((27131.28 30...\n5 30005.70   2825.594   387429.4 MULTIPOLYGON (((26451.03 30...\n\n\nThis provides you with complete information (columns and the number of values u want)."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-On_Ex01.html#plotting-data",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-On_Ex01.html#plotting-data",
    "title": "Hands-On Exercise 1 : Geospatial Wrangling With R",
    "section": "Plotting Data",
    "text": "Plotting Data\nNow, let’s plot the data in our mpsz as a visualization. We use the plot() function for it.\nThis provides the entire visualization.\n\nplot(mpsz)\n\n\n\n\nIf you want to just plot the different shapes in the map, we can do this:\n\nplot(st_geometry(mpsz))\n\n\n\n\nIf you want to plot just based off of one column/attribute, you can do this. Each color reprsents the a row.\n\nplot(mpsz[\"PLN_AREA_N\"])"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-On_Ex01.html#working-with-projection",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-On_Ex01.html#working-with-projection",
    "title": "Hands-On Exercise 1 : Geospatial Wrangling With R",
    "section": "Working with projection",
    "text": "Working with projection\nThere are multiple ways you can study geography. You can study it if it were on a flat piece of paper or curved 3d way on your laptop screen.\nWhen there are two different ways of expressing geography, the way the data is, is differnet. If you want to use two sets of geospatial data, you want to ensure the data has the same projection, i.e. represents locations on the earth’s surface the same way. Hence, you will have to translate one of the maps to the same projection of another.\n\nWorking with projection\nOne of the common issues that can happen is that the coordinate system of a geospatial data is mising or incorrect. To check and input the correct coordinate system, we execute the following command to check the coordinate reference system.\n\nst_crs(mpsz)\n\nCoordinate Reference System:\n  User input: SVY21 \n  wkt:\nPROJCRS[\"SVY21\",\n    BASEGEOGCRS[\"SVY21[WGS84]\",\n        DATUM[\"World Geodetic System 1984\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]],\n            ID[\"EPSG\",6326]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"Degree\",0.0174532925199433]]],\n    CONVERSION[\"unnamed\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]]]\n\n\nthen check at the end of the print if the ESPG code corresponds correctly to the coordinate system. Here, svy21 should correspond to 3414 not 9001. So we change it!\n\nmpsz3414 &lt;- st_set_crs(mpsz, 3414)\n\nNow let’s double check if the ESPG code is correct\n\nst_crs(mpsz3414)\n\nCoordinate Reference System:\n  User input: EPSG:3414 \n  wkt:\nPROJCRS[\"SVY21 / Singapore TM\",\n    BASEGEOGCRS[\"SVY21\",\n        DATUM[\"SVY21\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4757]],\n    CONVERSION[\"Singapore Transverse Mercator\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"northing (N)\",north,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"easting (E)\",east,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Cadastre, engineering survey, topographic mapping.\"],\n        AREA[\"Singapore - onshore and offshore.\"],\n        BBOX[1.13,103.59,1.47,104.07]],\n    ID[\"EPSG\",3414]]\n\n\n\n\nWorking with projection\nSometimes we change projection of a geospatial data to one that is congrunet with the type of analysis we are making.\nFor example, geographic coordinate system may not be appropriate if the analysis needs to use distance or/and area measurements.\nlet’s do some transformation here.\n\npreschool3414 &lt;- st_transform(preschool, \n                              crs = 3414)\n\n##Importing and Converting An Aspatial Data\n\n\nWhat is aspatial data?\nLet’s back track. Spatial data refers to physical location, shapes and positions of the objects on the earth’s surface.\nAspatial data refers to characteristics or properties of those objects. Airbnb is asptial data, so it focuses on what kind of house is it, how many bed rooms how many bathrooms, maybe its rental per night.\n\n\nImport spatial data\n\nlistings &lt;- read_csv(\"data/aspatial/listings.csv\")\n\nLet’s explore the data.\n\nlist(listings) \n\n[[1]]\n# A tibble: 3,457 × 18\n       id name      host_id host_name neighbourhood_group neighbourhood latitude\n    &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;               &lt;chr&gt;            &lt;dbl&gt;\n 1  71609 Villa in…  367042 Belinda   East Region         Tampines          1.35\n 2  71896 Home in …  367042 Belinda   East Region         Tampines          1.35\n 3  71903 Home in …  367042 Belinda   East Region         Tampines          1.35\n 4 275343 Rental u… 1439258 Kay       Central Region      Bukit Merah       1.29\n 5 275344 Rental u… 1439258 Kay       Central Region      Bukit Merah       1.29\n 6 289234 Home in …  367042 Belinda   East Region         Tampines          1.34\n 7 294281 Rental u… 1521514 Elizabeth Central Region      Newton            1.31\n 8 324945 Rental u… 1439258 Kay       Central Region      Bukit Merah       1.29\n 9 330095 Rental u… 1439258 Kay       Central Region      Bukit Merah       1.29\n10 369141 Place to… 1521514 Elizabeth Central Region      Newton            1.31\n# ℹ 3,447 more rows\n# ℹ 11 more variables: longitude &lt;dbl&gt;, room_type &lt;chr&gt;, price &lt;dbl&gt;,\n#   minimum_nights &lt;dbl&gt;, number_of_reviews &lt;dbl&gt;, last_review &lt;date&gt;,\n#   reviews_per_month &lt;dbl&gt;, calculated_host_listings_count &lt;dbl&gt;,\n#   availability_365 &lt;dbl&gt;, number_of_reviews_ltm &lt;dbl&gt;, license &lt;chr&gt;\n\n\n###Creating a simple feature data frame from an aspatial data frame\nLet’s do some conversion : transform one coordinate system to another.\nFirst code line, we convert the aspatial data from to a simple feature data frame. Then we change the coordinate system.\n\nlistings_sf &lt;- st_as_sf(listings, \n                       coords = c(\"longitude\", \"latitude\"),\n                       crs=4326) %&gt;%\n  st_transform(crs = 3414)\n\nThings to learn from the arguments above:\n\ncoords argument requires you to provide the column name of the x-coordinates first then followed by the column name of the y-coordinates.\ncrs argument requires you to provide the coordinates system in epsg format. EPSG: 4326 is wgs84 Geographic Coordinate System and EPSG: 3414 is Singapore SVY21 Projected Coordinate System. You can search for other country’s epsg code by referring to epsg.io.\n%&gt;% is used to nest st_transform() to transform the newly created simple feature data frame into svy21 projected coordinates system.\n\n\nglimpse(listings_sf)\n\nRows: 3,457\nColumns: 17\n$ id                             &lt;dbl&gt; 71609, 71896, 71903, 275343, 275344, 28…\n$ name                           &lt;chr&gt; \"Villa in Singapore · ★4.44 · 2 bedroom…\n$ host_id                        &lt;dbl&gt; 367042, 367042, 367042, 1439258, 143925…\n$ host_name                      &lt;chr&gt; \"Belinda\", \"Belinda\", \"Belinda\", \"Kay\",…\n$ neighbourhood_group            &lt;chr&gt; \"East Region\", \"East Region\", \"East Reg…\n$ neighbourhood                  &lt;chr&gt; \"Tampines\", \"Tampines\", \"Tampines\", \"Bu…\n$ room_type                      &lt;chr&gt; \"Private room\", \"Private room\", \"Privat…\n$ price                          &lt;dbl&gt; 150, 80, 80, 64, 78, 220, 85, 75, 69, 7…\n$ minimum_nights                 &lt;dbl&gt; 92, 92, 92, 60, 60, 92, 92, 60, 60, 92,…\n$ number_of_reviews              &lt;dbl&gt; 19, 24, 46, 20, 16, 12, 131, 17, 5, 81,…\n$ last_review                    &lt;date&gt; 2020-01-17, 2019-10-13, 2020-01-09, 20…\n$ reviews_per_month              &lt;dbl&gt; 0.13, 0.16, 0.30, 0.15, 0.11, 0.09, 0.9…\n$ calculated_host_listings_count &lt;dbl&gt; 5, 5, 5, 51, 51, 5, 7, 51, 51, 7, 7, 1,…\n$ availability_365               &lt;dbl&gt; 55, 91, 91, 183, 183, 54, 365, 183, 183…\n$ number_of_reviews_ltm          &lt;dbl&gt; 0, 0, 0, 0, 3, 0, 0, 1, 2, 0, 0, 0, 0, …\n$ license                        &lt;chr&gt; NA, NA, NA, \"S0399\", \"S0399\", NA, NA, \"…\n$ geometry                       &lt;POINT [m]&gt; POINT (41972.5 36390.05), POINT (…\n\n\nTable above shows the content of listing_sf. Notice that a new column called geometry has been added into the data frame. On the other hand, the longitude and latitude columns have been dropped from the data frame."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-On_Ex01.html#geoprocessing-with-sf-package",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-On_Ex01.html#geoprocessing-with-sf-package",
    "title": "Hands-On Exercise 1 : Geospatial Wrangling With R",
    "section": "Geoprocessing with sf package",
    "text": "Geoprocessing with sf package\nHere, we will be learning about buffering and point in polygon.\n###Buffering\n####What is bufferring\nLet’s say there is a point of interest like a cycling path You want to analyse a certain amount of area around it, and the amount would be the buffer. Let’s say buffer is 5, you would want to study the area in a 5 mile radius around it.\nLet’s have another working scenario:\nThe authority is planning to upgrade the exiting cycling path. To do so, they need to acquire 5 metres of reserved land on the both sides of the current cycling path. You are tasked to determine the extend of the land need to be acquired and their total area.\nLet’s create a data frame with a buffer zone around the cycling paths\n\nbuffer_cycling &lt;- st_buffer(cyclingpath, \n                               dist=5, nQuadSegs = 30)\n\nYou created the buffer in the previous command , now you can calculate the total area of the buffer by doing so.\n\nbuffer_cycling$AREA &lt;- st_area(buffer_cycling)\n\nNow the summation of the buffer land and the exisiting cycling path- total land involved.\n\nsum(buffer_cycling$AREA)\n\n1774367 [m^2]\n\n\n###Point-in-polygon count\n####What is Point-in-polygon count\nIt refers to counting the number of points in an area (polygon)\nLet’s say, a pre-school service group wants to find out the numbers of pre-schools in each Planning Subzone.\nThe code chunk below performs two operations at one go. Firstly, identify pre-schools located inside each Planning Subzone by using st_intersects(). Next, length() of Base R is used to calculate numbers of pre-schools that fall inside each planning subzone.\n\nmpsz3414$`PreSch Count`&lt;- lengths(st_intersects(mpsz3414, preschool3414))\n\n\nsummary(mpsz3414$`PreSch Count`)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   0.00    0.00    4.00    7.09   10.00   72.00 \n\n\nZone with most preschools\n\ntop_n(mpsz3414, 1, `PreSch Count`)\n\nSimple feature collection with 1 feature and 16 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 39655.33 ymin: 35966 xmax: 42940.57 ymax: 38622.37\nProjected CRS: SVY21 / Singapore TM\n  OBJECTID SUBZONE_NO     SUBZONE_N SUBZONE_C CA_IND PLN_AREA_N PLN_AREA_C\n1      189          2 TAMPINES EAST    TMSZ02      N   TAMPINES         TM\n     REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR   Y_ADDR SHAPE_Leng\n1 EAST REGION       ER 21658EAAF84F4D8D 2014-12-05 41122.55 37392.39   10180.62\n  SHAPE_Area                       geometry PreSch Count\n1    4339824 MULTIPOLYGON (((42196.76 38...           72\n\n\nCalculate density of presch\n\nmpsz3414$Area &lt;- mpsz3414 %&gt;%\n  st_area()\n\nmpsz3414 &lt;- mpsz3414 %&gt;%\n  mutate(`PreSch Density` = `PreSch Count`/Area * 1000000)\n\n##Exploratory Data Analysis (EDA)\nLet’s visualize our data to get a better idea of it.\n\nhist(mpsz3414$`PreSch Density`)\n\n\n\n\n\nggplot(data=mpsz3414, \n       aes(x= as.numeric(`PreSch Density`)))+\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"light blue\") +\n  labs(title = \"Are pre-school even distributed in Singapore?\",\n       subtitle= \"There are many planning sub-zones with a single pre-school, on the other hand, \\nthere are two planning sub-zones with at least 20 pre-schools\",\n      x = \"Pre-school density (per km sq)\",\n      y = \"Frequency\")\n\n\n\n\n\nggplot(data=mpsz3414, \n       aes(y = `PreSch Count`, \n           x= as.numeric(`PreSch Density`)))+\n  geom_point(color=\"black\", \n             fill=\"light blue\") +\n  xlim(0, 40) +\n  ylim(0, 40) +\n  labs(title = \"\",\n      x = \"Pre-school density (per km sq)\",\n      y = \"Pre-school count\")\n\n\n\n\nHello! In this page, i will be describing how i performed data wrangling - which is basically cleaning and transforming raw data into a more structured and usable format or later analysis."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-On_Ex01.html#installing-and-loading-r-packages-1",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-On_Ex01.html#installing-and-loading-r-packages-1",
    "title": "Hands-On Exercise 1 : Geospatial Wrangling With R",
    "section": "Installing and loading R packages",
    "text": "Installing and loading R packages\nIn this section, I will install and load ‘tidyverse’ and ‘sf’ packages.\n\nWhat are the sf and tidyverse packages?\nLet me give you a break down.\n\nTidyverse Package\nImagine you have a pile of dirty data that you need to handle. The Tidyverse package helps you handle your data. It actually has a bunch of packages within it to help handle data. It can do the following:\n\nreadr - reading and writing data into or out of a spreadsheet\ntidyr - organizing and tidying up your data\nggplot2 - visualizing your data\ndpylr - mainpulating your data, like doing some basic math to it.\n\n\n\nSF Package\nSF package provides us with tools to work with data related to maps and geospatial data. It can\n\nRead and write geospatial data from and into files.\nManipulate data - like cut out a specific area on a map\nVisualize data\n\nNow let’s load the packages into our environment.\n\n# Loads the p_load function on pacman which checks if packages are available, and then loads them into the R environment.\npacman::p_load(tidyverse, sf)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-On_Ex01.html#importing-geospatial-data-1",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-On_Ex01.html#importing-geospatial-data-1",
    "title": "Hands-On Exercise 1 : Geospatial Wrangling With R",
    "section": "Importing geospatial data",
    "text": "Importing geospatial data\n\nImporting the geospatial data in shapefile format\nHere we import the geospatial data which is in shapefile format as a a polygon feature data frame.\n\nWhat is a shapefile and what is a polygon feature data frame?\n\nShapefile\nShapefile format stores geographic location and attribute information of geographic features.\n\n\nPolygon feature data frame\nThink of polygons as shapes that represent areas on a map.\nData frames store details about the polygons (think area) on tables.\nSo polygon feature data frames store data about areas on the map.\n\n#st_read is a function from the SF package which helps with the handling of data. We read the data into the variable mpsz.\n\nmpsz &lt;- st_read(dsn = \"data/geospatial\",\n layer  = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `C:\\shaysnutss\\IS1455-GAA\\Hands-on_Ex\\Hands-on_Ex01\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\n\n\n\n\nImporting polyline feature data in shapefile form\n\nWhat is a polyline feature data frame?\nA polyline is made up of connected points and can represent things such as roads, rivers, or hiking trails.\nFeature Data Frames store information about these lines, like what is the name of the road, how long is it, what’s its surface?\nA polyline feature data frame stores information on lines that represent routes or rivers on a map.\nNow let’s import the polyline feature data\n\ncyclingpath = st_read(dsn = \"data/geospatial\", \n                         layer = \"CyclingPathGazette\")\n\nReading layer `CyclingPathGazette' from data source \n  `C:\\shaysnutss\\IS1455-GAA\\Hands-on_Ex\\Hands-on_Ex01\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 2558 features and 2 fields\nGeometry type: MULTILINESTRING\nDimension:     XY\nBounding box:  xmin: 11854.32 ymin: 28347.98 xmax: 42626.09 ymax: 48948.15\nProjected CRS: SVY21\n\n\n\n\n\nImporting GIS data in kml format\n\nWhat is a GIS data and kml format ?\n\nkml format\nIt is keyhole markup language used to describe places and features on a digital map. You can define shapes like drawing a line to show a hiking trail or a polygon to show a lake. You can also attach information to places.\n\n\nGIS data\nA map has information like shapes, paths, data and locations. GIS data is when all this information is organized in a computer friendly way. People use these data to analyse the environment, plan cities and much more.\nNow let’s import GIS data in kml format.\n\npreschool = st_read(\"data/geospatial/PreSchoolsLocation.kml\")\n\nReading layer `PRESCHOOLS_LOCATION' from data source \n  `C:\\shaysnutss\\IS1455-GAA\\Hands-on_Ex\\Hands-on_Ex01\\data\\geospatial\\PreSchoolsLocation.kml' \n  using driver `KML'\nSimple feature collection with 2290 features and 2 fields\nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 103.6878 ymin: 1.247759 xmax: 103.9897 ymax: 1.462134\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-On_Ex01.html#checking-out-the-contents-of-a-simple-feature-data-frame-1",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-On_Ex01.html#checking-out-the-contents-of-a-simple-feature-data-frame-1",
    "title": "Hands-On Exercise 1 : Geospatial Wrangling With R",
    "section": "Checking out the contents of a simple feature data frame",
    "text": "Checking out the contents of a simple feature data frame\n\nst_geometry\nShapes represent areas on a map. The data.frame in the package sf contains a column, and that column contains a list of those shapes in the specific class called ‘sfc’.\nLet’s execute this command\n\nst_geometry(mpsz)\n\nGeometry set for 323 features \nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\nFirst 5 geometries:\n\n\nThis commands tells you a few things:\n\nThe types of shapes you’re dealing with. Multipolygon means its an area with multiple connected lines.\nThe bounding box/ range of coordinates that coer the same\nAlso the first 5 shapes in the column of the data frame\n\n\n\nglimpse\nLet’s execute this command\n\nglimpse(mpsz)\n\nRows: 323\nColumns: 16\n$ OBJECTID   &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, …\n$ SUBZONE_NO &lt;int&gt; 1, 1, 3, 8, 3, 7, 9, 2, 13, 7, 12, 6, 1, 5, 1, 1, 3, 2, 2, …\n$ SUBZONE_N  &lt;chr&gt; \"MARINA SOUTH\", \"PEARL'S HILL\", \"BOAT QUAY\", \"HENDERSON HIL…\n$ SUBZONE_C  &lt;chr&gt; \"MSSZ01\", \"OTSZ01\", \"SRSZ03\", \"BMSZ08\", \"BMSZ03\", \"BMSZ07\",…\n$ CA_IND     &lt;chr&gt; \"Y\", \"Y\", \"Y\", \"N\", \"N\", \"N\", \"N\", \"Y\", \"N\", \"N\", \"N\", \"N\",…\n$ PLN_AREA_N &lt;chr&gt; \"MARINA SOUTH\", \"OUTRAM\", \"SINGAPORE RIVER\", \"BUKIT MERAH\",…\n$ PLN_AREA_C &lt;chr&gt; \"MS\", \"OT\", \"SR\", \"BM\", \"BM\", \"BM\", \"BM\", \"SR\", \"QT\", \"QT\",…\n$ REGION_N   &lt;chr&gt; \"CENTRAL REGION\", \"CENTRAL REGION\", \"CENTRAL REGION\", \"CENT…\n$ REGION_C   &lt;chr&gt; \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\",…\n$ INC_CRC    &lt;chr&gt; \"5ED7EB253F99252E\", \"8C7149B9EB32EEFC\", \"C35FEFF02B13E0E5\",…\n$ FMEL_UPD_D &lt;date&gt; 2014-12-05, 2014-12-05, 2014-12-05, 2014-12-05, 2014-12-05…\n$ X_ADDR     &lt;dbl&gt; 31595.84, 28679.06, 29654.96, 26782.83, 26201.96, 25358.82,…\n$ Y_ADDR     &lt;dbl&gt; 29220.19, 29782.05, 29974.66, 29933.77, 30005.70, 29991.38,…\n$ SHAPE_Leng &lt;dbl&gt; 5267.381, 3506.107, 1740.926, 3313.625, 2825.594, 4428.913,…\n$ SHAPE_Area &lt;dbl&gt; 1630379.27, 559816.25, 160807.50, 595428.89, 387429.44, 103…\n$ geometry   &lt;MULTIPOLYGON [m]&gt; MULTIPOLYGON (((31495.56 30..., MULTIPOLYGON (…\n\n\nThis command gives you a glimpse into the dataset.\nit tells you the following things:\n\nthe number of rows\nthe number of columns\nwhat columns are there\ndata type of each column\nfirst few values of each column\n\n\n\nhead()\nLet’s execute this command\n\nhead(mpsz, n=5) \n\nSimple feature collection with 5 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 25867.68 ymin: 28369.47 xmax: 32362.39 ymax: 30435.54\nProjected CRS: SVY21\n  OBJECTID SUBZONE_NO      SUBZONE_N SUBZONE_C CA_IND      PLN_AREA_N\n1        1          1   MARINA SOUTH    MSSZ01      Y    MARINA SOUTH\n2        2          1   PEARL'S HILL    OTSZ01      Y          OUTRAM\n3        3          3      BOAT QUAY    SRSZ03      Y SINGAPORE RIVER\n4        4          8 HENDERSON HILL    BMSZ08      N     BUKIT MERAH\n5        5          3        REDHILL    BMSZ03      N     BUKIT MERAH\n  PLN_AREA_C       REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR\n1         MS CENTRAL REGION       CR 5ED7EB253F99252E 2014-12-05 31595.84\n2         OT CENTRAL REGION       CR 8C7149B9EB32EEFC 2014-12-05 28679.06\n3         SR CENTRAL REGION       CR C35FEFF02B13E0E5 2014-12-05 29654.96\n4         BM CENTRAL REGION       CR 3775D82C5DDBEFBD 2014-12-05 26782.83\n5         BM CENTRAL REGION       CR 85D9ABEF0A40678F 2014-12-05 26201.96\n    Y_ADDR SHAPE_Leng SHAPE_Area                       geometry\n1 29220.19   5267.381  1630379.3 MULTIPOLYGON (((31495.56 30...\n2 29782.05   3506.107   559816.2 MULTIPOLYGON (((29092.28 30...\n3 29974.66   1740.926   160807.5 MULTIPOLYGON (((29932.33 29...\n4 29933.77   3313.625   595428.9 MULTIPOLYGON (((27131.28 30...\n5 30005.70   2825.594   387429.4 MULTIPOLYGON (((26451.03 30...\n\n\nThis provides you with complete information (columns and the number of values u want)."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-On_Ex01.html#plotting-data-1",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-On_Ex01.html#plotting-data-1",
    "title": "Hands-On Exercise 1 : Geospatial Wrangling With R",
    "section": "Plotting Data",
    "text": "Plotting Data\nNow, let’s plot the data in our mpsz as a visualization. We use the plot() function for it.\nThis provides the entire visualization.\n\nplot(mpsz)\n\n\n\n\nIf you want to just plot the different shapes in the map, we can do this:\n\nplot(st_geometry(mpsz))\n\n\n\n\nIf you want to plot just based off of one column/attribute, you can do this. Each color reprsents the a row.\n\nplot(mpsz[\"PLN_AREA_N\"])"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-On_Ex01.html#working-with-projection-3",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-On_Ex01.html#working-with-projection-3",
    "title": "Hands-On Exercise 1 : Geospatial Wrangling With R",
    "section": "Working with projection",
    "text": "Working with projection\nThere are multiple ways you can study geography. You can study it if it were on a flat piece of paper or curved 3d way on your laptop screen.\nWhen there are two different ways of expressing geography, the way the data is, is differnet. If you want to use two sets of geospatial data, you want to ensure the data has the same projection, i.e. represents locations on the earth’s surface the same way. Hence, you will have to translate one of the maps to the same projection of another.\n\nWorking with projection\nOne of the common issues that can happen is that the coordinate system of a geospatial data is mising or incorrect. To check and input the correct coordinate system, we execute the following command to check the coordinate reference system.\n\nst_crs(mpsz)\n\nCoordinate Reference System:\n  User input: SVY21 \n  wkt:\nPROJCRS[\"SVY21\",\n    BASEGEOGCRS[\"SVY21[WGS84]\",\n        DATUM[\"World Geodetic System 1984\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]],\n            ID[\"EPSG\",6326]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"Degree\",0.0174532925199433]]],\n    CONVERSION[\"unnamed\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]]]\n\n\nthen check at the end of the print if the ESPG code corresponds correctly to the coordinate system. Here, svy21 should correspond to 3414 not 9001. So we change it!\n\nmpsz3414 &lt;- st_set_crs(mpsz, 3414)\n\nNow let’s double check if the ESPG code is correct\n\nst_crs(mpsz3414)\n\nCoordinate Reference System:\n  User input: EPSG:3414 \n  wkt:\nPROJCRS[\"SVY21 / Singapore TM\",\n    BASEGEOGCRS[\"SVY21\",\n        DATUM[\"SVY21\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4757]],\n    CONVERSION[\"Singapore Transverse Mercator\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"northing (N)\",north,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"easting (E)\",east,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Cadastre, engineering survey, topographic mapping.\"],\n        AREA[\"Singapore - onshore and offshore.\"],\n        BBOX[1.13,103.59,1.47,104.07]],\n    ID[\"EPSG\",3414]]\n\n\n\n\nWorking with projection\nSometimes we change projection of a geospatial data to one that is congrunet with the type of analysis we are making.\nFor example, geographic coordinate system may not be appropriate if the analysis needs to use distance or/and area measurements.\nlet’s do some transformation here.\n\npreschool3414 &lt;- st_transform(preschool, \n                              crs = 3414)\n\n##Importing and Converting An Aspatial Data\n\n\nWhat is aspatial data?\nLet’s back track. Spatial data refers to physical location, shapes and positions of the objects on the earth’s surface.\nAspatial data refers to characteristics or properties of those objects. Airbnb is asptial data, so it focuses on what kind of house is it, how many bed rooms how many bathrooms, maybe its rental per night.\n\n\nImport spatial data\n\nlistings &lt;- read_csv(\"data/aspatial/listings.csv\")\n\nLet’s explore the data.\n\nlist(listings) \n\n[[1]]\n# A tibble: 3,457 × 18\n       id name      host_id host_name neighbourhood_group neighbourhood latitude\n    &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;               &lt;chr&gt;            &lt;dbl&gt;\n 1  71609 Villa in…  367042 Belinda   East Region         Tampines          1.35\n 2  71896 Home in …  367042 Belinda   East Region         Tampines          1.35\n 3  71903 Home in …  367042 Belinda   East Region         Tampines          1.35\n 4 275343 Rental u… 1439258 Kay       Central Region      Bukit Merah       1.29\n 5 275344 Rental u… 1439258 Kay       Central Region      Bukit Merah       1.29\n 6 289234 Home in …  367042 Belinda   East Region         Tampines          1.34\n 7 294281 Rental u… 1521514 Elizabeth Central Region      Newton            1.31\n 8 324945 Rental u… 1439258 Kay       Central Region      Bukit Merah       1.29\n 9 330095 Rental u… 1439258 Kay       Central Region      Bukit Merah       1.29\n10 369141 Place to… 1521514 Elizabeth Central Region      Newton            1.31\n# ℹ 3,447 more rows\n# ℹ 11 more variables: longitude &lt;dbl&gt;, room_type &lt;chr&gt;, price &lt;dbl&gt;,\n#   minimum_nights &lt;dbl&gt;, number_of_reviews &lt;dbl&gt;, last_review &lt;date&gt;,\n#   reviews_per_month &lt;dbl&gt;, calculated_host_listings_count &lt;dbl&gt;,\n#   availability_365 &lt;dbl&gt;, number_of_reviews_ltm &lt;dbl&gt;, license &lt;chr&gt;\n\n\n###Creating a simple feature data frame from an aspatial data frame\nLet’s do some conversion : transform one coordinate system to another.\nFirst code line, we convert the aspatial data from to a simple feature data frame. Then we change the coordinate system.\n\nlistings_sf &lt;- st_as_sf(listings, \n                       coords = c(\"longitude\", \"latitude\"),\n                       crs=4326) %&gt;%\n  st_transform(crs = 3414)\n\nThings to learn from the arguments above:\n\ncoords argument requires you to provide the column name of the x-coordinates first then followed by the column name of the y-coordinates.\ncrs argument requires you to provide the coordinates system in epsg format. EPSG: 4326 is wgs84 Geographic Coordinate System and EPSG: 3414 is Singapore SVY21 Projected Coordinate System. You can search for other country’s epsg code by referring to epsg.io.\n%&gt;% is used to nest st_transform() to transform the newly created simple feature data frame into svy21 projected coordinates system.\n\n\nglimpse(listings_sf)\n\nRows: 3,457\nColumns: 17\n$ id                             &lt;dbl&gt; 71609, 71896, 71903, 275343, 275344, 28…\n$ name                           &lt;chr&gt; \"Villa in Singapore · ★4.44 · 2 bedroom…\n$ host_id                        &lt;dbl&gt; 367042, 367042, 367042, 1439258, 143925…\n$ host_name                      &lt;chr&gt; \"Belinda\", \"Belinda\", \"Belinda\", \"Kay\",…\n$ neighbourhood_group            &lt;chr&gt; \"East Region\", \"East Region\", \"East Reg…\n$ neighbourhood                  &lt;chr&gt; \"Tampines\", \"Tampines\", \"Tampines\", \"Bu…\n$ room_type                      &lt;chr&gt; \"Private room\", \"Private room\", \"Privat…\n$ price                          &lt;dbl&gt; 150, 80, 80, 64, 78, 220, 85, 75, 69, 7…\n$ minimum_nights                 &lt;dbl&gt; 92, 92, 92, 60, 60, 92, 92, 60, 60, 92,…\n$ number_of_reviews              &lt;dbl&gt; 19, 24, 46, 20, 16, 12, 131, 17, 5, 81,…\n$ last_review                    &lt;date&gt; 2020-01-17, 2019-10-13, 2020-01-09, 20…\n$ reviews_per_month              &lt;dbl&gt; 0.13, 0.16, 0.30, 0.15, 0.11, 0.09, 0.9…\n$ calculated_host_listings_count &lt;dbl&gt; 5, 5, 5, 51, 51, 5, 7, 51, 51, 7, 7, 1,…\n$ availability_365               &lt;dbl&gt; 55, 91, 91, 183, 183, 54, 365, 183, 183…\n$ number_of_reviews_ltm          &lt;dbl&gt; 0, 0, 0, 0, 3, 0, 0, 1, 2, 0, 0, 0, 0, …\n$ license                        &lt;chr&gt; NA, NA, NA, \"S0399\", \"S0399\", NA, NA, \"…\n$ geometry                       &lt;POINT [m]&gt; POINT (41972.5 36390.05), POINT (…\n\n\nTable above shows the content of listing_sf. Notice that a new column called geometry has been added into the data frame. On the other hand, the longitude and latitude columns have been dropped from the data frame."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-On_Ex01.html#geoprocessing-with-sf-package-1",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-On_Ex01.html#geoprocessing-with-sf-package-1",
    "title": "Hands-On Exercise 1 : Geospatial Wrangling With R",
    "section": "Geoprocessing with sf package",
    "text": "Geoprocessing with sf package\nHere, we will be learning about buffering and point in polygon.\n###Buffering\n####What is bufferring\nLet’s say there is a point of interest like a cycling path You want to analyse a certain amount of area around it, and the amount would be the buffer. Let’s say buffer is 5, you would want to study the area in a 5 mile radius around it.\nLet’s have another working scenario:\nThe authority is planning to upgrade the exiting cycling path. To do so, they need to acquire 5 metres of reserved land on the both sides of the current cycling path. You are tasked to determine the extend of the land need to be acquired and their total area.\nLet’s create a data frame with a buffer zone around the cycling paths\n\nbuffer_cycling &lt;- st_buffer(cyclingpath, \n                               dist=5, nQuadSegs = 30)\n\nYou created the buffer in the previous command , now you can calculate the total area of the buffer by doing so.\n\nbuffer_cycling$AREA &lt;- st_area(buffer_cycling)\n\nNow the summation of the buffer land and the exisiting cycling path- total land involved.\n\nsum(buffer_cycling$AREA)\n\n1774367 [m^2]\n\n\n###Point-in-polygon count\n####What is Point-in-polygon count\nIt refers to counting the number of points in an area (polygon)\nLet’s say, a pre-school service group wants to find out the numbers of pre-schools in each Planning Subzone.\nThe code chunk below performs two operations at one go. Firstly, identify pre-schools located inside each Planning Subzone by using st_intersects(). Next, length() of Base R is used to calculate numbers of pre-schools that fall inside each planning subzone.\n\nmpsz3414$`PreSch Count`&lt;- lengths(st_intersects(mpsz3414, preschool3414))\n\n\nsummary(mpsz3414$`PreSch Count`)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   0.00    0.00    4.00    7.09   10.00   72.00 \n\n\nZone with most preschools\n\ntop_n(mpsz3414, 1, `PreSch Count`)\n\nSimple feature collection with 1 feature and 16 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 39655.33 ymin: 35966 xmax: 42940.57 ymax: 38622.37\nProjected CRS: SVY21 / Singapore TM\n  OBJECTID SUBZONE_NO     SUBZONE_N SUBZONE_C CA_IND PLN_AREA_N PLN_AREA_C\n1      189          2 TAMPINES EAST    TMSZ02      N   TAMPINES         TM\n     REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR   Y_ADDR SHAPE_Leng\n1 EAST REGION       ER 21658EAAF84F4D8D 2014-12-05 41122.55 37392.39   10180.62\n  SHAPE_Area                       geometry PreSch Count\n1    4339824 MULTIPOLYGON (((42196.76 38...           72\n\n\nCalculate density of presch\n\nmpsz3414$Area &lt;- mpsz3414 %&gt;%\n  st_area()\n\nmpsz3414 &lt;- mpsz3414 %&gt;%\n  mutate(`PreSch Density` = `PreSch Count`/Area * 1000000)\n\n##Exploratory Data Analysis (EDA)\nLet’s visualize our data to get a better idea of it.\n\nhist(mpsz3414$`PreSch Density`)\n\n\n\n\n\nggplot(data=mpsz3414, \n       aes(x= as.numeric(`PreSch Density`)))+\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"light blue\") +\n  labs(title = \"Are pre-school even distributed in Singapore?\",\n       subtitle= \"There are many planning sub-zones with a single pre-school, on the other hand, \\nthere are two planning sub-zones with at least 20 pre-schools\",\n      x = \"Pre-school density (per km sq)\",\n      y = \"Frequency\")\n\n\n\n\n\nggplot(data=mpsz3414, \n       aes(y = `PreSch Count`, \n           x= as.numeric(`PreSch Density`)))+\n  geom_point(color=\"black\", \n             fill=\"light blue\") +\n  xlim(0, 40) +\n  ylim(0, 40) +\n  labs(title = \"\",\n      x = \"Pre-school density (per km sq)\",\n      y = \"Pre-school count\")"
  },
  {
    "objectID": "data/MPSZ-2019.html",
    "href": "data/MPSZ-2019.html",
    "title": "IS415-GAA",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n        0 0     false"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "Hands-on_Ex/Hand-on_Ex03/Hands-on_Ex03.html",
    "href": "Hands-on_Ex/Hand-on_Ex03/Hands-on_Ex03.html",
    "title": "IS415-GAA",
    "section": "",
    "text": "here we are doing spatial point analysis where we are basically studying and analysing the spread of points in a space.\nIt helps us answer tje following questions:\n\nare the childcare centres in Singapore randomly distributed throughout the country?\nif the answer is not, then the next logical question is where are the locations with higher concentration of childcare centres?"
  },
  {
    "objectID": "Hands-on_Ex/Hand-on_Ex03/Hands-on_Ex03.html#overview",
    "href": "Hands-on_Ex/Hand-on_Ex03/Hands-on_Ex03.html#overview",
    "title": "IS415-GAA",
    "section": "",
    "text": "here we are doing spatial point analysis where we are basically studying and analysing the spread of points in a space.\nIt helps us answer tje following questions:\n\nare the childcare centres in Singapore randomly distributed throughout the country?\nif the answer is not, then the next logical question is where are the locations with higher concentration of childcare centres?"
  },
  {
    "objectID": "Hands-on_Ex/Hand-on_Ex03/Hands-on_Ex03.html#data",
    "href": "Hands-on_Ex/Hand-on_Ex03/Hands-on_Ex03.html#data",
    "title": "IS415-GAA",
    "section": "Data",
    "text": "Data\n\nchildcare\nMP14_SUBZONE_WEB_PL\nCostalOutline"
  },
  {
    "objectID": "Hands-on_Ex/Hand-on_Ex03/Hands-on_Ex03.html#installing-and-loading-the-r-packages",
    "href": "Hands-on_Ex/Hand-on_Ex03/Hands-on_Ex03.html#installing-and-loading-the-r-packages",
    "title": "IS415-GAA",
    "section": "Installing and loading the R packages",
    "text": "Installing and loading the R packages\nWe need to install and load the R packages into the R environment to use them\n\npacman::p_load(maptools, sf, raster, spatstat, tmap)\n\nIf you aren’t able to install maptools, do this:\n\ninstall.packages(\"maptools\", repos=\"http://R-Forge.R-project.org\")\n\nWarning: package 'maptools' is in use and will not be installed"
  },
  {
    "objectID": "Hands-on_Ex/Hand-on_Ex03/Hands-on_Ex03.html#spatial-data-wrangling",
    "href": "Hands-on_Ex/Hand-on_Ex03/Hands-on_Ex03.html#spatial-data-wrangling",
    "title": "IS415-GAA",
    "section": "Spatial Data Wrangling",
    "text": "Spatial Data Wrangling\n\nImporting the spatial data\nWe will use the st_read() function of sf package to import geospatial datasets into R.\nLet’s import the first one of ChildCareServices:\n\nchildcare_sf &lt;- st_read(\"data/ChildCareServices.geojson\")\n\nReading layer `ChildCareServices' from data source \n  `C:\\shaysnutss\\IS1455-GAA\\Hands-on_Ex\\Hand-on_Ex03\\data\\ChildCareServices.geojson' \n  using driver `GeoJSON'\nSimple feature collection with 1925 features and 2 fields\nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 103.6878 ymin: 1.247759 xmax: 103.9897 ymax: 1.462134\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84\n\n\nWe can see that the CRS is not what we want let’s transform it to SVY21\n\nchildcare_sf &lt;- st_transform(childcare_sf, crs = 3414)\n\nLet’s print out childcare_sf to see what is the CRS.\n\nst_geometry(childcare_sf)\n\nGeometry set for 1925 features \nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 11810.03 ymin: 25596.33 xmax: 45404.24 ymax: 49300.88\nz_range:       zmin: 0 zmax: 0\nProjected CRS: SVY21 / Singapore TM\nFirst 5 geometries:\n\n\nPOINT Z (40985.94 33848.38 0)\n\n\nPOINT Z (28308.65 45530.47 0)\n\n\nPOINT Z (17828.84 36607.36 0)\n\n\nPOINT Z (25579.73 29221.89 0)\n\n\nPOINT Z (38981.02 32483.41 0)\n\n\nLet’s import coastal outline.\n\nsg_sf &lt;- st_read(dsn = \"data/geospatial\", layer=\"CostalOutline\")\n\nReading layer `CostalOutline' from data source \n  `C:\\shaysnutss\\IS1455-GAA\\Hands-on_Ex\\Hand-on_Ex03\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 60 features and 4 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 2663.926 ymin: 16357.98 xmax: 56047.79 ymax: 50244.03\nProjected CRS: SVY21\n\n\nWhen we look at the coordinate reference system of coastal outline, we see that the user input is SVY21 but the correct EPSG code is 9001 instead of 3414.\n\nst_crs(sg_sf)\n\nCoordinate Reference System:\n  User input: SVY21 \n  wkt:\nPROJCRS[\"SVY21\",\n    BASEGEOGCRS[\"SVY21[WGS84]\",\n        DATUM[\"World Geodetic System 1984\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]],\n            ID[\"EPSG\",6326]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"Degree\",0.0174532925199433]]],\n    CONVERSION[\"unnamed\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]]]\n\n\nSo here we change it to 3414.\nReminder we don’t use st_transform() because the CRS is already SVY21, it’s just the ESPG code that is wrong!\n\nsg_sf &lt;- st_set_crs(sg_sf, 3414)\n\nWarning: st_crs&lt;- : replacing crs does not reproject data; use st_transform for\nthat\n\n\nNow let’s recheck if the coordiante system and ESPG code match.\n\nst_crs(sg_sf)\n\nCoordinate Reference System:\n  User input: EPSG:3414 \n  wkt:\nPROJCRS[\"SVY21 / Singapore TM\",\n    BASEGEOGCRS[\"SVY21\",\n        DATUM[\"SVY21\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4757]],\n    CONVERSION[\"Singapore Transverse Mercator\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"northing (N)\",north,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"easting (E)\",east,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Cadastre, engineering survey, topographic mapping.\"],\n        AREA[\"Singapore - onshore and offshore.\"],\n        BBOX[1.13,103.59,1.47,104.07]],\n    ID[\"EPSG\",3414]]\n\n\nNow let’s import MP_14_SUBZONE_WEB_PL.\n\nmpsz_sf &lt;- st_read(dsn = \"data/geospatial\", \n                layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `C:\\shaysnutss\\IS1455-GAA\\Hands-on_Ex\\Hand-on_Ex03\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\nLet’s check the coordinate system, it has to be SVY21 so that it is projected coordinate system, and the ESPG code has to match the SVY21.\n\nst_crs(mpsz_sf)\n\nCoordinate Reference System:\n  User input: SVY21 \n  wkt:\nPROJCRS[\"SVY21\",\n    BASEGEOGCRS[\"SVY21[WGS84]\",\n        DATUM[\"World Geodetic System 1984\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]],\n            ID[\"EPSG\",6326]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"Degree\",0.0174532925199433]]],\n    CONVERSION[\"unnamed\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]]]\n\n\nSince it’s 9001 and not 3414, let’s change the code.\n\nmpsz_sf &lt;- st_set_crs(mpsz_sf, 3414)\n\nWarning: st_crs&lt;- : replacing crs does not reproject data; use st_transform for\nthat\n\n\nLet’s check.\n\nst_crs(mpsz_sf)\n\nCoordinate Reference System:\n  User input: EPSG:3414 \n  wkt:\nPROJCRS[\"SVY21 / Singapore TM\",\n    BASEGEOGCRS[\"SVY21\",\n        DATUM[\"SVY21\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4757]],\n    CONVERSION[\"Singapore Transverse Mercator\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"northing (N)\",north,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"easting (E)\",east,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Cadastre, engineering survey, topographic mapping.\"],\n        AREA[\"Singapore - onshore and offshore.\"],\n        BBOX[1.13,103.59,1.47,104.07]],\n    ID[\"EPSG\",3414]]\n\n\n\n\nMapping the Geospatial data sets\n\ncol_names &lt;- names(childcare_sf)\nprint(col_names)\n\n[1] \"Name\"        \"Description\" \"geometry\"   \n\n\n\ntmap_mode(\"plot\")\n\ntmap mode set to plotting\n\ntm_shape(mpsz_sf) +\n  tm_polygons() +\n  tm_shape(childcare_sf) +  \n  tm_dots()"
  },
  {
    "objectID": "Hands-on_Ex/Hand-on_Ex03/Hands-on_Ex03.html#geospatial-data-wrangling",
    "href": "Hands-on_Ex/Hand-on_Ex03/Hands-on_Ex03.html#geospatial-data-wrangling",
    "title": "IS415-GAA",
    "section": "Geospatial Data Wrangling",
    "text": "Geospatial Data Wrangling\nAlthough simple feature data frame is gaining popularity again sp’s Spatial* classes, there are, however, many geospatial analysis packages require the input geospatial data in sp’s Spatial* classes. In this section, you will learn how to convert simple feature data frame to sp’s Spatial* class.\n\nConverting sf data frames to sp’s Spatial* class\n\nchildcare &lt;- as_Spatial(childcare_sf)\nmpsz &lt;- as_Spatial(mpsz_sf)\nsg &lt;- as_Spatial(sg_sf)\n\n\n#print(as.data.frame(childcare))\n\n\n#childcare\n\n\n#mpsz\n\n\n#sg\n\n\n\nConverting the Spatial* class into generic sp format\nspatstat requires the analytical data in ppp object form. There is no direct way to convert a Spatial* classes into ppp object. \nSpatial* classes -&gt; Spatial -&gt; ppp\nLet’s do spatial* classes -&gt; spatial first\nRemeber to use the correct type of shape\n\nchildcare_sp &lt;- as(childcare, \"SpatialPoints\")\nsg_sp &lt;- as(sg, \"SpatialPolygons\")\n\nWhat are the differences between Spatial* classes and generic sp object?\nIn summary, you would use Spatial* classes when you have simple spatial data without additional attributes, and you want a straightforward representation of points, lines, or polygons. On the other hand, if your spatial data includes associated attributes and you need to perform in-depth analysis, you would use generic sp objects like SpatialPointsDataFrame or SpatialPolygonsDataFrame.\nFor example, if you only need to plot the locations of childcare centers on a map, you can use the SpatialPoints class. However, if you want to perform statistical analysis on the childcare data, including attributes such as capacity or age group, you would convert it into a SpatialPointsDataFrame, which allows you to work with both spatial and attribute data seamlessly.\n\n\nConverting the generic sp format into spatstat’s ppp format\nsp -&gt; ppp. And now you can put it into spatstat.\nNow, we will use as.ppp() function of spatstat to convert the spatial data into spatstat’s ppp object format.\n\nchildcare_ppp &lt;- as(childcare_sp, \"ppp\")\nchildcare_ppp\n\nPlanar point pattern: 1925 points\nwindow: rectangle = [11810.03, 45404.24] x [25596.33, 49300.88] units\n\n\n\nplot(childcare_ppp)\n\n\n\n\nwhat’s the difference between plotting a geojson with spatial points and plotting the same data thats converted to ppp format\nPlotting a GeoJSON file with spatial points typically results in a standard spatial point map. Each point is displayed as a symbol or marker on the map, and you can customize the appearance of the points, such as color, size, and shape.\nConverting data to ppp format is typically done when you intend to perform spatial point process analysis. In this context, a point pattern represents the locations of events or objects of interest (e.g., tree locations, disease cases). ppp objects are used in the spatstat package for advanced spatial point pattern analysis. This includes tasks like assessing spatial clustering, estimating intensity, performing K-functions analysis, and simulating point patterns under different models.\n\nsummary(childcare_ppp)\n\nPlanar point pattern:  1925 points\nAverage intensity 2.417323e-06 points per square unit\n\n*Pattern contains duplicated points*\n\nCoordinates are given to 3 decimal places\ni.e. rounded to the nearest multiple of 0.001 units\n\nWindow: rectangle = [11810.03, 45404.24] x [25596.33, 49300.88] units\n                    (33590 x 23700 units)\nWindow area = 796335000 square units\n\n\n\n\nHandling duplicated points\nDuplicates, in this context, are multiple points that occupy exactly the same location in space.\nCoincident points are points that are so close to each other that they are considered to occupy the same location within a certain tolerance threshold.\nThese duplicates or coincident points can occur for various reasons, such as measurement errors, data collection methods, or the nature of the phenomenon being studied.\nLet’s check if there are any duplicates in the data:\n\nany(duplicated(childcare_ppp))\n\n[1] TRUE\n\n\nThe multiplicity() function is a function or method typically provided by spatial point pattern analysis software or packages like spatstat. Its purpose is to calculate the multiplicity of points in a point pattern. Multiplicity refers to the number of points that occupy the same location (coincident points) at each location where such coincidences occur.\n\n# Count the number of coincident\n#points in the childcare_ppp point pattern\nmultiplicity(childcare_ppp)\n\n\ntmap_mode('plot')\n\ntmap mode set to plotting\n\ntm_shape(childcare) +\n  tm_dots(alpha=0.4, \n          size=0.05)\n\n\n\n\nThere are three different approaches to address the issue of duplicate points in spatial point pattern analysis:\n\nDeleting Duplicates: This is the simplest approach, where you remove the duplicate points from your dataset. However, this method comes with a drawback, as it may result in the loss of valuable point events. If each point represents a meaningful observation, deleting duplicates can lead to the omission of important data.\nJittering: The second solution involves adding a small random perturbation to the duplicate points. This perturbation is often referred to as “jitter,” and it introduces a slight variation in the spatial locations of duplicate points, ensuring that they do not occupy the exact same space. Jittering is a way to retain all points while avoiding the issue of perfect overlap.\nAttaching Duplicates as Marks: The third solution is to treat each point as “unique” and then attach the duplicates of the points as marks or attributes of the points. In this approach, duplicate points are not removed or perturbed; instead, they are associated with additional information or attributes. This allows you to preserve all observations while acknowledging their duplications. Analytical techniques that consider these marks can be applied to study the spatial pattern or relationships among points.\n\n\nJittering\n\nchildcare_ppp_jit &lt;- rjitter(childcare_ppp, \n                             retry=TRUE, \n                             nsim=1, \n                             drop=TRUE)\n\nLet’s check if there are any duplicate points\n\nany(duplicated(childcare_ppp_jit))\n\n[1] FALSE\n\n\n\n\n\nCreating owin object\nThe owin object is essentially a spatial window that limits the analysis to a specific polygonal region. It provides a framework for handling point patterns within this spatial window. This is particularly important when you want to account for the geographic boundaries of the study area in your analysis.\n\n# this is from generic spatia lclass to owin\nsg_owin &lt;- as(sg_sp, \"owin\")\n\n\nplot(sg_owin)\n\n\n\n\n\nsummary(sg_owin)\n\nWindow: polygonal boundary\n60 separate polygons (no holes)\n            vertices        area relative.area\npolygon 1         38 1.56140e+04      2.09e-05\npolygon 2        735 4.69093e+06      6.27e-03\npolygon 3         49 1.66986e+04      2.23e-05\npolygon 4         76 3.12332e+05      4.17e-04\npolygon 5       5141 6.36179e+08      8.50e-01\npolygon 6         42 5.58317e+04      7.46e-05\npolygon 7         67 1.31354e+06      1.75e-03\npolygon 8         15 4.46420e+03      5.96e-06\npolygon 9         14 5.46674e+03      7.30e-06\npolygon 10        37 5.26194e+03      7.03e-06\npolygon 11        53 3.44003e+04      4.59e-05\npolygon 12        74 5.82234e+04      7.78e-05\npolygon 13        69 5.63134e+04      7.52e-05\npolygon 14       143 1.45139e+05      1.94e-04\npolygon 15       165 3.38736e+05      4.52e-04\npolygon 16       130 9.40465e+04      1.26e-04\npolygon 17        19 1.80977e+03      2.42e-06\npolygon 18        16 2.01046e+03      2.69e-06\npolygon 19        93 4.30642e+05      5.75e-04\npolygon 20        90 4.15092e+05      5.54e-04\npolygon 21       721 1.92795e+06      2.57e-03\npolygon 22       330 1.11896e+06      1.49e-03\npolygon 23       115 9.28394e+05      1.24e-03\npolygon 24        37 1.01705e+04      1.36e-05\npolygon 25        25 1.66227e+04      2.22e-05\npolygon 26        10 2.14507e+03      2.86e-06\npolygon 27       190 2.02489e+05      2.70e-04\npolygon 28       175 9.25904e+05      1.24e-03\npolygon 29      1993 9.99217e+06      1.33e-02\npolygon 30        38 2.42492e+04      3.24e-05\npolygon 31        24 6.35239e+03      8.48e-06\npolygon 32        53 6.35791e+05      8.49e-04\npolygon 33        41 1.60161e+04      2.14e-05\npolygon 34        22 2.54368e+03      3.40e-06\npolygon 35        30 1.08382e+04      1.45e-05\npolygon 36       327 2.16921e+06      2.90e-03\npolygon 37       111 6.62927e+05      8.85e-04\npolygon 38        90 1.15991e+05      1.55e-04\npolygon 39        98 6.26829e+04      8.37e-05\npolygon 40       415 3.25384e+06      4.35e-03\npolygon 41       222 1.51142e+06      2.02e-03\npolygon 42       107 6.33039e+05      8.45e-04\npolygon 43         7 2.48299e+03      3.32e-06\npolygon 44        17 3.28303e+04      4.38e-05\npolygon 45        26 8.34758e+03      1.11e-05\npolygon 46       177 4.67446e+05      6.24e-04\npolygon 47        16 3.19460e+03      4.27e-06\npolygon 48        15 4.87296e+03      6.51e-06\npolygon 49        66 1.61841e+04      2.16e-05\npolygon 50       149 5.63430e+06      7.53e-03\npolygon 51       609 2.62570e+07      3.51e-02\npolygon 52         8 7.82256e+03      1.04e-05\npolygon 53       976 2.33447e+07      3.12e-02\npolygon 54        55 8.25379e+04      1.10e-04\npolygon 55       976 2.33447e+07      3.12e-02\npolygon 56        61 3.33449e+05      4.45e-04\npolygon 57         6 1.68410e+04      2.25e-05\npolygon 58         4 9.45963e+03      1.26e-05\npolygon 59        46 6.99702e+05      9.35e-04\npolygon 60        13 7.00873e+04      9.36e-05\nenclosing rectangle: [2663.93, 56047.79] x [16357.98, 50244.03] units\n                     (53380 x 33890 units)\nWindow area = 748741000 square units\nFraction of frame area: 0.414\n\n\n\n\nCombining point events object and owin object\nIn this last step of geospatial data wrangling, we will extract childcare events that are located within Singapore by using the code chunk below.\n\nchildcareSG_ppp = childcare_ppp[sg_owin]\n\n\nplot(childcareSG_ppp)"
  },
  {
    "objectID": "Hands-on_Ex/Hand-on_Ex03/Hands-on_Ex03.html#first-order-spatial-point-patterns-analysis",
    "href": "Hands-on_Ex/Hand-on_Ex03/Hands-on_Ex03.html#first-order-spatial-point-patterns-analysis",
    "title": "IS415-GAA",
    "section": "First-order Spatial Point Patterns Analysis",
    "text": "First-order Spatial Point Patterns Analysis\n\nderiving kernel density estimation (KDE) layer for visualising and exploring the intensity of point processes\nperforming Confirmatory Spatial Point Patterns Analysis by using Nearest Neighbour statistics\n\n\nKernel Density Estimation\n\nComputing kernel density estimation using automatic bandwidth selection method\n\nkde_childcareSG_bw &lt;- density(childcareSG_ppp,\n                              sigma=bw.diggle,\n                              edge=TRUE,\n                            kernel=\"gaussian\") \n\nWarning: point-in-polygon test had difficulty with 215 points (total score not\n0 or 1)\n\n\nCode walkthrough:\n\ndensity: estimate density of a dataset\nsigma=bw.diggle\n\nsigma refers to the bandwidth / radius of the circle/base of the hill. remember, each hill would be given a weight at its peak and it slowly gradually decreases as it goes to the sides.\nbw.diggle is how we calculate the bandwidth\nkernel=gaussian refers to the smoothening method. we know the density is the top of the hill, and how it smoothens out to the edges of the hill is controlled by the kernel method\n\n\n\nplot(kde_childcareSG_bw)\n\n\n\n\nThe density values of the output range from 0 to 0.000035 which is way too small to comprehend. This is because the default unit of measurement of svy21 is in meter. As a result, the density values computed is in “number of points per square meter”.\nHow do you recover bandwidth:\n\nbw &lt;- bw.diggle(childcareSG_ppp)\nbw\n\n   sigma \n306.6986 \n\n\n\n\nRescalling KDE values\nkde_childcareSG_bw stores the density information / the density at different parts of the geographic space.\nNormally, density in the context of kernel density estimation is expressed in terms of “number of points per unit area.” Since the unit here is meters, the density is being expressed as “number of points per square meter.” This means that the value you see (like 0.000035) represents how many points you would expect to find in a square meter of space.\nWhy the Values are Small: Given that a square meter is a relatively small area for most geographical data (like locations of childcare centers, if that’s what your data represents), it’s common for the density values to be very small, especially if the points (childcare centers in this example) are not extremely densely packed.\nIn practical terms, when dealing with such small values in a large area (like a city or country), we might consider transforming the scale of your data or adjusting the units of measurement to make the data more interpretable. For instance, we could convert the densities into “number of points per square kilometer” by multiplying the densities by 1,000,000 (since there are 1,000,000 square meters in a square kilometer). This could make the values more comprehensible in a broader geographical context.\nIn the code chunk below, rescale() is used to covert the unit of measurement from meter to kilometer.\n\nchildcareSG_ppp.km &lt;- rescale(childcareSG_ppp, 1000, \"km\")\n\n\nkde_childcareSG.bw &lt;- density(childcareSG_ppp.km, sigma=bw.diggle, edge=TRUE, kernel=\"gaussian\")\n\nWarning: point-in-polygon test had difficulty with 215 points (total score not\n0 or 1)\n\nplot(kde_childcareSG.bw)\n\n\n\n\nNotice that output image looks identical to the earlier version, the only changes in the data values (refer to the legend).\n\n\n\nWorking with different automatic badwidth methods\nBeside bw.diggle(), there are three other spatstat functions can be used to determine the bandwidth, they are: bw.CvL(), bw.scott(), and bw.ppl().\n\n bw.CvL(childcareSG_ppp.km)\n\n   sigma \n4.543278 \n\n\n\nbw.scott(childcareSG_ppp.km)\n\n sigma.x  sigma.y \n2.159749 1.396455 \n\n\n\nbw.ppl(childcareSG_ppp.km)\n\nWarning: point-in-polygon test had difficulty with 215 points (total score not\n0 or 1)\n\nWarning: point-in-polygon test had difficulty with 215 points (total score not\n0 or 1)\n\nWarning: point-in-polygon test had difficulty with 215 points (total score not\n0 or 1)\n\nWarning: point-in-polygon test had difficulty with 215 points (total score not\n0 or 1)\n\nWarning: point-in-polygon test had difficulty with 215 points (total score not\n0 or 1)\n\nWarning: point-in-polygon test had difficulty with 215 points (total score not\n0 or 1)\n\nWarning: point-in-polygon test had difficulty with 215 points (total score not\n0 or 1)\n\nWarning: point-in-polygon test had difficulty with 215 points (total score not\n0 or 1)\n\nWarning: point-in-polygon test had difficulty with 215 points (total score not\n0 or 1)\n\nWarning: point-in-polygon test had difficulty with 215 points (total score not\n0 or 1)\n\nWarning: point-in-polygon test had difficulty with 215 points (total score not\n0 or 1)\n\nWarning: point-in-polygon test had difficulty with 215 points (total score not\n0 or 1)\n\nWarning: point-in-polygon test had difficulty with 215 points (total score not\n0 or 1)\n\nWarning: point-in-polygon test had difficulty with 215 points (total score not\n0 or 1)\n\nWarning: point-in-polygon test had difficulty with 215 points (total score not\n0 or 1)\n\nWarning: point-in-polygon test had difficulty with 215 points (total score not\n0 or 1)\n\n\n    sigma \n0.3897114 \n\n\n\nbw.diggle(childcareSG_ppp.km)\n\n    sigma \n0.3066986 \n\n\n\nkde_childcareSG.ppl &lt;- density(childcareSG_ppp.km, \n                               sigma=bw.ppl, \n                               edge=TRUE,\n                               kernel=\"gaussian\")\n\nWarning: point-in-polygon test had difficulty with 215 points (total score not\n0 or 1)\n\nWarning: point-in-polygon test had difficulty with 215 points (total score not\n0 or 1)\n\nWarning: point-in-polygon test had difficulty with 215 points (total score not\n0 or 1)\n\nWarning: point-in-polygon test had difficulty with 215 points (total score not\n0 or 1)\n\nWarning: point-in-polygon test had difficulty with 215 points (total score not\n0 or 1)\n\nWarning: point-in-polygon test had difficulty with 215 points (total score not\n0 or 1)\n\nWarning: point-in-polygon test had difficulty with 215 points (total score not\n0 or 1)\n\nWarning: point-in-polygon test had difficulty with 215 points (total score not\n0 or 1)\n\nWarning: point-in-polygon test had difficulty with 215 points (total score not\n0 or 1)\n\nWarning: point-in-polygon test had difficulty with 215 points (total score not\n0 or 1)\n\nWarning: point-in-polygon test had difficulty with 215 points (total score not\n0 or 1)\n\nWarning: point-in-polygon test had difficulty with 215 points (total score not\n0 or 1)\n\nWarning: point-in-polygon test had difficulty with 215 points (total score not\n0 or 1)\n\nWarning: point-in-polygon test had difficulty with 215 points (total score not\n0 or 1)\n\nWarning: point-in-polygon test had difficulty with 215 points (total score not\n0 or 1)\n\nWarning: point-in-polygon test had difficulty with 215 points (total score not\n0 or 1)\n\nWarning: point-in-polygon test had difficulty with 215 points (total score not\n0 or 1)\n\npar(mfrow=c(1,2))\nplot(kde_childcareSG.bw, main = \"bw.diggle\")\nplot(kde_childcareSG.ppl, main = \"bw.ppl\")\n\n\n\n\n\n\nWorking with different kernel methods\nBy default, the kernel method used in density.ppp() is gaussian. But there are three other options, namely: Epanechnikov, Quartic and Dics.\nThe code chunk below will be used to compute three more kernel density estimations by using these three kernel function.\npar(mfrow=c(2,2)) plot(density(childcareSG_ppp.km, sigma=bw.ppl, edge=TRUE, kernel=“gaussian”), main=“Gaussian”) plot(density(childcareSG_ppp.km, sigma=bw.ppl, edge=TRUE, kernel=“epanechnikov”), main=“Epanechnikov”) plot(density(childcareSG_ppp.km, sigma=bw.ppl, edge=TRUE, kernel=“quartic”), main=“Quartic”) plot(density(childcareSG_ppp.km, sigma=bw.ppl, edge=TRUE, kernel=“disc”), main=“Disc”)"
  },
  {
    "objectID": "Hands-on_Ex/Hand-on_Ex03/Hands-on_Ex03.html#fixed-and-adaptive-kde",
    "href": "Hands-on_Ex/Hand-on_Ex03/Hands-on_Ex03.html#fixed-and-adaptive-kde",
    "title": "IS415-GAA",
    "section": "Fixed and Adaptive KDE",
    "text": "Fixed and Adaptive KDE\n\nComputing KDE by using fixed bandwidth\nNext, you will compute a KDE layer by defining a bandwidth of 600 meter. Notice that in the code chunk below, the sigma value used is 0.6. This is because the unit of measurement of childcareSG_ppp.km object is in kilometer, hence the 600m is 0.6km.\n\nkde_childcareSG_600 &lt;- density(childcareSG_ppp.km, sigma=0.6, edge=TRUE, kernel=\"gaussian\")\n\nWarning: point-in-polygon test had difficulty with 215 points (total score not\n0 or 1)\n\nplot(kde_childcareSG_600)\n\n\n\n\n\n\nComputing KDE by using adaptive bandwidth\nFixed bandwidth method is very sensitive to highly skew distribution of spatial point patterns over geographical units for example urban versus rural. One way to overcome this problem is by using adaptive bandwidth instead.\nIn this section, you will learn how to derive adaptive kernel density estimation by using density.adaptive() of spatstat.\n\nkde_childcareSG_adaptive &lt;- adaptive.density(childcareSG_ppp.km, method=\"kernel\")\n\nWarning: point-in-polygon test had difficulty with 215 points (total score not\n0 or 1)\n\nWarning: point-in-polygon test had difficulty with 215 points (total score not\n0 or 1)\n\nWarning: point-in-polygon test had difficulty with 215 points (total score not\n0 or 1)\n\nWarning: point-in-polygon test had difficulty with 215 points (total score not\n0 or 1)\n\nWarning: point-in-polygon test had difficulty with 215 points (total score not\n0 or 1)\n\nWarning: point-in-polygon test had difficulty with 215 points (total score not\n0 or 1)\n\nWarning: point-in-polygon test had difficulty with 215 points (total score not\n0 or 1)\n\nWarning: point-in-polygon test had difficulty with 215 points (total score not\n0 or 1)\n\nWarning: point-in-polygon test had difficulty with 215 points (total score not\n0 or 1)\n\nWarning: point-in-polygon test had difficulty with 215 points (total score not\n0 or 1)\n\nWarning: point-in-polygon test had difficulty with 215 points (total score not\n0 or 1)\n\nWarning: point-in-polygon test had difficulty with 215 points (total score not\n0 or 1)\n\nWarning: point-in-polygon test had difficulty with 215 points (total score not\n0 or 1)\n\nWarning: point-in-polygon test had difficulty with 215 points (total score not\n0 or 1)\n\nWarning: point-in-polygon test had difficulty with 215 points (total score not\n0 or 1)\n\nWarning: point-in-polygon test had difficulty with 215 points (total score not\n0 or 1)\n\nWarning: point-in-polygon test had difficulty with 215 points (total score not\n0 or 1)\n\nWarning: point-in-polygon test had difficulty with 215 points (total score not\n0 or 1)\n\nWarning: point-in-polygon test had difficulty with 215 points (total score not\n0 or 1)\n\nWarning: point-in-polygon test had difficulty with 215 points (total score not\n0 or 1)\n\nWarning: point-in-polygon test had difficulty with 215 points (total score not\n0 or 1)\n\nWarning: point-in-polygon test had difficulty with 215 points (total score not\n0 or 1)\n\nWarning: point-in-polygon test had difficulty with 215 points (total score not\n0 or 1)\n\nWarning: point-in-polygon test had difficulty with 215 points (total score not\n0 or 1)\n\nWarning: point-in-polygon test had difficulty with 215 points (total score not\n0 or 1)\n\nWarning: point-in-polygon test had difficulty with 215 points (total score not\n0 or 1)\n\nWarning: point-in-polygon test had difficulty with 215 points (total score not\n0 or 1)\n\nWarning: point-in-polygon test had difficulty with 215 points (total score not\n0 or 1)\n\nWarning: point-in-polygon test had difficulty with 215 points (total score not\n0 or 1)\n\nWarning: point-in-polygon test had difficulty with 215 points (total score not\n0 or 1)\n\nWarning: point-in-polygon test had difficulty with 215 points (total score not\n0 or 1)\n\nWarning: point-in-polygon test had difficulty with 215 points (total score not\n0 or 1)\n\nWarning: point-in-polygon test had difficulty with 215 points (total score not\n0 or 1)\n\nWarning: point-in-polygon test had difficulty with 215 points (total score not\n0 or 1)\n\nWarning: point-in-polygon test had difficulty with 215 points (total score not\n0 or 1)\n\nWarning: point-in-polygon test had difficulty with 215 points (total score not\n0 or 1)\n\nWarning: point-in-polygon test had difficulty with 215 points (total score not\n0 or 1)\n\nWarning: point-in-polygon test had difficulty with 215 points (total score not\n0 or 1)\n\nWarning: point-in-polygon test had difficulty with 215 points (total score not\n0 or 1)\n\nWarning: point-in-polygon test had difficulty with 215 points (total score not\n0 or 1)\n\nWarning: point-in-polygon test had difficulty with 215 points (total score not\n0 or 1)\n\nWarning: point-in-polygon test had difficulty with 215 points (total score not\n0 or 1)\n\nWarning: point-in-polygon test had difficulty with 215 points (total score not\n0 or 1)\n\nWarning: point-in-polygon test had difficulty with 215 points (total score not\n0 or 1)\n\nWarning: point-in-polygon test had difficulty with 215 points (total score not\n0 or 1)\n\nWarning: point-in-polygon test had difficulty with 215 points (total score not\n0 or 1)\n\nWarning: point-in-polygon test had difficulty with 215 points (total score not\n0 or 1)\n\nWarning: point-in-polygon test had difficulty with 215 points (total score not\n0 or 1)\n\nWarning: point-in-polygon test had difficulty with 215 points (total score not\n0 or 1)\n\nWarning: point-in-polygon test had difficulty with 215 points (total score not\n0 or 1)\n\nWarning: point-in-polygon test had difficulty with 215 points (total score not\n0 or 1)\n\nWarning: point-in-polygon test had difficulty with 215 points (total score not\n0 or 1)\n\nWarning: point-in-polygon test had difficulty with 215 points (total score not\n0 or 1)\n\nWarning: point-in-polygon test had difficulty with 215 points (total score not\n0 or 1)\n\nWarning: point-in-polygon test had difficulty with 215 points (total score not\n0 or 1)\n\nWarning: point-in-polygon test had difficulty with 215 points (total score not\n0 or 1)\n\nWarning: point-in-polygon test had difficulty with 215 points (total score not\n0 or 1)\n\nWarning: point-in-polygon test had difficulty with 215 points (total score not\n0 or 1)\n\nWarning: point-in-polygon test had difficulty with 215 points (total score not\n0 or 1)\n\nWarning: point-in-polygon test had difficulty with 215 points (total score not\n0 or 1)\n\nplot(kde_childcareSG_adaptive)\n\n\n\n\n\n\nConverting KDE output into grid object\nA grid object in spatial analysis represents a regular tessellation of a surface into cells or squares, each with its own value. In the context of KDE output, each cell in the grid would represent the estimated density of points within that cell.\nWhy do we do this?\nCell-wise Comparisons: Once the KDE output is in a grid format, you can perform cell-wise comparisons or calculations. For example, you might want to find areas where the density exceeds a certain threshold or calculate the total area that has a density within a specific range.\nIntegration with GIS Tools: Many Geographic Information System (GIS) tools are optimized to work with grid data. Converting KDE output to a grid format makes it easier to use these tools for further analysis, modeling, or mapping.\nLet’s convert\n\ngridded_kde_childcareSG_bw &lt;- as.SpatialGridDataFrame.im(kde_childcareSG.bw)\n\nspplot(gridded_kde_childcareSG_bw)\n\n\n\n\n\nConverting gridded output into raster\nWhy do we do this?\nBoth raster data and gridded outputs represent spatial information using a grid format, but they serve slightly different purposes and have varying degrees of compatibility with Geographic Information Systems (GIS) and third-party tools.\n\nInteroperability with GIS and Tools: While both gridded outputs and raster data are grid-formatted, raster data is specifically formatted for and thus more directly interoperable with GIS software and third-party tools. Raster formats are designed to be easily imported, manipulated, analyzed, and visualized in GIS platforms.\nMetadata and Georeferencing: Raster datasets typically include metadata and georeferencing information, making them immediately useful for spatial analyses without requiring additional steps to define the spatial extent, projection, or coordinate system.\nEfficiency and Compatibility: Raster formats often support compression, pyramids for efficient zooming, and spatial indexing, which enhances their performance in GIS applications. They are widely supported across different platforms, ensuring compatibility and ease of use in various analyses and application contexts.\n\nNext, we will convert the gridded kernal density objects into RasterLayer object by using raster() of raster package.\n\nkde_childcareSG_bw_raster &lt;- raster(gridded_kde_childcareSG_bw)\n\nLet us take a look at the properties of kde_childcareSG_bw_raster RasterLayer.\n\nkde_childcareSG_bw_raster\n\nclass      : RasterLayer \ndimensions : 128, 128, 16384  (nrow, ncol, ncell)\nresolution : 0.4170614, 0.2647348  (x, y)\nextent     : 2.663926, 56.04779, 16.35798, 50.24403  (xmin, xmax, ymin, ymax)\ncrs        : NA \nsource     : memory\nnames      : v \nvalues     : -1.014191e-14, 32.45281  (min, max)\n\n\ndimensions: 128, 128, 16384:\n\nnrow: The raster has 128 rows.\nncol: The raster has 128 columns.\nncell: The total number of cells (or pixels) in the raster is 16,384, which is the product of the number of rows and columns (128 x 128).\n\nresolution: 0.4170614, 0.2647348 (x, y): This specifies the size of each cell in the units of the raster’s coordinate reference system (CRS). The first number is the width of each cell (x-direction), and the second number is the height of each cell (y-direction). This tells you how much geographic area each pixel represents.\n\n\nAssigning projection systems\nWe saw that there was no projection system on the raster variable. So let’s assign it!\n\nprojection(kde_childcareSG_bw_raster) &lt;- CRS(\"+init=EPSG:3414\")\nkde_childcareSG_bw_raster\n\nclass      : RasterLayer \ndimensions : 128, 128, 16384  (nrow, ncol, ncell)\nresolution : 0.4170614, 0.2647348  (x, y)\nextent     : 2.663926, 56.04779, 16.35798, 50.24403  (xmin, xmax, ymin, ymax)\ncrs        : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +units=m +no_defs \nsource     : memory\nnames      : v \nvalues     : -1.014191e-14, 32.45281  (min, max)\n\n\n\n\n\nVisualising the output in tmap\nFinally, we will display the raster in cartographic quality map using tmap package\n\ntm_shape(kde_childcareSG_bw_raster) + \n  tm_raster(\"v\") +\n  tm_layout(legend.position = c(\"right\", \"bottom\"), frame = FALSE)\n\nVariable(s) \"v\" contains positive and negative values, so midpoint is set to 0. Set midpoint = NA to show the full spectrum of the color palette.\n\n\n\n\n\nif i did not put tm_raster i would have put tm_fill or tm_polgon.tm_raster, tm_fill, and tm_polygon are functions used within the tmap package in R to add different types of layers to a map,\ntm_raster is specifically designed for adding and visualizing raster data layers on a map.\n\n\ntm_fill and tm_polygon\n\ntm_fill: This function is used to add and visualize areas on a map by filling them with colors based on their attributes. It’s often used with spatial polygons data where each polygon represents a distinct area, such as countries, states, districts, or other regions.\ntm_polygon: Very similar to tm_fill, tm_polygon is used to add polygon layers to a map. It can fill the polygons with color based on their attributes and also allows for the customization of border colors and styles. Essentially, tm_fill is a shortcut for tm_polygon with a focus on filling the polygons rather than styling their borders.\n\n\n\nComparing Spatial Point Patterns using KDE\nIn this section, you will learn how to compare KDE of childcare at Ponggol, Tampines, Chua Chu Kang and Jurong West planning areas\n\n4.7.5.1 Extracting study area\nThe code chunk below will be used to extract the target planning areas.\n\npg = mpsz[mpsz@data$PLN_AREA_N == \"PUNGGOL\",]\ntm = mpsz[mpsz@data$PLN_AREA_N == \"TAMPINES\",]\nck = mpsz[mpsz@data$PLN_AREA_N == \"CHOA CHU KANG\",]\njw = mpsz[mpsz@data$PLN_AREA_N == \"JURONG WEST\",]\n\n\nplot(pg, main = \"Ponggol\")\n\n\n\nplot(tm, main = \"Tampines\")\n\n\n\nplot(ck, main = \"Choa Chu Kang\")\n\n\n\nplot(jw, main = \"Jurong West\")\n\n\n\n\n\n\nConverting the spatial point data frame into generic sp format\nso mspz, and any data from it was converted from sf to sp’s spatial* class\nIn summary, you would use Spatial* classes when you have simple spatial data without additional attributes, and you want a straightforward representation of points, lines, or polygons. On the other hand, if your spatial data includes associated attributes and you need to perform in-depth analysis, you would use generic sp objects like SpatialPointsDataFrame or SpatialPolygonsDataFrame.\n\npg_sp = as(pg, \"SpatialPolygons\")\ntm_sp = as(tm, \"SpatialPolygons\")\nck_sp = as(ck, \"SpatialPolygons\")\njw_sp = as(jw, \"SpatialPolygons\")\n\n\n\nCreating owin object\nNow, we will convert these SpatialPolygons objects into owin objects that is required by spatstat.\nspatstat cant take generic sp obejcts and tehy need to be converted into owin objects\nThis is so that we confine our geospatial analysis to that area.\n\nConverting sp Objects to spatstat Objects\nTo use sp objects with spatstat, you generally need to convert them into a format that spatstat understands:\n\nFrom sp to owin: If you have a spatial polygons object from the sp package that defines the study area or observation window, you would convert it to an owin object to define the observation window in spatstat.\nFrom sp Point to ppp: Similarly, if you have point data in an sp format, you would convert it to a ppp object (point pattern object) for analysis in spatstat.\n\n\npg_owin = as(pg_sp, \"owin\")\ntm_owin = as(tm_sp, \"owin\")\nck_owin = as(ck_sp, \"owin\")\njw_owin = as(jw_sp, \"owin\")\n\nchildcare_ppp_jit was made when we tried to remove duplicated points. And that was made from childcare_ppp that was made from an sp object, as ppp objects were needed to put into the spatstat object for spatial analysis.\n\n\n\nCombining childcare points and the study area\n\nchildcare_pg_ppp = childcare_ppp_jit[pg_owin]\nchildcare_tm_ppp = childcare_ppp_jit[tm_owin]\nchildcare_ck_ppp = childcare_ppp_jit[ck_owin]\nchildcare_jw_ppp = childcare_ppp_jit[jw_owin]\n\nNext, rescale() function is used to trasnform the unit of measurement from metre to kilometre.\n\nchildcare_pg_ppp.km = rescale(childcare_pg_ppp, 1000, \"km\")\nchildcare_tm_ppp.km = rescale(childcare_tm_ppp, 1000, \"km\")\nchildcare_ck_ppp.km = rescale(childcare_ck_ppp, 1000, \"km\")\nchildcare_jw_ppp.km = rescale(childcare_jw_ppp, 1000, \"km\")\n\nThe code chunk below is used to plot these four study areas and the locations of the childcare centres.\n\nplot(childcare_pg_ppp.km, main=\"Punggol\")\n\n\n\nplot(childcare_tm_ppp.km, main=\"Tampines\")\n\n\n\nplot(childcare_ck_ppp.km, main=\"Choa Chu Kang\")\n\n\n\nplot(childcare_jw_ppp.km, main=\"Jurong West\")\n\n\n\n\n\n\nComputing KDE\nLet’s compute KDE in each location!\n\nplot(density(childcare_pg_ppp.km, \n             sigma=bw.diggle, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Punggol\")\n\n\n\nplot(density(childcare_tm_ppp.km, \n             sigma=bw.diggle, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Tempines\")\n\n\n\nplot(density(childcare_ck_ppp.km, \n             sigma=bw.diggle, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Choa Chu Kang\")\n\n\n\nplot(density(childcare_jw_ppp.km, \n             sigma=bw.diggle, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"JUrong West\")\n\n\n\n\n\n\nComputing fixed bandwidth KDE\nFor comparison purposes, we will use 250m as the bandwidth\n\nplot(density(childcare_ck_ppp.km, \n             sigma=0.25, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Chou Chu Kang\")\n\n\n\nplot(density(childcare_jw_ppp.km, \n             sigma=0.25, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"JUrong West\")\n\n\n\nplot(density(childcare_pg_ppp.km, \n             sigma=0.25, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Punggol\")\n\n\n\nplot(density(childcare_tm_ppp.km, \n             sigma=0.25, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Tampines\")"
  },
  {
    "objectID": "Hands-on_Ex/Hand-on_Ex03/Hands-on_Ex03.html#nearest-neighbour-analysis",
    "href": "Hands-on_Ex/Hand-on_Ex03/Hands-on_Ex03.html#nearest-neighbour-analysis",
    "title": "IS415-GAA",
    "section": "Nearest Neighbour Analysis",
    "text": "Nearest Neighbour Analysis\nIn this section, we will perform the Clark-Evans test of aggregation for a spatial point pattern by using clarkevans.test() of statspat.\nThe test hypotheses are:\nHo = The distribution of childcare services are randomly distributed.\nH1= The distribution of childcare services are not randomly distributed.\nThe 95% confident interval will be used.\n\nTesting spatial point patterns using Clark and Evans Test\n\nclarkevans.test(childcareSG_ppp,\n                correction=\"none\",\n                clipregion=\"sg_owin\",\n                alternative=c(\"clustered\"),\n                nsim=99)\n\n\n    Clark-Evans test\n    No edge correction\n    Z-test\n\ndata:  childcareSG_ppp\nR = 0.5062, p-value &lt; 2.2e-16\nalternative hypothesis: clustered (R &lt; 1)\n\n\n\n\nClark and Evans Test: Choa Chu Kang planning area\nIn the code chunk below, clarkevans.test() of spatstat is used to performs Clark-Evans test of aggregation for childcare centre in Choa Chu Kang planning area.\n\nclarkevans.test(childcare_ck_ppp,\n                correction=\"none\",\n                clipregion=NULL,\n                alternative=c(\"two.sided\"),\n                nsim=999)\n\n\n    Clark-Evans test\n    No edge correction\n    Z-test\n\ndata:  childcare_ck_ppp\nR = 0.86219, p-value = 0.02334\nalternative hypothesis: two-sided\n\n\n\n\nClark and Evans Test: Tampines planning area\n\nclarkevans.test(childcare_tm_ppp,\n                correction=\"none\",\n                clipregion=NULL,\n                alternative=c(\"two.sided\"),\n                nsim=999)\n\n\n    Clark-Evans test\n    No edge correction\n    Z-test\n\ndata:  childcare_tm_ppp\nR = 0.70015, p-value = 5.481e-10\nalternative hypothesis: two-sided"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html",
    "title": "Hands-On Exercise 2 : Thematic Mapping and GeoVisualisation with R",
    "section": "",
    "text": "Hello! In this page, i will be describing how i performed Thematic Mapping and GeoVisualisation with R. I will mainly be trying to create a choropleth map using the tmap package!\nWe also use pivot_wider() of the tidyr package, and mutate() ,group_by() , select() and filter() of the dplyr package, which i will be explaining in detail.\nI hope you can follow along, and be able to create choropleth maps!"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#loading-packages",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#loading-packages",
    "title": "Hands-On Exercise 2 : Thematic Mapping and GeoVisualisation with R",
    "section": "Loading packages",
    "text": "Loading packages\nThis is a very important step as we have to load the packages before we can use them.\n\npacman::p_load(sf, tmap, tidyverse)\n\n\nWhat is the tmap package?\nThe tmap package provides you with many handy functions to create your own thematic maps like choropleth maps."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#importing-the-data",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#importing-the-data",
    "title": "Hands-On Exercise 2 : Thematic Mapping and GeoVisualisation with R",
    "section": "Importing the data",
    "text": "Importing the data\n\nDownloading the data\nThe links of the data i used are here (download shp format) and here\n\n\nImporting Geospatial Data into R\nAs learnt in the previous page, we use st_read() function of the sf package to read in the data.\n\nmpsz &lt;- st_read(dsn = \"data/geospatial\", \n                layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `C:\\shaysnutss\\IS1455-GAA\\Hands-on_Ex\\Hands-on_Ex02\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\nWhen we realtiple features (geographical entities), with 15 fields, or attributes (such as temperature, precipitation for example).\nLet us examine it further by just printing mpsz (the variable we have stored the data in, that is of sf data object).\n\nmpsz\n\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\nFirst 10 features:\n   OBJECTID SUBZONE_NO       SUBZONE_N SUBZONE_C CA_IND      PLN_AREA_N\n1         1          1    MARINA SOUTH    MSSZ01      Y    MARINA SOUTH\n2         2          1    PEARL'S HILL    OTSZ01      Y          OUTRAM\n3         3          3       BOAT QUAY    SRSZ03      Y SINGAPORE RIVER\n4         4          8  HENDERSON HILL    BMSZ08      N     BUKIT MERAH\n5         5          3         REDHILL    BMSZ03      N     BUKIT MERAH\n6         6          7  ALEXANDRA HILL    BMSZ07      N     BUKIT MERAH\n7         7          9   BUKIT HO SWEE    BMSZ09      N     BUKIT MERAH\n8         8          2     CLARKE QUAY    SRSZ02      Y SINGAPORE RIVER\n9         9         13 PASIR PANJANG 1    QTSZ13      N      QUEENSTOWN\n10       10          7       QUEENSWAY    QTSZ07      N      QUEENSTOWN\n   PLN_AREA_C       REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR\n1          MS CENTRAL REGION       CR 5ED7EB253F99252E 2014-12-05 31595.84\n2          OT CENTRAL REGION       CR 8C7149B9EB32EEFC 2014-12-05 28679.06\n3          SR CENTRAL REGION       CR C35FEFF02B13E0E5 2014-12-05 29654.96\n4          BM CENTRAL REGION       CR 3775D82C5DDBEFBD 2014-12-05 26782.83\n5          BM CENTRAL REGION       CR 85D9ABEF0A40678F 2014-12-05 26201.96\n6          BM CENTRAL REGION       CR 9D286521EF5E3B59 2014-12-05 25358.82\n7          BM CENTRAL REGION       CR 7839A8577144EFE2 2014-12-05 27680.06\n8          SR CENTRAL REGION       CR 48661DC0FBA09F7A 2014-12-05 29253.21\n9          QT CENTRAL REGION       CR 1F721290C421BFAB 2014-12-05 22077.34\n10         QT CENTRAL REGION       CR 3580D2AFFBEE914C 2014-12-05 24168.31\n     Y_ADDR SHAPE_Leng SHAPE_Area                       geometry\n1  29220.19   5267.381  1630379.3 MULTIPOLYGON (((31495.56 30...\n2  29782.05   3506.107   559816.2 MULTIPOLYGON (((29092.28 30...\n3  29974.66   1740.926   160807.5 MULTIPOLYGON (((29932.33 29...\n4  29933.77   3313.625   595428.9 MULTIPOLYGON (((27131.28 30...\n5  30005.70   2825.594   387429.4 MULTIPOLYGON (((26451.03 30...\n6  29991.38   4428.913  1030378.8 MULTIPOLYGON (((25899.7 297...\n7  30230.86   3275.312   551732.0 MULTIPOLYGON (((27746.95 30...\n8  30222.86   2208.619   290184.7 MULTIPOLYGON (((29351.26 29...\n9  29893.78   6571.323  1084792.3 MULTIPOLYGON (((20996.49 30...\n10 30104.18   3454.239   631644.3 MULTIPOLYGON (((24472.11 29...\n\n\nWe can also use glimpse() or head().\n\n\nImporting Attribute Data into R\nNow let us import the aspatial or attribute data into R.\n\nWhat is aspatial/attribute data?\nIt is data that’s not related to shape/location or coordinates but provides more context to the data. For example, if the data is about Airbnb, it tells you the number of rooms, the pricing, the number of people living in each Airbnb house.\nLet’s import it:\n\npopdata &lt;- read_csv(\"data/aspatial/respopagesextod2011to2020.csv\")\n\nNow, let’s view it:\n\nlist(popdata) \n\n[[1]]\n# A tibble: 984,656 × 7\n   PA         SZ                     AG     Sex     TOD                Pop  Time\n   &lt;chr&gt;      &lt;chr&gt;                  &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt;            &lt;dbl&gt; &lt;dbl&gt;\n 1 Ang Mo Kio Ang Mo Kio Town Centre 0_to_4 Males   HDB 1- and 2-Ro…     0  2011\n 2 Ang Mo Kio Ang Mo Kio Town Centre 0_to_4 Males   HDB 3-Room Flats    10  2011\n 3 Ang Mo Kio Ang Mo Kio Town Centre 0_to_4 Males   HDB 4-Room Flats    30  2011\n 4 Ang Mo Kio Ang Mo Kio Town Centre 0_to_4 Males   HDB 5-Room and …    50  2011\n 5 Ang Mo Kio Ang Mo Kio Town Centre 0_to_4 Males   HUDC Flats (exc…     0  2011\n 6 Ang Mo Kio Ang Mo Kio Town Centre 0_to_4 Males   Landed Properti…     0  2011\n 7 Ang Mo Kio Ang Mo Kio Town Centre 0_to_4 Males   Condominiums an…    40  2011\n 8 Ang Mo Kio Ang Mo Kio Town Centre 0_to_4 Males   Others               0  2011\n 9 Ang Mo Kio Ang Mo Kio Town Centre 0_to_4 Females HDB 1- and 2-Ro…     0  2011\n10 Ang Mo Kio Ang Mo Kio Town Centre 0_to_4 Females HDB 3-Room Flats    10  2011\n# ℹ 984,646 more rows\n\n\n\n\n\nData Preparation\nWe need to prepare data to be in the form of a data table (like a normal sql table) before we can prepare a choropleth map.\nTo prepare the data, we use a few methods. i will go through them here before we actually execute them.\nThese are the methods we will go through:\n\npivot_wider() - tidyr package\nmutate() - dplyr package\ngroup_by() - dplyr package\nselect() - dplyr package\nfilter() - dplyr package\n\n\npivot_wider()\nIt helps transform data from long format to wide format.\nLong format: There are more rows and less columns. Here data is normalised and there are many rows for one subject. This can make it hard to compare the different subjects, and is not very good for visualization.\nHere is how a long format table could look like:\n\nHere there are multiple rows if you want to see the amount of sales in a particular month , or for a particular product. It is not very conducive for comparisons.\nWide format: There are less rows and more columns.\n\nHere there are lesser rows (usually there are more rows but in this case it worked out to still being 3 columns). And it is clearer as to how much sales there is per product per month.\nHow it’s used:\nwide_data &lt;- long_data %&gt;% pivot_wider(names_from = Product, values_from = Sales)\nnames_from: is the argument where you specify the column that will be used to create columns in the wide format\nvalues_from: specifies the column containing the values that will be used to file the new wide format columns\n\n\nmutate()\nmutate() is used to execute operations on existing columns and transform them, create new columns, or transform/create data using conditional statements.\n\nHere we see that we create a new column called Grade and if the score is greater than 90, we populate it with A , if not B. We then process existing data by increasing age by 1.\n\n\nfilter()\nThis method is used to filter rows based on a condition. Here is how it is used:\n\nHere we filter the rows and include rows in the data frame ONLY if the score was equal to or greater than 90.\n\n\nselect()\nThis is like the select query in sql, and you use this to select columns you are interested in.\n\n\n\ngroup_by()\nHere you group the data into groups due to a column, like gender for instance. You can then do group wide operations.\n\n\n\nData Wrangling\nNow this is the code we are supposed to run to prepare the data. Before we prepare the data let’s take a step back.\nOur purpose here is to build a choropleth map that shows you the distribution of the dependency variable across the country/ per region. The dependency variable is calculated like so: DEPENDENCY = (YOUNG + AGED) /`ECONOMY ACTIVE. Thus, we need to know the number of people who are young, aged and economy active in each region. Hence, we have to make sure we make rows that tell us these values per region - that requires group by the two values PA and SZ. We can also group_by AG first so we can see the number of people per age group in each location.\nI have added the explanation for each line of code above that code.\n\npopdata2020 &lt;- popdata %&gt;%\n  filter(Time == 2020) %&gt;%\n  group_by(PA, SZ, AG) %&gt;%\n  summarise(`POP` = sum(`Pop`)) %&gt;%\n  ungroup()%&gt;%\n  pivot_wider(names_from=AG, \n              values_from=POP) %&gt;%\n  mutate(YOUNG = rowSums(.[3:6])\n         +rowSums(.[12])) %&gt;%\nmutate(`ECONOMY ACTIVE` = rowSums(.[7:11])+\nrowSums(.[13:15]))%&gt;%\nmutate(`AGED`=rowSums(.[16:21])) %&gt;%\nmutate(`TOTAL`=rowSums(.[3:21])) %&gt;%  \nmutate(`DEPENDENCY` = (`YOUNG` + `AGED`)\n/`ECONOMY ACTIVE`) %&gt;%\n  # we then just want to display columns that are these\n  select(`PA`, `SZ`, `YOUNG`, \n       `ECONOMY ACTIVE`, `AGED`, \n       `TOTAL`, `DEPENDENCY`)\n\n\nCode walkthrough\n\nfilter(Time == 2020) %&gt;%\n\nHere, we are only selecting the rows that have the value 2020 under the time category.\nHere’s a sample data set to help you understand.\n\n\ngroup_by(PA, SZ, AG) %&gt;%\n\nIn group by, we make different groups, each with a unique combination of PA, SZ, and AG. Any row in the dataset that matches that group’s combination of PA, SZ, and AG will be in that group. Hence, each group will have different number of rows. All of the columns of the row will still be there when just executing a group_by(). When grouping, we can do group wide operations, instead of data wide.\n\nsummarise(POP = sum(Pop)) %&gt;%\n\nWe now have the data in groups, with each group having different number of rows.\nWe then add up all the population figures of all the rows in each group and store them in a new column called pop. Now, since the population values in all rows in each group are added together, we have one row per group. Each group has its own combination of pa, sz and ag and its aggragated total summary for population. All the other columns like sex and tod are discarded. You chose to add the population values of all rows together, and did not mention what to do with the other rows, hence they are discarded!\nIn the picture below, you can see that the number of rows have decreased as there were some groups with more than 1 row, and when the population values were added, they were summarised into a row.\nWe now know the number of people per age group per region. This is vital because we have different categories such as young, aged and economy active that we need to compute. Each young, aged and economy active category would consitute of different age categories. Having various different age categories allows us to pick and choose the varying age categories we want to compute to find out the population for each of the young, aged and economy active categories for each region.\n\n\npivot_wider(names_from=AG, values_from=POP) %&gt;%\n\nTo make it easier to compute the population values for each young, aged and economy active category per region, we should make the age categories column names. Then, we can make the population the values under each age category. As illustrated below, we can easily see the number of people per age category per region.\n\n\nmutate(YOUNG = rowSums(.[3:6]) +rowSums(.[12])) %&gt;%\n\nAs we said before, there are different age categories , like 0-4 being one column 5-9 being another column. Hence, we add up populations under columns 3 to 6 in each row as that is the age range that we think is young. After calculating that for each row, we create a column called young and populate it.\n\n\nmutate(ECONOMY ACTIVE = rowSums(.[7:11])+ rowSums(.[13:15]))%&gt;%\n\nnext we want to find out what is the total number of pople in each subzone that are economically active. so we want to add up the values under age range categories that we think are eco active in each row and then make a eco active column and populate it underneath that\n\nmutate(AGED=rowSums(.[16:21])) %&gt;%\n\nDo the same thing for total number of people for each subzone\n\nmutate(TOTAL=rowSums(.[3:21])) %&gt;%\n\nDo the same thing for total number of people for each subzone\n\nmutate(DEPENDENCY = (YOUNG + AGED) /ECONOMY ACTIVE) %&gt;%\n\nwe then calculate the dependency\n\nselect(PA, SZ, YOUNG, ECONOMY ACTIVE, AGED, TOTAL, DEPENDENCY)\n\nwe then just select the columns we want\n\n\n\n\n\nJoining attribute and geographical data\nThe values in PA and SZ are made up of lower and upper case so lets make them all uppercase\n\npopdata2020 &lt;- popdata2020 %&gt;%\n  mutate_at(.vars = vars(PA, SZ), \n          .funs = list(toupper)) %&gt;%\n  filter(`ECONOMY ACTIVE` &gt; 0)\n\nNow let’s join them based on one common column that is SZ - it is present in both the geospatial and attribute data.\n\nmpsz_pop2020 &lt;- left_join(mpsz, popdata2020,\n                          by = c(\"SUBZONE_N\" = \"SZ\"))\n\nLet’s write it into a file. Remember to create the directory and file data/rds/mpszpop2020.rds before executing this.\n\nwrite_rds(mpsz_pop2020, \"data/rds/mpszpop2020.rds\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#choropleth-mapping-geospatial-data-using-tmap",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#choropleth-mapping-geospatial-data-using-tmap",
    "title": "Hands-On Exercise 2 : Thematic Mapping and GeoVisualisation with R",
    "section": "Choropleth Mapping Geospatial Data using tmap",
    "text": "Choropleth Mapping Geospatial Data using tmap\nYou can do this two ways\n\nPlotting a thematic map quickly by using qtm()\nPlotting highly customisable thematic map by using tmap elements\n\n\nUsing qtm()\nWhen we say fill = “DEPENDENCY”, we mean that the colors of the choropleth map would vary based on the values of dependency.\nThis method is quick but reduces the scope for customization.\n\ntmap_mode(\"plot\")\nqtm(mpsz_pop2020, \n    fill = \"DEPENDENCY\")\n\n\n\n\n\n\nUsing tmap()\nWe can start of with learning about using tm_shape() and tm_fill().\nWe are showing a geographical distribution of dependency.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\")\n\n\n\n\nIf we wanted to customise more, and use tm_borders,\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\") +\n  tm_borders(lwd = 0.1,  alpha = 1)\n\n\n\n\nIf we wanted ultimate customisation, these are the functions and arguments available.\nstyle= quantile shows that we use quantile to classify the data and come up with categories/classes. This means there are four categories with the number of data points each category being equal.\nThe palette=blues is the theme of the map.\nThe layout method often focuses on the aesthetics of the map and is quite self explanatory.\ntm_compass() adds a compass to the map.\ntm_scale_bar() adds the scale to show the relationship between the real measurements to the measurements on the screen.\ntm_grid() adds grid lines to show longitutde and latitude.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"Dependency ratio\") +\n  tm_layout(main.title = \"Distribution of Dependency Ratio by planning subzone\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar() +\n  tm_grid(alpha =0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\\n and Population data from Department of Statistics DOS\", \n             position = c(\"left\", \"bottom\"))\n\n\n\n\nHere we have used tmap and used tm_shape() + tm_fill(). tm_fill() automatically draws out polygons in your map and fills them with the color necessiated by its relevant value.\nLet’s say you wanted to draw other shapes in your map other than polygons. You could do that with tm_lines() or tm_raster()\nBut let’s say we wanted to say outright that we wanted to draw out polygons. We can use tm_polygons()\n\n\nUsing tm_polygons()\n\ntm_shape(mpsz_pop2020) +\n  tm_polygons()\n\n\n\n\nRemember: we have to do tm_shape first to load our data in.\nIf we wanted to also say what variable we wanted to show a geographical distribution of :\n\ntm_shape(mpsz_pop2020)+\n  tm_polygons(\"DEPENDENCY\")\n\n\n\n\n\n\nDrawing a choropleth map using tm_fill() and *tm_border()*\nWritten right underneath the tmap section for better flow.\n\n\nData classification methods of tmap\nThere are different ways to classify data into categories.\n\nPlotting choropleth maps with built-in classification methods\nHere we are using jenks, which means it looks for natural clusters in the data and then groups them together. And we are looking to divide the data into 5 natural clusters.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"jenks\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\nWe can also use equal, where the interval in data values is equal. But this will not be good when the data is skewed in one direction as there might not be any data points in many of the intervals.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"equal\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\nDIY (1)\nDIY: Using what you had learned, prepare choropleth maps by using different classification methods supported by tmap and compare their differences.\nLet’s try sd:\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"sd\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\nLet’s try quantile:\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"quantile\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\nI can see that quantile shows more of a geographic distribution. sd shows a lesser range- perhaps the sd was too huge. Hence, there were large number of data points per category and consequently, the number of categories was also less - showing less of a range.\n\n\nDIY (2)\nDIY: Preparing choropleth maps by using similar classification method but with different numbers of classes (i.e. 2, 6, 10, 20). Compare the output maps, what observation can you draw?\nLet’s try quantile.\nLet’s start with 6 classes.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 6,\n          style = \"quantile\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\nLet’s start with 6 classes.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 7,\n          style = \"quantile\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\nWe can see the map when the classes are 5,6 and 7. As it increases from 5 to 6, we get a greater nuance in the geographic distribution of the dependency column. However, when we go from 6 to 7, there is not much of an increase in nuance. The difference in category boundaries are very minimal. Hence, the ideal number of classes would be 6.\n\n\nPlotting choropleth maps with custom breaks\nIn the methods we used before, the breakpoints were set automatically by them. If we wanted to define the boundaries of each category ourselves, we can do that too. However, to do that, we need more information about the data to understand it. Getting the data’s mean/quantile values, minimum, median, can help with setting those boundaries.\nSo let’s get those values through the use of this method.\n\nsummary(mpsz_pop2020$DEPENDENCY)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n 0.1111  0.7147  0.7866  0.8585  0.8763 19.0000      92 \n\n\nNow let’s choose our breakpoints and plot the choropleth map.\nOur breakpoints are at 0, 0.6. 0.7, 0., 0.9, 1.00.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          breaks = c(0, 0.60, 0.70, 0.80, 0.90, 1.00)) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\nColor Scheme\nNow on to the fun part! We get to delve into the color scheme of things\n\nUsing ColourBrewer palette\nLet’s try the blue palette.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 6,\n          style = \"quantile\",\n          palette = \"Blues\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\nLet’s try the green palette.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          style = \"quantile\",\n          palette = \"-Greens\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\nMap Layouts\nWhen we talk about map layouts, we want to control how a map looks. We want to ensure that all aspects of a map are present. This includes the title, the scale bar, the compass, legend, margins and aspect ratios. We also can control the style of the map.\n\nMap Style\nThis refers to tmap_style(), not the style argument in your tm_fill where you declare the type of data classification you choose to employ.\nIt controls how your map would look like. These are the arguments you could use:\nThe available styles are: \"white\", \"gray\", \"natural\", \"cobalt\", \"col_blind\", \"albatross\", \"beaver\", \"bw\", \"classic\", \"watercolor\" \nIf we were to use classic:\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"-Greens\") +\n  tm_borders(alpha = 0.5) +\n  tmap_style(\"classic\")\n\n\n\n\nIf we used dark,\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"-Greens\") +\n  tm_borders(alpha = 0.5) +\n  tmap_style(\"cobalt\")\n\n\n\n\n\n\nMap Legend\nLet’s try adding the legend first.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"jenks\", \n          palette = \"Blues\", \n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(main.title = \"Distribution of Dependency Ratio by planning subzone \\n(Jenks classification)\",\n            main.title.position = \"center\",\n            main.title.size = 1,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            legend.outside = FALSE,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = FALSE) +\n  tm_borders(alpha = 0.5)\n\n\n\n\nLet’s go through what the legend part of the code means.\nlegend.hist= TRUE means that we are going to include a histogram like legend. This will allow the viewer to see the distribution of values in the form of a histogram as well.\nlegend.is.portrait = TRUE means that the legend/histogram will be in portrait orientation.\nlegend.hist.z=0.1 means the legend will be slightly behind the map.\n\n\nCartographic furniture\nThis refers to adding of the compass, scale bar and grid lines that we talked about earlier.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"No. of persons\") +\n  tm_layout(main.title = \"Distribution of Dependency Ratio \\nby planning subzone\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar(width = 0.15) +\n  tm_grid(lwd = 0.1, alpha = 0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\\n and Population data from Department of Statistics DOS\", \n             position = c(\"left\", \"bottom\"))\n\n\n\n\n\n\n\nDrawing Small Multiple Choropleth Maps\nSmall multiple maps, also referred to as facet maps, are composed of many maps arranged side-by-side or vertically. Small multiple maps enable the visualisation of how spatial relationships change with respect to another variable, such as time.\nIn tmap, small multiple maps can be plotted in three ways:\n\nby assigning multiple values to at least one of the asthetic arguments,\nby defining a group-by variable in tm_facets(), and\nby creating multiple stand-alone maps with tmap_arrange().\n\n\nBy assigning multiple values to at least one of the aesthetic arguments\nHere you want both to have the data classification of equal.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(c(\"YOUNG\", \"AGED\"),\n          style = \"equal\", \n          palette = \"Blues\") +\n  tm_layout(legend.position = c(\"right\", \"bottom\")) +\n  tm_borders(alpha = 0.5) +\n  tmap_style(\"white\")\n\n\n\n\nNow, you want each of them to have different data classifications, and different looks.\n\ntm_shape(mpsz_pop2020)+ \n  tm_polygons(c(\"DEPENDENCY\",\"AGED\"),\n          style = c(\"equal\", \"quantile\"), \n          palette = list(\"Blues\",\"Greens\")) +\n  tm_layout(legend.position = c(\"right\", \"bottom\"))\n\n\n\n\nIn these two sets of graphs (4), the two maps in each set are showcasing different variables across Singapore . In the first set, we show the distribution of young and then the distribution of old.\nNow what if we wanted the maps to be linked to each other, as in, show the distribution of the same variable?\n\n\nBy defining a group-by variable in tm_facets()\n\ntm_shape(mpsz_pop2020) +\n  tm_fill(\"DEPENDENCY\",\n          style = \"quantile\",\n          palette = \"Blues\",\n          thres.poly = 0) + \n  tm_facets(by=\"REGION_N\", \n            free.coords=TRUE, \n            drop.shapes=TRUE) +\n  tm_layout(legend.show = FALSE,\n            title.position = c(\"center\", \"center\"), \n            title.size = 20) +\n  tm_borders(alpha = 0.5)\n\n\n\n\nHere, we show the distribution of the same dependency variable across multiple regions.\n\n\nBy creating multiple stand-alone maps with tmap_arrange()\n\nyoungmap &lt;- tm_shape(mpsz_pop2020)+ \n  tm_polygons(\"YOUNG\", \n              style = \"quantile\", \n              palette = \"Blues\")\n\nagedmap &lt;- tm_shape(mpsz_pop2020)+ \n  tm_polygons(\"AGED\", \n              style = \"quantile\", \n              palette = \"Blues\")\n\ntmap_arrange(youngmap, agedmap, asp=1, ncol=2)\n\n\n\n\n\n\n\nMappping Spatial Object Meeting a Selection Criterion\nInstead of creating small multiple choropleth map, you can also use selection funtion to map spatial objects meeting the selection criterion.\n\ntm_shape(mpsz_pop2020[mpsz_pop2020$REGION_N==\"CENTRAL REGION\", ])+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\", \n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(legend.outside = TRUE,\n            legend.height = 0.45, \n            legend.width = 5.0,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = FALSE) +\n  tm_borders(alpha = 0.5)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05-Global.html",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05-Global.html",
    "title": "Hands-on Exercise 9 : Global Measures of Spatial Autocorrelation",
    "section": "",
    "text": "In this hands-on exercise, you will learn how to compute Global Measures of Spatial Autocorrelation (GMSA) by using spdep package. By the end to this hands-on exercise, you will be able to:\n\nimport geospatial data using appropriate function(s) of sf package,\nimport csv file using appropriate function of readr package,\nperform relational join using appropriate join function of dplyr package,\ncompute Global Spatial Autocorrelation (GSA) statistics by using appropriate functions of spdep package,\n\nplot Moran scatterplot,\ncompute and plot spatial correlogram using appropriate function of spdep package.\n\nprovide statistically correct interpretation of GSA statistics."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05-Global.html#overview",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05-Global.html#overview",
    "title": "Hands-on Exercise 9 : Global Measures of Spatial Autocorrelation",
    "section": "",
    "text": "In this hands-on exercise, you will learn how to compute Global Measures of Spatial Autocorrelation (GMSA) by using spdep package. By the end to this hands-on exercise, you will be able to:\n\nimport geospatial data using appropriate function(s) of sf package,\nimport csv file using appropriate function of readr package,\nperform relational join using appropriate join function of dplyr package,\ncompute Global Spatial Autocorrelation (GSA) statistics by using appropriate functions of spdep package,\n\nplot Moran scatterplot,\ncompute and plot spatial correlogram using appropriate function of spdep package.\n\nprovide statistically correct interpretation of GSA statistics."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05-Global.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05-Global.html#getting-started",
    "title": "Hands-on Exercise 9 : Global Measures of Spatial Autocorrelation",
    "section": "9.2 Getting Started",
    "text": "9.2 Getting Started\n\n9.2.1 The analytical question\nIn spatial policy, one of the main development objective of the local government and planners is to ensure equal distribution of development in the province. Our task in this study, hence, is to apply appropriate spatial statistical methods to discover\n\nif development are even distributed geographically.\nIf the answer is No. Then, our next question will be “is there sign of spatial clustering?”.\nAnd, if the answer for this question is yes, then our next question will be “where are these clusters?”\n\nIn this case study, we are interested to examine the spatial pattern of a selected development indicator (i.e. GDP per capita) of Hunan Provice, People Republic of China.\n\n\n9.2.2 The Study Area and Data\nTwo data sets will be used in this hands-on exercise, they are:\n\nHunan province administrative boundary layer at county level. This is a geospatial data set in ESRI shapefile format.\nHunan_2012.csv: This csv file contains selected Hunan’s local development indicators in 2012.\n\n\n\n9.2.3 Setting the Analytical Toolls\nBefore we get started, we need to ensure that spdep, sf, tmap and tidyverse packages of R are currently installed in your R.\n\nsf is use for importing and handling geospatial data in R,\ntidyverse is mainly use for wrangling attribute data in R,\nspdep will be used to compute spatial weights, global and local spatial autocorrelation statistics, and\ntmap will be used to prepare cartographic quality chropleth map.\n\nThe code chunk below is used to perform the following tasks:\n\ncreating a package list containing the necessary R packages,\nchecking if the R packages in the package list have been installed in R,\n\nif they have yet to be installed, RStudio will installed the missing packages,\n\nlaunching the packages into R environment.\n\n\npacman::p_load(sf, spdep, tmap, tidyverse)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05-Global.html#getting-the-data-into-r-environment",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05-Global.html#getting-the-data-into-r-environment",
    "title": "Hands-on Exercise 9 : Global Measures of Spatial Autocorrelation",
    "section": "9.3 Getting the Data Into R Environment",
    "text": "9.3 Getting the Data Into R Environment\nIn this section, you will learn how to bring a geospatial data and its associated attribute table into R environment. The geospatial data is in ESRI shapefile format and the attribute table is in csv fomat.\n\n9.3.1 Import shapefile into r environment\nThe code chunk below uses st_read() of sf package to import Hunan shapefile into R. The imported shapefile will be simple features Object of sf.\n\nhunan &lt;- st_read(dsn = \"../Hands-On_Ex04/data/geospatial\", \n                 layer = \"Hunan\")\n\nReading layer `Hunan' from data source \n  `C:\\shaysnutss\\IS1455-GAA\\Hands-on_Ex\\Hands-on_Ex04\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 88 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\n\n\n\n\n9.3.2 Import csv file into r environment\nNext, we will import Hunan_2012.csv into R by using read_csv() of readr package. The output is R data frame class.\n\nhunan2012 &lt;- read_csv(\"../Hands-On_Ex04/data/aspatial/Hunan_2012.csv\")\n\n\n\n9.3.3 Performing relational join\nThe code chunk below will be used to update the attribute table of hunan’s SpatialPolygonsDataFrame with the attribute fields of hunan2012 dataframe. This is performed by using left_join() of dplyr package\n\nhunan &lt;- left_join(hunan,hunan2012) %&gt;%\n  select(1:4, 7, 15)\n\n\n\n9.3.4 Visualising Regional Development Indicator\nNow, we are going to prepare a basemap and a choropleth map showing the distribution of GDPPC 2012 by using qtm() of tmap package.\n\nequal &lt;- tm_shape(hunan) +\n  tm_fill(\"GDPPC\",\n          n = 5,\n          style = \"equal\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Equal interval classification\")\n\nquantile &lt;- tm_shape(hunan) +\n  tm_fill(\"GDPPC\",\n          n = 5,\n          style = \"quantile\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Equal quantile classification\")\n\ntmap_arrange(equal, \n             quantile, \n             asp=1, \n             ncol=2)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05-Global.html#global-measures-of-spatial-autocorrelation",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05-Global.html#global-measures-of-spatial-autocorrelation",
    "title": "Hands-on Exercise 9 : Global Measures of Spatial Autocorrelation",
    "section": "9.4 Global Measures of Spatial Autocorrelation",
    "text": "9.4 Global Measures of Spatial Autocorrelation\nIn this section, you will learn how to compute global spatial autocorrelation statistics and to perform spatial complete randomness test for global spatial autocorrelation.\n\n9.4.1 Computing Contiguity Spatial Weights\nBefore we can compute the global spatial autocorrelation statistics, we need to construct a spatial weights of the study area. The spatial weights is used to define the neighbourhood relationships between the geographical units (i.e. county) in the study area.\nIn the code chunk below, poly2nb() of spdep package is used to compute contiguity weight matrices for the study area. This function builds a neighbours list based on regions with contiguous boundaries. If you look at the documentation you will see that you can pass a “queen” argument that takes TRUE or FALSE as options. If you do not specify this argument the default is set to TRUE, that is, if you don’t specify queen = FALSE this function will return a list of first order neighbours using the Queen criteria.\nMore specifically, the code chunk below is used to compute Queen contiguity weight matrix.\n\nwm_q &lt;- poly2nb(hunan, \n                queen=TRUE)\nsummary(wm_q)\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \nLink number distribution:\n\n 1  2  3  4  5  6  7  8  9 11 \n 2  2 12 16 24 14 11  4  2  1 \n2 least connected regions:\n30 65 with 1 link\n1 most connected region:\n85 with 11 links\n\n\nThe summary report above shows that there are 88 area units in Hunan. The most connected area unit has 11 neighbours. There are two area units with only one neighbours.\n\n\n9.4.2 Row-standardised weights matrix\nNext, we need to assign weights to each neighboring polygon. In our case, each neighboring polygon will be assigned equal weight (style=“W”). This is accomplished by assigning the fraction 1/(#ofneighbors) to each neighboring county then summing the weighted income values. While this is the most intuitive way to summaries the neighbors’ values it has one drawback in that polygons along the edges of the study area will base their lagged values on fewer polygons thus potentially over- or under-estimating the true nature of the spatial autocorrelation in the data. For this example, we’ll stick with the style=“W” option for simplicity’s sake but note that other more robust options are available, notably style=“B”.\n\nrswm_q &lt;- nb2listw(wm_q, \n                   style=\"W\", \n                   zero.policy = TRUE)\nrswm_q\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \n\nWeights style: W \nWeights constants summary:\n   n   nn S0       S1       S2\nW 88 7744 88 37.86334 365.9147"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05-Global.html#global-measures-of-spatial-autocorrelation-morans-i",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05-Global.html#global-measures-of-spatial-autocorrelation-morans-i",
    "title": "Hands-on Exercise 9 : Global Measures of Spatial Autocorrelation",
    "section": "9.5 Global Measures of Spatial Autocorrelation: Moran’s I",
    "text": "9.5 Global Measures of Spatial Autocorrelation: Moran’s I\nIn this section, you will learn how to perform Moran’s I statistics testing by using moran.test() of spdep.\n\n9.5.1 Maron’s I test\nThe code chunk below performs Moran’s I statistical testing using moran.test() of spdep.\n\nmoran.test(hunan$GDPPC, \n           listw=rswm_q, \n           zero.policy = TRUE, \n           na.action=na.omit)\n\n\n    Moran I test under randomisation\n\ndata:  hunan$GDPPC  \nweights: rswm_q    \n\nMoran I statistic standard deviate = 4.7351, p-value = 1.095e-06\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n      0.300749970      -0.011494253       0.004348351 \n\n\n\nhunan$GDPPC: This is specifying the variable on which the Moran’s I test will be performed. It looks like it’s taking a column named GDPPC (which might stand for “Gross Domestic Product Per Capita”) from a data frame or a list named hunan. This is the dataset that contains the values we want to test for spatial autocorrelation.\nlistw=rswm_q: The listw parameter stands for “list weights” and is used to pass a spatial weights object to the function. The object rswm_q contains information about the spatial relationships between the observations in the hunan dataset. These weights define how the influence of each data point is spread out to its neighboring points. The type of weights (e.g., rook, queen, distance-based) and their specifics are defined in the rswm_q object.\nzero.policy = TRUE: This parameter is used to handle cases where there are regions with no neighbors (islands) in the spatial weights list. By setting zero.policy = TRUE, the function is instructed to proceed with the Moran’s I test even if some areas have no neighbors according to the weights matrix. Essentially, it’s a way to tell the function to ignore these “islands” rather than throwing an error.\n\n\n\n\n\n\n\nNote\n\n\n\nThis means that the values in nearby locations are dissimilar and that is statistically significant. The null hypothesis that the data follows random distribution should be rejected\n\n\n\n\n9.5.2 Computing Monte Carlo Moran’s I\nWhat is Monte Carlo?\n\nStart with Real Spatial Data: You have a map with data points, each representing a value of interest, like crime rates in different neighborhoods.\nThe Question: You want to know if high crime rates in one area are related to high crime rates in neighboring areas (positive spatial autocorrelation), or if it’s just random.\nCreating a Simulated World:\n\nImagine you take the map and erase all the crime rate data.\nThen, you redistribute those crime rates randomly across the map. This means you’re shuffling the data points so they no longer have their original spatial pattern. Each area gets a crime rate, but now it’s randomly assigned, not based on the real-world data.\n\nCalculating Moran’s I for the Simulated World:\n\nsimulations (which represent a world of randomness without any spatial pattern\nWith this shuffled map, you calculate Moran’s I, a statistic that measures how much the crime rate in one area is similar to the rates in nearby areas.\nThis calculation gives you a sense of whether the shuffled (randomized) data still shows any pattern of similar values being close together.\n\nRepeat the Simulation Multiple Times:\n\nYou don’t do this process just once. You repeat it many times, each time reshuffling the data and calculating a new Moran’s I.\nThis creates a bunch of Moran’s I values from worlds that are similar to ours but with randomized crime rates.\n\nCompare the Real World to the Simulated Worlds:\n\nNow, you compare the Moran’s I you calculated from your real data to the range of Moran’s I values you got from your simulations.\nIf the real Moran’s I is much higher than what you mostly see in the simulations, it suggests that the pattern in your real data is not random; it’s statistically significant.\nso if moran i of real data is much greater tahn the ranges of moran i we got from the simulations, means there are culstered data and observations closer to each other are similar\n\n\nThe code chunk below performs permutation test for Moran’s I statistic by using moran.mc() of spdep. A total of 100 simulation will be performed.\n\nset.seed(1234)\nbperm= moran.mc(hunan$GDPPC, \n                listw=rswm_q, \n                nsim=99, \n                zero.policy = TRUE, \n                na.action=na.omit)\nbperm\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  hunan$GDPPC \nweights: rswm_q  \nnumber of simulations + 1: 100 \n\nstatistic = 0.30075, observed rank = 100, p-value = 0.01\nalternative hypothesis: greater\n\n\n\nbperm$res[1:99]\n\n [1]  0.0579802047  0.0995374215  0.0694251386 -0.1042239884  0.0038110193\n [6] -0.0601316902 -0.0898497549  0.0886477537 -0.0495291344  0.0044879277\n[11] -0.0403080940 -0.1030595506 -0.0274239009  0.1083713535 -0.0188529094\n[16] -0.0484197610 -0.0253754744 -0.0212514958  0.0469779612  0.0408910440\n[21] -0.0324240897  0.0244599283 -0.0285621520 -0.0315422476 -0.0648166118\n[26]  0.0507360621 -0.0196479277 -0.1012356629 -0.0033345890 -0.0451875786\n[31] -0.0169780147  0.0972702432  0.1049185834 -0.0001699816 -0.0783668376\n[36] -0.0360203633 -0.0554003109 -0.0262207742  0.0066261362 -0.0584681167\n[41] -0.0168718257  0.0082456956 -0.0546958795 -0.0373107908 -0.0653454082\n[46]  0.0462297591 -0.0150531959  0.0282874005  0.0157705925 -0.0836056491\n[51] -0.1190189000  0.0172090478  0.0255428252 -0.0029558492 -0.0355542593\n[56] -0.1258924633  0.0435571651  0.1046473070 -0.0859109762  0.0829705234\n[61] -0.0943879694 -0.0587038328 -0.0135097048 -0.0506202830 -0.0528293680\n[66] -0.1124883683 -0.0770476050 -0.1152233509 -0.0955935516 -0.0970605978\n[71]  0.0637698821 -0.1005808607 -0.0349010557 -0.0481678951 -0.0460736682\n[76] -0.0446534235  0.0496933101  0.0463172140 -0.0224125804 -0.0671087934\n[81] -0.0187950839 -0.0221283710 -0.0038543168  0.0485712996 -0.0670291358\n[86] -0.0340662445 -0.0547678023 -0.1115993786 -0.0286081812 -0.0189904775\n[91] -0.0010492210 -0.0588839587 -0.0013944345  0.0691749580  0.1021861142\n[96]  0.0186604784  0.0091017099 -0.0265195474  0.0001244783\n\n\nThe code mean(bperm$res[1:999]) in R is used to calculate the average (mean) of a subset of simulation results from a Monte Carlo test for spatial autocorrelation (Moran’s I).\n\nmean(bperm$res[1:99])\n\n[1] -0.01842288\n\n\nThe R code var(bperm$res[1:999]) is used to calculate the variance of a subset of simulation results.\n\nvar(bperm$res[1:99])\n\n[1] 0.00338786\n\n\nyou can assess the variability in the Moran’s I values that would be expected under the null hypothesis of no spatial autocorrelation. If the actual Moran’s I calculated from your data is outside this range of variability, it may suggest that the pattern observed in your data is not due to random chance, and there is indeed spatial autocorrelation.\n\nhist(bperm$res, \n     freq=TRUE, \n     breaks=20, \n     xlab=\"Simulated Moran's I\")\nabline(v=0, \n       col=\"red\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05-Global.html#global-measures-of-spatial-autocorrelation-gearys-c",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05-Global.html#global-measures-of-spatial-autocorrelation-gearys-c",
    "title": "Hands-on Exercise 9 : Global Measures of Spatial Autocorrelation",
    "section": "9.6 Global Measures of Spatial Autocorrelation: Geary’s C",
    "text": "9.6 Global Measures of Spatial Autocorrelation: Geary’s C\nIn this section, you will learn how to perform Geary’s C statistics testing by using appropriate functions of spdep package.\n\n9.6.1 Geary’s C test\nThe code chunk below performs Geary’s C test for spatial autocorrelation by using geary.test() of spdep.\n\ngeary.test(hunan$GDPPC, listw=rswm_q)\n\n\n    Geary C test under randomisation\n\ndata:  hunan$GDPPC \nweights: rswm_q \n\nGeary C statistic standard deviate = 3.6108, p-value = 0.0001526\nalternative hypothesis: Expectation greater than statistic\nsample estimates:\nGeary C statistic       Expectation          Variance \n        0.6907223         1.0000000         0.0073364 \n\n\nvalues are dissimilar\n\n\n9.6.2 Computing Monte Carlo Geary’s C\nThe code chunk below performs permutation test for Geary’s C statistic by using geary.mc() of spdep.\n\nset.seed(1234)\nbperm=geary.mc(hunan$GDPPC, \n               listw=rswm_q, \n               nsim=99)\nbperm\n\n\n    Monte-Carlo simulation of Geary C\n\ndata:  hunan$GDPPC \nweights: rswm_q \nnumber of simulations + 1: 100 \n\nstatistic = 0.69072, observed rank = 1, p-value = 0.01\nalternative hypothesis: greater\n\n\n\n\n9.6.3 Visualising the Monte Carlo Geary’s C\nNext, we will plot a histogram to reveal the distribution of the simulated values by using the code chunk below.\n\nmean(bperm$res[1:99])\n\n[1] 1.00126\n\n\n\nvar(bperm$res[1:99])\n\n[1] 0.006247721\n\n\n\nsummary(bperm$res[1:99])\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.8289  0.9563  0.9971  1.0013  1.0413  1.2385"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05-Global.html#spatial-correlogram",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05-Global.html#spatial-correlogram",
    "title": "Hands-on Exercise 9 : Global Measures of Spatial Autocorrelation",
    "section": "9.7 Spatial Correlogram",
    "text": "9.7 Spatial Correlogram\nSpatial correlograms are great to examine patterns of spatial autocorrelation in your data or model residuals. They show how correlated are pairs of spatial observations when you increase the distance (lag) between them - they are plots of some index of autocorrelation (Moran’s I or Geary’s c) against distance.Although correlograms are not as fundamental as variograms (a keystone concept of geostatistics), they are very useful as an exploratory and descriptive tool. For this purpose they actually provide richer information than variograms.\n\n9.7.1 Compute Moran’s I correlogram\nIn the code chunk below, sp.correlogram() of spdep package is used to compute a 6-lag spatial correlogram of GDPPC. The global spatial autocorrelation used in Moran’s I. The plot() of base Graph is then used to plot the output.\n\nMI_corr &lt;- sp.correlogram(wm_q, \n                          hunan$GDPPC, \n                          order=6, \n                          method=\"I\", \n                          style=\"W\")\nplot(MI_corr)\n\n\n\n\norder=6: This tells the command to look at six levels of connection. So not just next-door neighbors (first-level connections), but also the neighbors’ neighbors, and so on, up to six steps away.\n\nPoints: Each point on the correlogram represents the Moran’s I statistic calculated for that particular lag. The value of Moran’s I at lag 1 being the highest and positive suggests that neighboring areas (the first level of neighbors) have the most significant positive spatial autocorrelation, indicating that areas with similar GDP per capita values are geographically close.\nVertical Lines (Error Bars): The vertical lines extending from each point represent the uncertainty or variability in the Moran’s I estimates, often corresponding to confidence intervals. The length of the line indicates the range of Moran’s I values that are statistically plausible for that lag. If the line crosses the horizontal line at Moran’s I = 0, the spatial autocorrelation for that lag is not statistically significant at the chosen confidence level.8u/000\n\nBy plotting the output might not allow us to provide complete interpretation. This is because not all autocorrelation values are statistically significant. Hence, it is important for us to examine the full analysis report by printing out the analysis results as in the code chunk below.\n\nprint(MI_corr)\n\nSpatial correlogram for hunan$GDPPC \nmethod: Moran's I\n         estimate expectation   variance standard deviate Pr(I) two sided    \n1 (88)  0.3007500  -0.0114943  0.0043484           4.7351       2.189e-06 ***\n2 (88)  0.2060084  -0.0114943  0.0020962           4.7505       2.029e-06 ***\n3 (88)  0.0668273  -0.0114943  0.0014602           2.0496        0.040400 *  \n4 (88)  0.0299470  -0.0114943  0.0011717           1.2107        0.226015    \n5 (88) -0.1530471  -0.0114943  0.0012440          -4.0134       5.984e-05 ***\n6 (88) -0.1187070  -0.0114943  0.0016791          -2.6164        0.008886 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n9.7.2 Compute Geary’s C correlogram and plot\nIn the code chunk below, sp.correlogram() of spdep package is used to compute a 6-lag spatial correlogram of GDPPC. The global spatial autocorrelation used in Geary’s C. The plot() of base Graph is then used to plot the output.\n\nGC_corr &lt;- sp.correlogram(wm_q, \n                          hunan$GDPPC, \n                          order=6, \n                          method=\"C\", \n                          style=\"W\")\nplot(GC_corr)\n\n\n\n\n\nprint(GC_corr)\n\nSpatial correlogram for hunan$GDPPC \nmethod: Geary's C\n        estimate expectation  variance standard deviate Pr(I) two sided    \n1 (88) 0.6907223   1.0000000 0.0073364          -3.6108       0.0003052 ***\n2 (88) 0.7630197   1.0000000 0.0049126          -3.3811       0.0007220 ***\n3 (88) 0.9397299   1.0000000 0.0049005          -0.8610       0.3892612    \n4 (88) 1.0098462   1.0000000 0.0039631           0.1564       0.8757128    \n5 (88) 1.2008204   1.0000000 0.0035568           3.3673       0.0007592 ***\n6 (88) 1.0773386   1.0000000 0.0058042           1.0151       0.3100407    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html",
    "title": "Hands-on Exercise 7 : Network Constrained Spatial Point Patterns Analysis",
    "section": "",
    "text": "Network constrained Spatial Point Patterns Analysis (NetSPAA) is a collection of spatial point patterns analysis methods special developed for analysing spatial point event occurs on or alongside network. The spatial point event can be locations of traffic accident or childcare centre for example. The network, on the other hand can be a road network or river network.\nIn this hands-on exercise, you are going to gain hands-on experience on using appropriate functions of spNetwork package:\n\nto derive network constrained kernel density estimation (NetKDE), and\nto perform network G-function and k-function analysis\n\nHow common is it for a point to have its nearest neighbor within 10 units\ncalculates the probability that a point is of ≤x distance away from its nearest neighbour\nprobability that the distance from a randomly chosen point to its nearest neighbor is less than or equal to a certain value.\n\nIf you find that the G function indicates a higher probability of short distances between flowers than you’d expect by chance, it suggests that flowers are clustered rather than randomly or evenly distributed."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#overview",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#overview",
    "title": "Hands-on Exercise 7 : Network Constrained Spatial Point Patterns Analysis",
    "section": "",
    "text": "Network constrained Spatial Point Patterns Analysis (NetSPAA) is a collection of spatial point patterns analysis methods special developed for analysing spatial point event occurs on or alongside network. The spatial point event can be locations of traffic accident or childcare centre for example. The network, on the other hand can be a road network or river network.\nIn this hands-on exercise, you are going to gain hands-on experience on using appropriate functions of spNetwork package:\n\nto derive network constrained kernel density estimation (NetKDE), and\nto perform network G-function and k-function analysis\n\nHow common is it for a point to have its nearest neighbor within 10 units\ncalculates the probability that a point is of ≤x distance away from its nearest neighbour\nprobability that the distance from a randomly chosen point to its nearest neighbor is less than or equal to a certain value.\n\nIf you find that the G function indicates a higher probability of short distances between flowers than you’d expect by chance, it suggests that flowers are clustered rather than randomly or evenly distributed."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#the-data",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#the-data",
    "title": "Hands-on Exercise 7 : Network Constrained Spatial Point Patterns Analysis",
    "section": "7.2 The Data",
    "text": "7.2 The Data\nIn this study, we will analyse the spatial distribution of childcare centre in Punggol planning area. For the purpose of this study, two geospatial data sets will be used. They are:\n\nPunggol_St, a line features geospatial data which store the road network within Punggol Planning Area.\nPunggol_CC, a point feature geospatial data which store the location of childcare centres within Punggol Planning Area.\n\nBoth data sets are in ESRI shapefile format."
  },
  {
    "objectID": "In-class_Ex/In-class-Ex03/In-class-Ex03.html",
    "href": "In-class_Ex/In-class-Ex03/In-class-Ex03.html",
    "title": "Hands-On Exercise 3 :",
    "section": "",
    "text": "pacman::p_load(maptools, sf,raster,spatstat,tmap,tidyverse)\nHere we use two different types of reading because for childcare we know the exact file but the other file is a shapefile that requires multiple files, so you use the dsn part.\nchildcare_sf &lt;- st_read(\"../../Hands-On_Ex/Hand-on_Ex03/data/ChildCareServices.geojson\") %&gt;%\n  st_transform(crs=3414)\n\nReading layer `ChildCareServices' from data source \n  `C:\\shaysnutss\\IS1455-GAA\\Hands-on_Ex\\Hand-on_Ex03\\data\\ChildCareServices.geojson' \n  using driver `GeoJSON'\nSimple feature collection with 1925 features and 2 fields\nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 103.6878 ymin: 1.247759 xmax: 103.9897 ymax: 1.462134\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84\n\nmpsz_sfe &lt;- st_read(dsn=\"data/geospatial\", layer=\"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `C:\\shaysnutss\\IS1455-GAA\\In-class_Ex\\In-class-Ex03\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\nclass(mpsz_sfe)\n\n[1] \"sf\"         \"data.frame\""
  },
  {
    "objectID": "In-class_Ex/In-class-Ex03/In-class-Ex03.html#creating-coastal-outline",
    "href": "In-class_Ex/In-class-Ex03/In-class-Ex03.html#creating-coastal-outline",
    "title": "Hands-On Exercise 3 :",
    "section": "Creating Coastal outline",
    "text": "Creating Coastal outline\nWhen you apply st_union() to a collection of polygons, each representing a state within a country, the function merges all these state polygons into a single polygon that represents the entire country, effectively dissolving the boundaries (state lines) between them.\n\nsg_sf &lt;- mpsz_sfe %&gt;%\n  st_union()\n# st_combine combines geometries wirhout dissolve/resolve boundaries but st_union does\n\n\nplot(sg_sf)"
  },
  {
    "objectID": "In-class_Ex/In-class-Ex03/In-class-Ex03.html#geospatial-data-wrangling",
    "href": "In-class_Ex/In-class-Ex03/In-class-Ex03.html#geospatial-data-wrangling",
    "title": "Hands-On Exercise 3 :",
    "section": "geospatial data wrangling",
    "text": "geospatial data wrangling\ncreating ppp objects : sf method\neasier to do this to get from sf -&gt; ppp, instead of sf -&gt; sp* -&gt; sp general -&gt; ppp\n\nchildcare_ppp &lt;- as.ppp(childcare_sf)\n\n\nsummary(childcare_ppp)\n\nMarked planar point pattern:  1925 points\nAverage intensity 2.417323e-06 points per square unit\n\nCoordinates are given to 3 decimal places\ni.e. rounded to the nearest multiple of 0.001 units\n\nmarks are of type 'character'\nSummary:\n   Length     Class      Mode \n     1925 character character \n\nWindow: rectangle = [11810.03, 45404.24] x [25596.33, 49300.88] units\n                    (33590 x 23700 units)\nWindow area = 796335000 square units\n\n\n\nplot(childcare_ppp)\n\n\n\n\n\nHandling duplicated points\nWe can check the duplication in a ppp object by using the code chunk below.\nThis is very important for postal code data. Postal code covers a large area and muliple data points have the same postal code, so you must check if there are duplicates + use jittering\n\nany(duplicated(childcare_ppp))\n\n[1] FALSE\n\n\n\n\nCreating owin object : sf method\nThis .owin helps with the conversion aagain without you doing sf -&gt; sp* -&gt; sp general -&gt; owin\n\n# . functions work with sf layer, so take the originl data\nsg_owin &lt;- as.owin(sg_sf)\n\n\nplot(sg_owin)\n\n\n\n\n\nsummary(sg_owin)\n\nWindow: polygonal boundary\n80 separate polygons (35 holes)\n                  vertices         area relative.area\npolygon 1            14650  6.97996e+08      8.93e-01\npolygon 2 (hole)         3 -2.21090e+00     -2.83e-09\npolygon 3              285  1.61128e+06      2.06e-03\npolygon 4 (hole)         3 -2.05920e-03     -2.63e-12\npolygon 5 (hole)         3 -8.83647e-03     -1.13e-11\npolygon 6              668  5.40368e+07      6.91e-02\npolygon 7               44  2.26577e+03      2.90e-06\npolygon 8               27  1.50315e+04      1.92e-05\npolygon 9              711  1.28815e+07      1.65e-02\npolygon 10 (hole)       36 -4.01660e+04     -5.14e-05\npolygon 11 (hole)      317 -5.11280e+04     -6.54e-05\npolygon 12 (hole)        3 -3.41405e-01     -4.37e-10\npolygon 13 (hole)        3 -2.89050e-05     -3.70e-14\npolygon 14              77  3.29939e+05      4.22e-04\npolygon 15              30  2.80002e+04      3.58e-05\npolygon 16 (hole)        3 -2.83151e-01     -3.62e-10\npolygon 17              71  8.18750e+03      1.05e-05\npolygon 18 (hole)        3 -1.68316e-04     -2.15e-13\npolygon 19 (hole)       36 -7.79904e+03     -9.97e-06\npolygon 20 (hole)        4 -2.05611e-02     -2.63e-11\npolygon 21 (hole)        3 -2.18000e-06     -2.79e-15\npolygon 22 (hole)        3 -3.65501e-03     -4.67e-12\npolygon 23 (hole)        3 -4.95057e-02     -6.33e-11\npolygon 24 (hole)        3 -3.99521e-02     -5.11e-11\npolygon 25 (hole)        3 -6.62377e-01     -8.47e-10\npolygon 26 (hole)        3 -2.09065e-03     -2.67e-12\npolygon 27              91  1.49663e+04      1.91e-05\npolygon 28 (hole)       26 -1.25665e+03     -1.61e-06\npolygon 29 (hole)      349 -1.21433e+03     -1.55e-06\npolygon 30 (hole)       20 -4.39069e+00     -5.62e-09\npolygon 31 (hole)       48 -1.38338e+02     -1.77e-07\npolygon 32 (hole)       28 -1.99862e+01     -2.56e-08\npolygon 33              40  1.38607e+04      1.77e-05\npolygon 34 (hole)       40 -6.00381e+03     -7.68e-06\npolygon 35 (hole)        7 -1.40545e-01     -1.80e-10\npolygon 36 (hole)       12 -8.36709e+01     -1.07e-07\npolygon 37              45  2.51218e+03      3.21e-06\npolygon 38             142  3.22293e+03      4.12e-06\npolygon 39             148  3.10395e+03      3.97e-06\npolygon 40              75  1.73526e+04      2.22e-05\npolygon 41              83  5.28920e+03      6.76e-06\npolygon 42             211  4.70521e+05      6.02e-04\npolygon 43             106  3.04104e+03      3.89e-06\npolygon 44             266  1.50631e+06      1.93e-03\npolygon 45              71  5.63061e+03      7.20e-06\npolygon 46              10  1.99717e+02      2.55e-07\npolygon 47             478  2.06120e+06      2.64e-03\npolygon 48             155  2.67502e+05      3.42e-04\npolygon 49            1027  1.27782e+06      1.63e-03\npolygon 50 (hole)        3 -1.16959e-03     -1.50e-12\npolygon 51              65  8.42861e+04      1.08e-04\npolygon 52              47  3.82087e+04      4.89e-05\npolygon 53               6  4.50259e+02      5.76e-07\npolygon 54             132  9.53357e+04      1.22e-04\npolygon 55 (hole)        3 -3.23310e-04     -4.13e-13\npolygon 56               4  2.69313e+02      3.44e-07\npolygon 57 (hole)        3 -1.46474e-03     -1.87e-12\npolygon 58            1045  4.44510e+06      5.68e-03\npolygon 59              22  6.74651e+03      8.63e-06\npolygon 60              64  3.43149e+04      4.39e-05\npolygon 61 (hole)        3 -1.98390e-03     -2.54e-12\npolygon 62 (hole)        4 -1.13774e-02     -1.46e-11\npolygon 63              14  5.86546e+03      7.50e-06\npolygon 64              95  5.96187e+04      7.62e-05\npolygon 65 (hole)        4 -1.86410e-02     -2.38e-11\npolygon 66 (hole)        3 -5.12482e-03     -6.55e-12\npolygon 67 (hole)        3 -1.96410e-03     -2.51e-12\npolygon 68 (hole)        3 -5.55856e-03     -7.11e-12\npolygon 69             234  2.08755e+06      2.67e-03\npolygon 70              10  4.90942e+02      6.28e-07\npolygon 71             234  4.72886e+05      6.05e-04\npolygon 72 (hole)       13 -3.91907e+02     -5.01e-07\npolygon 73              15  4.03300e+04      5.16e-05\npolygon 74             227  1.10308e+06      1.41e-03\npolygon 75              10  6.60195e+03      8.44e-06\npolygon 76              19  3.09221e+04      3.95e-05\npolygon 77             145  9.61782e+05      1.23e-03\npolygon 78              30  4.28933e+03      5.49e-06\npolygon 79              37  1.29481e+04      1.66e-05\npolygon 80               4  9.47108e+01      1.21e-07\nenclosing rectangle: [2667.54, 56396.44] x [15748.72, 50256.33] units\n                     (53730 x 34510 units)\nWindow area = 781945000 square units\nFraction of frame area: 0.422\n\n\n\nchildcareSG_ppp &lt;- childcare_ppp[sg_owin]\n\n\npg &lt;- mpsz_sfe %&gt;% \n  filter(PLN_AREA_N == \"PUNGGOL\")\ntm &lt;- mpsz_sfe %&gt;% \n  filter(PLN_AREA_N == \"TAMPINES\")\nckk &lt;- mpsz_sfe %&gt;% \n  filter(PLN_AREA_N == \"CHOA CHU KANG\")\njw &lt;- mpsz_sfe %&gt;% \n  filter(PLN_AREA_N == \"JURONG WEST\")\n\nPlotting target planning area\n\nplot(pg, main =\"Punggol\")\n\n\n\n\n\nplot(tm, main =\"Tampines\")\n\n\n\n\n\nplot(ckk, main =\"Chua Chu Kang\")\n\n\n\n\n\nplot(jw, main =\"Jurong West\")"
  },
  {
    "objectID": "In-class_Ex/In-class-Ex03/In-class-Ex03.html#network-constrained-kernel-density",
    "href": "In-class_Ex/In-class-Ex03/In-class-Ex03.html#network-constrained-kernel-density",
    "title": "Hands-On Exercise 3 :",
    "section": "Network constrained kernel density",
    "text": "Network constrained kernel density"
  },
  {
    "objectID": "In-class_Ex/In-class-Ex03/In-class-Ex03.html#getting-started",
    "href": "In-class_Ex/In-class-Ex03/In-class-Ex03.html#getting-started",
    "title": "Hands-On Exercise 3 :",
    "section": "Getting started",
    "text": "Getting started\n\npacman::p_load(sp,rgdal,sf, spNetwork, tmap, classInt, viridis, tidyverse)"
  },
  {
    "objectID": "In-class_Ex/In-class-Ex03/In-class-Ex03.html#data-import-and-preparation",
    "href": "In-class_Ex/In-class-Ex03/In-class-Ex03.html#data-import-and-preparation",
    "title": "Hands-On Exercise 3 :",
    "section": "2. Data Import and Preparation",
    "text": "2. Data Import and Preparation\n\nnetwork &lt;- st_read(dsn=\"data/geospatial\", layer=\"Punggol_St\")\n\nReading layer `Punggol_St' from data source \n  `C:\\shaysnutss\\IS1455-GAA\\In-class_Ex\\In-class-Ex03\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 2642 features and 2 fields\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: 34038.56 ymin: 40941.11 xmax: 38882.85 ymax: 44801.27\nProjected CRS: SVY21 / Singapore TM\n\n\n\nchildcareNew &lt;- st_read(dsn=\"data/geospatial\", layer=\"Punggol_CC\")\n\nReading layer `Punggol_CC' from data source \n  `C:\\shaysnutss\\IS1455-GAA\\In-class_Ex\\In-class-Ex03\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 61 features and 1 field\nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 34423.98 ymin: 41503.6 xmax: 37619.47 ymax: 44685.77\nz_range:       zmin: 0 zmax: 0\nProjected CRS: SVY21 / Singapore TM\n\n\n\nChange projection system\n\nchildcareNew &lt;-childcareNew %&gt;% st_transform(crs =3414)\n\nnetwork &lt;-network  %&gt;% st_transform(crs =3414)\n\n\n\nPlot\n\ntmap_mode('plot')\ntm_shape(childcareNew) + \n  tm_dots() + \n  tm_shape(network) +\n  tm_lines()\n\n\n\n\n\nlixels &lt;- lixelize_lines(network,750,mindist = 375)\n\nlength of a lixel is set to 750m and min length of a lixel is set to 375\n\nsamples &lt;- lines_center(lixels)\n\n\ndensities &lt;- nkde(network, \n                  events = childcareNew,\n                  w = rep(1,nrow(childcareNew)),\n                  samples = samples,\n                  kernel_name = \"quartic\",\n                  bw = 300, \n                  div= \"bw\", \n                  method = \"simple\", \n                  digits = 1, \n                  tol = 1,\n                  grid_shape = c(1,1), \n                  max_depth = 8,\n                  agg = 5, #we aggregate events within a 5m radius (faster calculation)\n                  sparse = TRUE,\n                  verbose = FALSE)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "IS1455-GAA",
    "section": "",
    "text": "Welcome to IS415 Geospatial Analytics and Applications.\nThis is the course website of the IS415 mod that I am embarking on this semester. You will find my course materials here. Have fun:)"
  }
]